<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Code | Bella Nicholson</title><link>https://bellanich.github.io/tag/code/</link><atom:link href="https://bellanich.github.io/tag/code/index.xml" rel="self" type="application/rss+xml"/><description>Code</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 01 Dec 2024 00:00:00 +0000</lastBuildDate><image><url>https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_512x512_fill_lanczos_center_3.png</url><title>Code</title><link>https://bellanich.github.io/tag/code/</link></image><item><title>Shrinking the Impossible: Deploying My Own Multi-Modal Edge Foundation Model</title><link>https://bellanich.github.io/portfolio/edge-llm/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/edge-llm/</guid><description>&lt;p>Generative AI tooling has become a staple for everyday tasks — from creating presentation visuals to finding the right code syntax. But I&amp;rsquo;ve always felt a little too uneasy trusting cloud-hosted LLMs with my private chats.&lt;/p>
&lt;p>As an ML engineer, I assumed that the only alternative was spinning up an expensive private LLM in the cloud — until &lt;a href="https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/" target="_blank" rel="noopener">the Google I/O Connect event (June 2024)&lt;/a>. That’s where Google revealed their smallest LLM to date, &lt;a href="https://deepmind.google/technologies/gemini/nano/" target="_blank" rel="noopener">Gemini Nano&lt;/a>, running directly &lt;a href="https://www.androidauthority.com/how-to-get-gemini-nano-on-pixel-8-8a-3450466/" target="_blank" rel="noopener">on the Pixel 8&lt;/a>. Seeing an ultra-private AI assistant on edge was inspiring, but for solo developers like me, the open-source tools weren’t quite ready yet.&lt;/p>
&lt;p>Fast-forward six months, and the landscape has changed. Thanks to projects like &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">the Machine Learning Compiler (MLC) framework&lt;/a>, solo developers can now optimize and deploy powerful LLMs on edge devices. Rather than sticking to a unimodal LLM — a well-worn path — I set my sights higher: deploying a multimodal LLM on the smallest edge devices possible.&lt;/p>
&lt;p>The end result? I successfully embedded &lt;a href="https://huggingface.co/google/gemma-2-2b-it" target="_blank" rel="noopener">the multilingiual Gemma 2B&lt;/a> and &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">the multi-modal LLaVA-OneVision Qwen2 0.5B&lt;/a> models on my laptop (among other devices). Take a look for yourself:&lt;/p>
&lt;img src="images/demo.gif">
&lt;p>As shown, my embedded model can comfortably discuss the content of an image and give some interesting synopses. Pretty impressive, right? Here’s a look at how it all works:&lt;/p>
&lt;figure>
&lt;img src="images/model_flowchart.png">
&lt;figcaption>
At 800M parameters, the &lt;a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision Qwen2 0.5B&lt;/a> is ideal for edge deployment but it struggles with complex user instructions. Thus, I used Google’s ultra-efficient &lt;a href="https://huggingface.co/google/gemma-2b">Gemma 2B model&lt;/a> as a fallback.
&lt;/figcaption>
&lt;/figure>
&lt;p>For the full story, including technical details, check out my corresponding &lt;strong>&amp;ldquo;Shrinking the Impossible&amp;rdquo; blog series&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../../post/edge-llm-mlc/">Part 1: Optimizing Foundation Models for Edge Devices with MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-vision-encoders/">Part 2: Teaching Chatbots to See with LLaVA, CLIP, and SigLIP&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-embed-llava/">Part 3: Embedding a Custom-Defined LLaVA-OneVision Model with MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-app/">Part 4: Deploying My Own Pocket-Sized Multi-Modal Large Language Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The source code is also available for your viewing pleasure on &lt;a href="https://github.com/bellanich/pocket-llm" target="_blank" rel="noopener">GitHub&lt;/a>. Consider it your guide to shrinking impossibly large LLMs down to something that fits inside your pocket.&lt;/p></description></item><item><title>Python ML Template</title><link>https://bellanich.github.io/portfolio/python-ml-template/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/python-ml-template/</guid><description>&lt;p>In the past month, I&amp;rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it&amp;rsquo;s also about crafting solutions that rise above them altogether.&lt;/p>
&lt;p>In this case, I found myself asking: &amp;ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project&amp;rdquo;? Well, from putting together &lt;a href="https://github.com/bellanich/python-ml-template" target="_blank" rel="noopener">this Python ML Template repository&lt;/a>, my answer is three. (See the demo above for a quick example.)&lt;/p>
&lt;p>If you&amp;rsquo;re curious about my implementation, checkout &lt;a href="https://github.com/bellanich/python-ml-template/blob/main/README.md" target="_blank" rel="noopener">my project&amp;rsquo;s README file&lt;/a> and &lt;a href="https://github.com/bellanich/python-ml-template/blob/main/docs/0_overview.md" target="_blank" rel="noopener">documentation&lt;/a>. Happy ML feature development!&lt;/p></description></item></channel></rss>