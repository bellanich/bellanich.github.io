<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Generative AI | Bella Nicholson</title><link>https://bellanich.github.io/tag/generative-ai/</link><atom:link href="https://bellanich.github.io/tag/generative-ai/index.xml" rel="self" type="application/rss+xml"/><description>Generative AI</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 26 Nov 2025 00:00:00 +0000</lastBuildDate><image><url>https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_512x512_fill_lanczos_center_3.png</url><title>Generative AI</title><link>https://bellanich.github.io/tag/generative-ai/</link></image><item><title>Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth</title><link>https://bellanich.github.io/post/edge-diffusion-3/</link><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-diffusion-3/</guid><description>&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;!-- - [Table of Contents](#table-of-contents) -->
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-three-stage-approach">A Three-Stage Approach&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#stage-1-person-segmentation-1s">Stage 1: Person Segmentation (~1s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#stage-2-conditional-background-generation">Stage 2: Conditional Background Generation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#prompt-engineering">Prompt Engineering&lt;/a>&lt;/li>
&lt;li>&lt;a href="#thread-safety-and-process-survival">Thread Safety and Process Survival&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#stage-3-compositing--style-filters-1s">Stage 3: Compositing &amp;amp; Style Filters (&amp;lt;1s)&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#lessons-from-the-edge">Lessons from the Edge&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#1-at-small-scales-hardware-aware-wins">1. At small scales, hardware-aware wins&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-if-your-prompt-loses-focus-stable-diffusion-will-too">2. If your prompt loses focus, Stable Diffusion will too&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-architecture-solves-capability-gaps">3. Architecture solves capability gaps&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-joy-of-building-small">The Joy of Building Small&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I&amp;rsquo;m on a personal mission to recreate the Nano Banana experience on the edge&amp;hellip;or at least get as close as physics and open-source tools will allow. In &lt;a href="../edge-diffusion-1/">my first blogpost&lt;/a>, I explained why Apple&amp;rsquo;s CoreML Stable Diffusion (SD) is my best bet. In &lt;a href="../edge-diffusion-2/">the last post&lt;/a>, I broke down how Apple squeezed a 6GB model down to 1.5GB while still delivering sub-10-second generation on iOS.&lt;/p>
&lt;p>But there&amp;rsquo;s catch: Appleâ€™s implementation breaks my use case. If I try a simple img2img portrait edit, the subject&amp;rsquo;s identity collapses. As seen in Figure 1, once I make the denoising strength high enough to change the background, my subject morphs into a loosely related stranger who just happens to be wearing a similar outfit.&lt;/p>
&lt;figure>
&lt;img src="images/failed-portrait-edit.png" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 1.&lt;/strong> Apple's CoreML Stable Diffusion models fails to maintain character consistency. We attempt to transport the lovely Sabrina Carpenter from the 2025 MTV Video Music Awards (&lt;a href="https://www.gettyimages.com/detail/news-photo/sabrina-carpenter-winner-of-the-the-best-album-award-for-news-photo/2233699406?">image credit&lt;/a>) to a festive holiday backdrop. The background and prop swaps are convincing, but the resulting blonde is definitely not Sabrina.
&lt;/figcaption>
&lt;/figure>
&lt;p>If I rely solely on Apple&amp;rsquo;s CoreML Stable Diffusion model, I run into an impossible trade-off:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Low strength (0.3-0.5):&lt;/strong> Character consistency is maintained, but the background barely changes&lt;/li>
&lt;li>&lt;strong>High strength (0.7-0.9):&lt;/strong> Background transforms perfectly to align with the given text prompt; however, the person pictures just becomes unrecognizable.&lt;/li>
&lt;/ul>
&lt;p>This is hardly a surprise, since &lt;strong>(a)&lt;/strong> the original Nano Banana model (released in August 2025) broke the internet for its ability to maintain character consistency and &lt;strong>(b)&lt;/strong> we&amp;rsquo;re working with a hyper-optimized version of a 2022 model. The problem with our approach is that Stable Diffusion can&amp;rsquo;t distinguish between &amp;ldquo;keep this&amp;rdquo; and &amp;ldquo;change that&amp;rdquo;. It&amp;rsquo;s trying to equally transform each pixel. Hence, it&amp;rsquo;s trying to do two conflicting jobs at once: preserve user identity &lt;em>and&lt;/em> dramatically transform the background.&lt;/p>
&lt;p>The lesson? Stop asking Stable Diffusion to multitask. I need to handle identity preservation and scene transformation separately. This blogpost shows how to do this on a shoestring compute budget.&lt;/p>
&lt;h2 id="a-three-stage-approach">A Three-Stage Approach&lt;/h2>
&lt;p>Iâ€™m a fan of simple solutions, especially under a tight runtime budget. So, I started with the simplest move possible: I isolated the subject and focused Stable Diffusion&amp;rsquo;s efforts on background generation. This created a lightweight, three-stage pipeline:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Segment.&lt;/strong> I used &lt;a href="https://developer.apple.com/documentation/vision" target="_blank" rel="noopener">Apple&amp;rsquo;s Vision framework&lt;/a> to perform person segmentation. This process yields &lt;strong>(a)&lt;/strong> a cutout of the person with transparent background, and &lt;strong>(b)&lt;/strong> an inverted mask marking which pixels need regeneration.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Generate.&lt;/strong> I feed the inverted mask and text prompt into Stable Diffusion&amp;rsquo;s img2img pipeline. SD regenerates only the masked background regions while leaving the subject&amp;rsquo;s pixels untouched.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Composite.&lt;/strong> I then layer the original subject cutout over the newly generated background. In order to deliver a photo booth-like user experience, I also added optional Instagram-style filters to make the final outputs more shareable.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>
&lt;img src="images/pipeline-diagram.png" style="width: 35%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 2. &lt;/strong> My hyper-optimized three-stage pipeline uses &lt;a href="https://github.com/apple/ml-stable-diffusion">Apple's CoreML Stable Diffusion model&lt;/a> for on-device conditional image generation. By isolating the heavy diffusion step to background regeneration, the system preserves identity consistency despite the tiny model's quality constraints.
&lt;/figcaption>
&lt;/figure>
&lt;p>The final result is a lean, fully on-device conditional image generation pipeline that runs within an average of ~27 seconds. This puts me safely below my 60 second limit.&lt;/p>
&lt;p>Now, let&amp;rsquo;s dive into the details. To demonstrate each stage&amp;rsquo;s output, we&amp;rsquo;ll successfully transport Sabrina Carpenter from the concert stage to a Winter Wonderland.&lt;/p>
&lt;figure>
&lt;img src="images/original_photo.jpg" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 3. &lt;/strong> Let's assume this concert photo of Sabrina Carpenter, performing onstage during the 2024 Coachella Valley Music and Arts Festival, as our running example. Our goal is to transport her to a Winter Wonderland (&lt;a href="https://www.gettyimages.com/detail/news-photo/sabrina-carpenter-performs-at-the-coachella-stage-during-news-photo/2149329158">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="stage-1-person-segmentation-1s">Stage 1: Person Segmentation (~1s)&lt;/h3>
&lt;p>First, I extract the subject from the background using Apple&amp;rsquo;s &lt;a href="https://developer.apple.com/documentation/vision" target="_blank" rel="noopener">Vision framework&lt;/a>, specifically the &lt;a href="https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest" target="_blank" rel="noopener">&lt;code>VNGeneratePersonSegmentationRequest&lt;/code>&lt;/a> API. This built-in segmentation model ships with iOS and is already optimized for the Neural Engine.&lt;/p>
&lt;p>Deploying Apple&amp;rsquo;s off the shelf solution let&amp;rsquo;s me focus on core problem without getting distracted by additional deployment overhead. Apple has already hyper-optimized this image segmentation model for their Apple Neural Engine (ANE) hardware accelerator. Meaning, even when I set &lt;a href="https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/qualitylevel/accurate" target="_blank" rel="noopener">the preferred quality level to high&lt;/a>, the segmentation model still returns a result within ~1 second. I&amp;rsquo;ve alloted about ~67% of my inference time budget to Stable Diffusion (40 seconds) and the remainder to everything else. Keeping the segmentation step leaves me with plenty of breathing room.&lt;/p>
&lt;p>Figure 3 shows an example of this model&amp;rsquo;s outputs, where its separates the subject from her surroundings.&lt;/p>
&lt;figure style="display: flex; flex-direction: column; align-items: center; text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 10px; align-items: flex-end;">
&lt;div style="width: 45%; display: flex; flex-direction: column;">
&lt;img src="images/1_isolated_subject.png" style="width: 100%; height: auto; max-height: 300px; object-fit: contain;">
&lt;div style="margin-top: 5px; font-size: 0.75em;">(a)&lt;/div>
&lt;/div>
&lt;div style="width: 45%; display: flex; flex-direction: column;">
&lt;img src="images/1_background_mask.png" style="width: 100%; height: auto; max-height: 300px; object-fit: contain;">
&lt;div style="margin-top: 5px; font-size: 0.75em;">(b)&lt;/div>
&lt;/div>
&lt;/div>
&lt;figcaption style="margin-top: 15px;">
&lt;strong>Figure 4.&lt;/strong> Sample person segmentation results. &lt;strong>(a)&lt;/strong> The subject is now isolated against an alpha transparency background. &lt;strong>(b)&lt;/strong> A background mask for subsequent conditional image generation, where only white pixels will be repainted.
&lt;/figcaption>
&lt;/figure>
&lt;p>Now, letâ€™s whisk Sabrina Carpenter off the Coachella stage and drop her straight into a glittery Winter Wonderland for a festive, snow-dusted performance.&lt;/p>
&lt;h3 id="stage-2-conditional-background-generation">Stage 2: Conditional Background Generation&lt;/h3>
&lt;p>This step is where the magic happens â€” and where most of my runtime budget disappears. I feed the background mask from Stage 1 (see Figure 4C) and the text prompt into the Stable Diffusion. The mask acts like a stencil: white regions get regenerated, black pixels (the subject) stay untouched. Everything gets resized to 512Ã—512 before inference, since &lt;a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener">thatâ€™s SDâ€™s native training resolution&lt;/a>.&lt;/p>
&lt;p>For denoising strength, I stayed within &lt;a href="https://huggingface.co/docs/diffusers/en/using-diffusers/write_your_own_pipeline#classifier-free-guidance" target="_blank" rel="noopener">the commonly recommended 0.65â€“0.85 range&lt;/a>: low enough to preserve subject boundaries, high enough to meaningfully transform the background. I used the standard &lt;a href="../edge-diffusion-2/#scheduler-optimization">25 DPM-Solver steps&lt;/a> and set the default guidance scale to 7.5.&lt;/p>
&lt;h4 id="prompt-engineering">Prompt Engineering&lt;/h4>
&lt;p>Prompt engineering took longer than I&amp;rsquo;d like to admit. I wanted to create a visually striking background, so I started with maximalist prompts (&lt;code>&amp;quot;A winter wonderland with snow-covered pine trees, twinkling fairy lights, ice sculptures, frosted windows...&amp;quot;&lt;/code>). CoreML Stable Diffusion got overwhelmed and returned incoherent mush. Then I went ultra-minimal (&lt;code>&amp;quot;A winter scene&amp;quot;&lt;/code>) and got a bleak, featureless white void.&lt;/p>
&lt;p>The sweet spot was photography-style phrasing with a few concrete details, like &lt;code>&amp;quot;A glittery winter wonderland with snow, twinkling lights, warm glow&amp;quot;&lt;/code>. Enough direction, not enough to overwhelm. Along the way, I learned:&lt;/p>
&lt;ul>
&lt;li>Stable Diffusion trims anything past ~75 tokens&lt;/li>
&lt;li>Evocative scene vibes are better than itemized lists&lt;/li>
&lt;li>Lighting cues, like â€œwarm orange glowâ€ vs. â€œblue hour twilightâ€, can set the entire mood&lt;/li>
&lt;/ul>
&lt;p>Now, letâ€™s see what all that work actually produces. Hereâ€™s the raw background Stable Diffusion generated before the subject gets composited back in (Figure 5).&lt;/p>
&lt;figure>
&lt;img src="images/2_generated_background.png" style="width: 50%; height: auto;">
&lt;figcaption>
&lt;strong>Figure 5.&lt;/strong> Stable Diffusionâ€™s raw output for the prompt â€œoutside in magical winter village at blue hour lighting, charming snow-covered cottage with glowing windows.â€ The scene is coherent, though not perfect, details like the opaque â€œwindow/doorâ€ remain ambiguous.
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="thread-safety-and-process-survival">Thread Safety and Process Survival&lt;/h4>
&lt;p>Running a Stable Diffusion pipeline on-device means juggling two hard problems:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Thread safety.&lt;/strong> Segmentation, SD inference, and UI updates all touch the same shared state, creating the perfect incubator for race conditions.&lt;/li>
&lt;li>&lt;strong>Process survival.&lt;/strong> I need to keep the UI responsive while SD runs for ~27 seconds in the background. At the same time, iOS locks the screen after 30 seconds of inactivity and suspends the app, which kills image generation.&lt;/li>
&lt;/ol>
&lt;p>In short, I had to choose between concurrency or chaos. I enabled Swift 6&amp;rsquo;s strict concurrency to catch threading bugs at compile time rather than dealing with surprises in production. With strict concurrency, everything needs explicit actor boundaries. The UI state (&lt;code>@Published&lt;/code> properties, view model updates) runs on the main thread, while Stable Diffusion inference runs on background threads to prevent freezing the entire app.&lt;/p>
&lt;pre>&lt;code class="language-swift">// Simplified coordinator pattern
func generateBackground() {
isProcessing = true // MainActor UI update
Task.detached { // Background thread for heavy work
let result = await pipeline.generate(...)
await MainActor.run { // Back to MainActor for UI
self.outputImage = result
self.isProcessing = false
}
}
}
&lt;/code>&lt;/pre>
&lt;p style="text-align: center; font-size: 0.75em; margin-top: 0.5em; color: #66;">
&lt;strong>Figure 6.&lt;/strong> Actor coordination pattern in Swift. The thread hopping pattern runs inference on a background thread, then returns to the main thread (&lt;code>@MainActor&lt;/code>) for UI updates.
&lt;/p>
&lt;p>I also registered Stable Diffusion as a background task to ensure image generation continues if the screen locks. Without it, we&amp;rsquo;d be left with half a cottage and no Winter Wonderland magic. Once the final image is composited, the background task is released.&lt;/p>
&lt;h3 id="stage-3-compositing--style-filters-1s">Stage 3: Compositing &amp;amp; Style Filters (&amp;lt;1s)&lt;/h3>
&lt;p>With the background generated, I&amp;rsquo;m ready to layer the isolated subject (Fig. 4a) into the new scene. I use &lt;a href="https://developer.apple.com/documentation/coregraphics" target="_blank" rel="noopener">Core Graphics&lt;/a> Apple&amp;rsquo;s low-level 2D rendering framework, to composite these two layers together. This process is fast, clean, and basically free in terms of runtime.&lt;/p>
&lt;figure>
&lt;img src="images/3_original_output.jpg" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 7. &lt;/strong> Let's assume this concert photo of Sabrina Carpenter, performing onstage during the 67th Annual GRAMMY Awards, as our running example. Our goal is to transport her to a Winter Wonderland (&lt;a href="https://github.com/apple/ml-stable-diffusion">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Originally, I planned to chain multiple Stable Diffusion generations together to create a flexible style transfer experience, allowing the user to further personalize their images, but each extra pass incurs another ~25 seconds. No one is going to wait 2+ minutes to try on a different look.&lt;/p>
&lt;p>So I switched to &lt;a href="https://developer.apple.com/documentation/coreimage" target="_blank" rel="noopener">Core Image filters&lt;/a>, which run instantly. I added four curated styles plus an intensity slider, letting users experiment in real time, turning the whole system into a fully on-device, pocket-sized photobooth. Figure 8 highlights a few results, any of which could slide neatly onto Sabrinaâ€™s holiday-themed merch.&lt;/p>
&lt;figure style="display: flex; flex-direction: column; align-items: center; margin: 2em 0;">
&lt;div style="display: flex; flex-direction: column; gap: 15px; width: 100%;">
&lt;!-- Row 1: Original + Pop Art -->
&lt;div style="display: flex; justify-content: center; gap: 10px;">
&lt;div style="width: 45%;">
&lt;img src="images/3_vintage_output.jpg" style="width: 100%; height: auto;">
&lt;div style="margin-top: 5px; font-size: 0.7em; text-align: center;">(a) Vintage &lt;/div>
&lt;/div>
&lt;div style="width: 45%;">
&lt;img src="images/3_pop_art_output.jpg" style="width: 100%; height: auto;">
&lt;div style="margin-top: 5px; font-size: 0.7em; text-align: center;">(b) Pop Art&lt;/div>
&lt;/div>
&lt;/div>
&lt;!-- Row 2: Posterize + Mosaic -->
&lt;div style="display: flex; justify-content: center; gap: 10px;">
&lt;div style="width: 45%;">
&lt;img src="images/3_posterize_output.jpg" style="width: 100%; height: auto;">
&lt;div style="margin-top: 5px; font-size: 0.7em; text-align: center;">(c) Posterize&lt;/div>
&lt;/div>
&lt;div style="width: 45%;">
&lt;img src="images/3_mosiac_output.jpg" style="width: 100%; height: auto;">
&lt;div style="margin-top: 5px; font-size: 0.7em; text-align: center;">(d) Mosaic&lt;/div>
&lt;/div>
&lt;/div>
&lt;figcaption style="margin-top: 1em; text-align: center;">
&lt;strong>Figure 8.&lt;/strong> The same composited image with different Core Image filter styles applied. Each filter applies instantly (&amp;lt;1s) with adjustable intensity.
&lt;/figcaption>
&lt;/figure>
&lt;p>Of course, this system isnâ€™t a one-hit wonder. Figure 9 shows the same pipeline dropping Sabrina underneath a Christmas tree, into San Diegoâ€™s Balboa Park, and even into a groovy reimagining of La Jolla Cove, proving that this pocket-sized photobooth travels just as well as she does.&lt;/p>
&lt;figure style="margin: 2em 0;">
&lt;div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 8px;">
&lt;div>
&lt;img src="images/3_gifts.PNG" style="width: 100%; height: 200px; object-fit: cover; margin-bottom: 3px;">
&lt;img src="images/3_gifts_sabrina.JPG" style="width: 100%; height: 200px; object-fit: cover;">
&lt;div style="margin-top: 3px; font-size: 0.75em; text-align: center;">(a) Gradient Gift Descent&lt;/div>
&lt;/div>
&lt;div>
&lt;img src="images/3_balboa.PNG" style="width: 100%; height: 200px; object-fit: cover; margin-bottom: 3px;">
&lt;img src="images/3_balboa_sabrina.JPG" style="width: 100%; height: 200px; object-fit: cover;">
&lt;div style="margin-top: 3px; font-size: 0.75em; text-align: center;"> (b) Balboa Park&lt;/div>
&lt;/div>
&lt;div>
&lt;img src="images/3_la_jolla.PNG" style="width: 100%; height: 200px; object-fit: cover; margin-bottom: 3px;">
&lt;img src="images/3_la_jolla_sabrina.JPG" style="width: 100%; height: 200px; object-fit: cover;">
&lt;div style="margin-top: 3px; font-size: 0.75em; text-align: center;">(c) La Jolla Cove&lt;/div>
&lt;/div>
&lt;/div>
&lt;figcaption style="text-align: center; margin-top: 1em;">
&lt;strong>Figure 9.&lt;/strong> Sabrina Carpenter, re-imagined in three different scenes. Show both the raw Stable Diffusion generated backgrounds (top) and the final composites (bottom).
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="lessons-from-the-edge">Lessons from the Edge&lt;/h2>
&lt;p>When I started this project, I knew bringing the Nano Banana experience to mobile would be tough, but I just didnâ€™t realize how tough. I learned four valuable lessons along the way:&lt;/p>
&lt;h3 id="1-at-small-scales-hardware-aware-wins">1. At small scales, hardware-aware wins&lt;/h3>
&lt;p>Users wonâ€™t accept bad images just because the model runs fast. Once we shrink diffusion, quality depends heavily on hardware we donâ€™t control or fully understand. Appleâ€™s vertical integration makes some optimizations look effortless, but replicating them from the outside is anything but.&lt;/p>
&lt;h3 id="2-if-your-prompt-loses-focus-stable-diffusion-will-too">2. If your prompt loses focus, Stable Diffusion will too&lt;/h3>
&lt;p>I learned quickly that mixing themes (e.g., Winter Wonderland + robots) just produces incoherent mush. Chaining prompts or tweaking denoising strength didnâ€™t help either: high strength erased the scene, low strength lead to incoherent, blurry transformation.&lt;/p>
&lt;p>Blending multiple semantic concepts is a completely different problem, one tackled by disentangled-control methods like &lt;a href="https://arxiv.org/abs/2302.05543" target="_blank" rel="noopener">ControlNet&lt;/a> and &lt;a href="https://arxiv.org/abs/2308.06721" target="_blank" rel="noopener">IP-Adapter&lt;/a>. But those techniques rely on extra conditioning modules that add hundreds of megabytes and several seconds of latency. That&amp;rsquo;s fine on a workstation, disastrous for a sub-30-second mobile experience.&lt;/p>
&lt;h3 id="3-architecture-solves-capability-gaps">3. Architecture solves capability gaps&lt;/h3>
&lt;p>Splitting the process into Segment â†’ Generate â†’ Composite avoided the pitfalls of an all-in-one model. Segmentation preserved identity, SD produced high-quality backgrounds, and fast filters enabled rapid style iteration. Even if on-device disentangled-control were possible, the gains for the end user would be minimal. The modular workflow already delivers strong speed and consistency. This model has its limitations, but good system design works around them.&lt;/p>
&lt;p>Of course, as base model improves, so does the space for smarter systems built around its new limits.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>I accomplished my original goal of building a Nano-Bananaâ€“style photobooth that runs entirely on-device. Along the way, I also ended up with a compact computer-vision playground ready for whatever comes next.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;p>From here, my options are boundless but they include: automating the tedious prompt engineering process using &lt;a href="https://arxiv.org/abs/2305.13301" target="_blank" rel="noopener">reinforcement learning techniques like DDPO&lt;/a>, trying to recreate some aspects of &lt;a href="https://arxiv.org/abs/2506.06802" target="_blank" rel="noopener">identity-preserving style transfer&lt;/a> on-device, or fine-tuning my tiny Stable Diffusion model with &lt;a href="https://huggingface.co/blog/lora" target="_blank" rel="noopener">LoRA adapters&lt;/a> so it knows why &lt;a href="https://www.npr.org/2025/09/10/nx-s1-5535826/sabrina-carpenter-mans-best-friend-charts" target="_blank" rel="noopener">Sabrina Carpenter is &amp;ldquo;Man&amp;rsquo;s Best Friend&amp;rdquo;&lt;/a>. I now have a solid foundation and free to explore whatâ€™s genuinely interesting.&lt;/p>
&lt;h3 id="the-joy-of-building-small">The Joy of Building Small&lt;/h3>
&lt;p>A GPU cluster would brute-force most of the problems I hit, edge constraints push me to invent better solutions. With tiny, local models, I skip cloud overhead, iterate faster, and avoid unwanted surprise bills. Starting at the bottom of &lt;a href="https://openai.com/index/scaling-laws-for-neural-language-models/" target="_blank" rel="noopener">the scale curve&lt;/a> is liberating: any capability I unlock here will only get stronger as the model scales. And in the end, the setup stays small, but the possibilities donâ€™t.&lt;/p>
&lt;p>Iâ€™m excited to keep iterating on this work. If you want to dig into the details, the full implementation is on &lt;a href="https://github.com/bellanich/pocket-diffusion" target="_blank" rel="noopener">GitHub&lt;/a>. Questions, feedback, or wild ideas? Drop a comment or reach out on &lt;a href="https://www.linkedin.com/in/bella-nicholson/" target="_blank" rel="noopener">LinkedIn&lt;/a>. I always enjoy meeting people working at the edge of what&amp;rsquo;s possible â€” on-device or in the cloud.&lt;/p></description></item><item><title>Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion</title><link>https://bellanich.github.io/post/edge-diffusion-2/</link><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-diffusion-2/</guid><description>&lt;!-- Welcome ðŸ‘‹ -->
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;!-- - [Table of Contents](#table-of-contents) -->
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#apples-optimization-strategy">Apple&amp;rsquo;s Optimization Strategy&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#model-compression">Model Compression&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#phase-1-quantization">Phase 1: Quantization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#phase-2-palettization">Phase 2: Palettization&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#neural-engine-optimization">Neural Engine Optimization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#model-chunking">Model Chunking&lt;/a>&lt;/li>
&lt;li>&lt;a href="#attention-variants">Attention Variants&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#1-the-original-attention-mechanism">1. The Original Attention Mechanism&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-split-einsum-attention">2. Split Einsum Attention&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#scheduler-optimization">Scheduler Optimization&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#whats-next">What&amp;rsquo;s next?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>After tackling text with &lt;a href="../../portfolio/edge-llm/">my edge multi-modal LLM project&lt;/a> last year, I&amp;rsquo;ve become fascinated with the image side of foundation models. The media frenzy ignited by &lt;a href="https://deepmind.google/models/gemini-image/" target="_blank" rel="noopener">Google DeepMind&amp;rsquo;s initial Nano Banana release&lt;/a> acts as a testament to how image generation just hits differently than text.&lt;/p>
&lt;p>This begs the ultimate question: Can we deliver a Nano Banana-like experience on the edge? The answer isn&amp;rsquo;t simple. Diffusion models are brutally sensitive to noise. In &lt;a href="../edge-diffusion-1/">my last blog post&lt;/a>, &lt;a href="../edge-diffusion-1/#tiny-sd-starting-as-small-as-possible">my naive port of Tiny SD failed spectacularly&lt;/a>, yielding noisy and psychedelic outputs.&lt;/p>
&lt;p>As a result, I&amp;rsquo;ve turned my attention to &lt;a href="https://huggingface.co/apple/coreml-stable-diffusion-v1-5" target="_blank" rel="noopener">Apple&amp;rsquo;s CoreML Stable Diffusion&lt;/a> model, betting its proprietary, hardware-aware design will save this project. How did Apple&amp;rsquo;s engineers successfully squeeze a 6GB model into a sub-2GB, sub-10-second iOS package while maintaining quality? Their secret lies in the careful orchestration of quantization, hardware co-design, and architectural compromises.&lt;/p>
&lt;figure>
&lt;img src="images/coreml-quick-test.png" style="width: 40%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 1.&lt;/strong> Prototyping testing. A generated image from the text prompt &lt;code>"A beautiful landscape with mountains and a lake, golden hour lighting"&lt;/code> using Appleâ€™s CoreML Stable Diffusion model. The output is a high-quality, realistic rendering of a rustic mountain range at sunset.
&lt;/figcaption>
&lt;/figure>
&lt;p>This post dissects exactly how Apple pulled off that optimization miracle. By examining which layers got quantized, how Neural Engine constraints shaped the architecture, and where the remaining quality trade-offs live, we&amp;rsquo;ll be ready to build our edge conditional image generation experience in &lt;a href="../edge-diffusion-3/">this blogpost series&amp;rsquo;s conclusion&lt;/a>.&lt;/p>
&lt;h2 id="apples-optimization-strategy">Apple&amp;rsquo;s Optimization Strategy&lt;/h2>
&lt;p>Let&amp;rsquo;s start with the numbers. Apple took &lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5" target="_blank" rel="noopener">Runway ML&amp;rsquo;s 6GB Stable Diffusion&lt;/a> implementation, compressed it by over 70%, and then embedded it onto their proprietary Apple Neural Engine (ANE) hardware. The ANE is the third processor in Apple Silicon, sitting alongside the standard CPU and GPU. It&amp;rsquo;s a dedicated Neural Processing Unit (NPU) purpose-built for the high-throughput matrix operations that define neural network inference.&lt;/p>
&lt;figure>
&lt;img src="images/original-sd-demo.png" style="width: 50%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 2.&lt;/strong> An image by generated by the Runway ML's 6GB Stable Diffusion model, which served as the base model for Apple's CoreML implementation (&lt;a href="https://deepinfra.com/runwayml/stable-diffusion-v1-5/api">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="images/coreml_readme.png" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 3.&lt;/strong> Sample images from Apple's official CoreML Stable Diffusion demo. As seen, there's some quality degradation when compared to Fig. 2, but the majority of model quality is preserved (&lt;a href="https://github.com/apple/ml-stable-diffusion">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>This hyper-specialized hardware enabled Apple to achieve blisteringly fast results: each denoising step takes only ~0.37â€“0.39 seconds and a high-quality images are generated within 20 steps (documented &lt;a href="https://github.com/apple/ml-stable-diffusion" target="_blank" rel="noopener">here&lt;/a>). Of course, Apple didn&amp;rsquo;t use a single optimization technique to achieves these impressive runtimes; rather, they employed an entire optimization playbook:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Model precision reduction.&lt;/strong> Apple applies a crucial two-step precision reduction process of &lt;strong>(a)&lt;/strong> initial quantization from &lt;code>float32&lt;/code> to &lt;code>float16&lt;/code> to halve memory consumption and &lt;strong>(b)&lt;/strong> followed by aggressive palettization to &lt;code>6-bit&lt;/code> weights to yield another ~40-50% reduction. That&amp;rsquo;s how we went from a 6GB model to a 1.5GB one.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Neural Engine optimization.&lt;/strong> CoreML&amp;rsquo;s ANE-specific compilation pipeline fuses common machine learning operators and optimizes tensor memory layouts for the ANE&amp;rsquo;s specialized compute units. &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers" target="_blank" rel="noopener">Apple&amp;rsquo;s own benchmarks&lt;/a> show that ANE-optimized models achieve up to 10Ã— speedup with 14Ã— reduction in peak memory consumption compared to unoptimized implementations on the iPhone 13.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Model chunking.&lt;/strong> The pipeline is split into four independently loadable components (&lt;code>.mlmodelc&lt;/code> files). iOS dynamically swaps these components in the &lt;code>reduceMemory&lt;/code> option to keep peak memory usage below the 2GB limit. The trade-off is increased end-to-end latency due to this just-in-time loading overhead as seen in their &lt;a href="https://github.com/apple/ml-stable-diffusion#performance-benchmark" target="_blank" rel="noopener">official benchmarks&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Attention implementation.&lt;/strong> Apple uses a &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers" target="_blank" rel="noopener">SPLIT_EINSUM attention variant&lt;/a>. It breaks multi-head attention into explicit single-head functions and relies on einsum operations to avoid the reshape and transpose steps that trigger memory copies. Because the ANE excels at fixed 4D tensor layouts, keeping data in this shape is crucial. Combined with Appleâ€™s other optimizations, this approach delivered up to &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers" target="_blank" rel="noopener">10Ã— faster inference and 14Ã— lower peak memory on their distilbert benchmark&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Scheduler Optimization.&lt;/strong> The original model was used &lt;a href="https://arxiv.org/abs/2202.09778" target="_blank" rel="noopener">the PNDM (Pseudo-numerical methods for Denoising Models) scheduler&lt;/a>, which requires 50+ computational steps. Apple swapped this for the modern &lt;a href="https://arxiv.org/abs/2211.01095" target="_blank" rel="noopener">DPMSolverMultistepScheduler&lt;/a>. PNDM uses estimates from recent steps to predict the denoising direction, allowing it to learn from recent history to make smarter jumps. This drastically reduces the required denoising steps from &lt;a href="https://arxiv.org/abs/2211.01095" target="_blank" rel="noopener">50+ to 20-25 without sacrificing image quality&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Each optimization technique targets a different performance bottleneck, whether it be memory footprint, inference latency, or peak memory usage. Let&amp;rsquo;s now examine each technique in detail.&lt;/p>
&lt;h3 id="model-compression">Model Compression&lt;/h3>
&lt;p>Apple engineers pulled off an impressive 70% size reduction through a two-phase compression strategy. They started with a straightforward precision reduction from &lt;code>float32&lt;/code> to &lt;code>float16&lt;/code> and followed up with palettization to jump from &lt;code>float16&lt;/code> to just &lt;code>6-bit&lt;/code>.&lt;/p>
&lt;h4 id="phase-1-quantization">Phase 1: Quantization&lt;/h4>
&lt;p>Model parameters in standard PyTorch models consume 32 bits (float32), making the 1.3B parameter Stable Diffusion consume 5.2GB of RAM. In &lt;strong>quantization&lt;/strong>, we lower the bit-precision used to store each weight, sacrificing some information for a smaller memory footprint.&lt;/p>
&lt;p>The first step is to halve the precision to &lt;code>float16&lt;/code>. CoreML achieved this using simple datatype casting (no algorithmic quantization, clustering, or lookup tables), which instantly halves memory with minimal accuracy loss. This simple technique works because
&lt;a href="https://apple.github.io/coremltools/docs-guides/source/quantization-neural-network.html" target="_blank" rel="noopener">float16 is considered the &amp;ldquo;safe&amp;rdquo; floor&lt;/a> for neural networks. This precision level retains enough dynamic range (the span of representable values) and precision to accurately encode most model weights.&lt;/p>
&lt;p>This reduces memory consumption from 5.2GB to 2.6GB, which is a great start but above my sub-2GB target. If we naively lower the precision any further, we risk catastrophic information loss and would get corrupted outputs like those from my Tiny SD port. Meaning, we need a more clever quantization technique.&lt;/p>
&lt;h4 id="phase-2-palettization">Phase 2: Palettization&lt;/h4>
&lt;p>Apple wanted to build a generalizable framework for compress &lt;em>any&lt;/em> Stable Diffusion checkpoint, including community fine-tunes. Hence, they needed a flexible approach that didn&amp;rsquo;t depend on access to the original training data. This immediately rules out state-of-the-art methods like &lt;a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener">AWQ&lt;/a> or &lt;a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener">GPTQ&lt;/a>, which require calibration data to analyze activations and identify the most salient (important) weights.&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="noopener">Palettization&lt;/a> offers a perfect alternative. It achieves aggressive compression without calibration data by using simple, interpretable k-means clustering. This technique actually originates from &lt;a href="https://dl.acm.org/doi/10.1145/800064.801294" target="_blank" rel="noopener">color quantization&lt;/a> in computer graphics. Essentially, during image compression, we needed to map millions of possible colors in an image to a smaller fixed set of representative values.&lt;/p>
&lt;figure>
&lt;img src="images/color-quantization.png" style="width: 70%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 4.&lt;/strong> An example of color quantization, where the original photograph palette was reduced seven distinct colors (&lt;a href="https://demonstrations.wolfram.com/ColorQuantizationOfPhotographicImagesIPaletteFromColorsInThe/">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>The same logic can be applied to model weights. Here&amp;rsquo;s how it works:&lt;/p>
&lt;ol>
&lt;li>Analyze the distribution of weights in each layer&lt;/li>
&lt;li>Cluster similar weights using k-means to create a &amp;ldquo;palette&amp;rdquo; of representative values (e.g., 64 values for 6-bit palettization, since $2^6 = 64$)&lt;/li>
&lt;li>Replace each original weight with an index pointing to its nearest palette entry&lt;/li>
&lt;li>Store the compact palette plus the many small indices instead of full-precision weights&lt;/li>
&lt;/ol>
&lt;p>The bit-width determines how many palette entries you get, and thus your compression ratio. Table 1 summarizes the trade-offs in palette entry number selection.&lt;/p>
&lt;style>
.centered-table {
display: flex;
flex-direction: column;
align-items: center;
margin: 2rem 0;
}
.centered-table table {
border-collapse: collapse;
}
.centered-table th, .centered-table td {
padding: 0.5rem 1rem;
border: 1px solid #ddd;
}
.centered-table figcaption {
margin-top: 0.5rem;
font-style: normal;
text-align: center;
}
&lt;/style>
&lt;div class="centered-table">
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Bit Width&lt;/th>
&lt;th>Palette Entries&lt;/th>
&lt;th>Compression vs Float16&lt;/th>
&lt;th>Quality Impact&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8-bit&lt;/td>
&lt;td>256&lt;/td>
&lt;td>~2Ã—&lt;/td>
&lt;td>Minimal quality loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6-bit&lt;/td>
&lt;td>64&lt;/td>
&lt;td>~2.67Ã—&lt;/td>
&lt;td>Acceptable quality trade-off&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4-bit&lt;/td>
&lt;td>16&lt;/td>
&lt;td>~4Ã—&lt;/td>
&lt;td>Noticeable degradation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2-bit&lt;/td>
&lt;td>4 &lt;/td>
&lt;td>~8Ã— &lt;/td>
&lt;td>Severe quality issues&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;figcaption> &lt;strong> Table 1. &lt;/strong> Palettization bit-width options and their associated trade-offs.&lt;/figcaption>
&lt;/div>
&lt;p>For our selected CoreML Stable Diffusion variant, &lt;a href="https://huggingface.co/apple/coreml-stable-diffusion-v1-5-palettized" target="_blank" rel="noopener">Apple used 6-bit palettization&lt;/a> to achieve a final ~1.5GB model size. For even larger models like &lt;a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0" target="_blank" rel="noopener">SDXL&lt;/a> (6.94 GB), Apple used &lt;a href="https://github.com/apple/ml-stable-diffusion/blob/main/README.md#compression-lower-than-6-bits" target="_blank" rel="noopener">mixed-bit palettization&lt;/a> for stronger model compression. Here, we assign different bit-widths (1, 2, 4, 6, or 8 bits) to different layers based on a sensitivity analysis.&lt;/p>
&lt;h3 id="neural-engine-optimization">Neural Engine Optimization&lt;/h3>
&lt;p>Apple&amp;rsquo;s Neural Engine (NE) debuted &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers" target="_blank" rel="noopener">in 2017 inside the iPhone X&amp;rsquo;s A11 Bionic chip&lt;/a> to power Face ID. &lt;a href="https://support.apple.com/en-us/102381" target="_blank" rel="noopener">The TrueDepth camera fires over 30,000 infrared dots&lt;/a> to map your face, and handles that data in real time. That stream in real time was too slow and too power-hungry for the GPU, pushing Apple to develop their own NPU.&lt;/p>
&lt;p>That &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers" target="_blank" rel="noopener">first-gen ANE delivered 0.6 teraflops of float16&lt;/a> compute. By 2018, with release of &lt;a href="https://www.apple.com/newsroom/2018/09/iphone-xs-and-iphone-xs-max-bring-the-best-and-biggest-displays-to-iphone/" target="_blank" rel="noopener">the A12 chip&lt;/a> and &lt;a href="https://devstreaming-cdn.apple.com/videos/wwdc/2017/710vxa4hl8hyb72/710/710_core_ml_in_depth.pdf" target="_blank" rel="noopener">Core ML&lt;/a>, Apple opened the Neural Engine to developers, and today itâ€™s baked into every modern iOS device.&lt;/p>
&lt;figure>
&lt;img src="images/ane-tflops-stats.png" style="width: 80%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 5.&lt;/strong> The evolution of the Apple Neural Engine from 2017 to 2021. The 16-core Neural Engine on on the A15 Bionic chip (iPhone 13 Pro) has a peak throughput 26 times higher than its original counterpart. (&lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>But where does &lt;a href="https://developer.apple.com/documentation/coreml" target="_blank" rel="noopener">Core ML&lt;/a> fit into all this? Since ANE is proprietary hardware, thereâ€™s no public API to program it directly. Its architecture, instruction set, and compiler are all trade secrets. With no official documentation on ANE-supported operations or optimization methods, most developer knowledge comes from &lt;a href="https://github.com/hollance/neural-engine" target="_blank" rel="noopener">trial-and-error and reverse engineering&lt;/a>. Core ML is the only way iOS developers can access the Neural Engine.&lt;/p>
&lt;p>It consists of two parts:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://opensource.apple.com/projects/coreml-tools/" target="_blank" rel="noopener">coremltools&lt;/a> is an open source Python package that converts models from frameworks like PyTorch and TensorFlow into Core ML&amp;rsquo;s optimized format&lt;/li>
&lt;li>The on-device Core ML framework that loads these compiled models and executes them.&lt;/li>
&lt;/ol>
&lt;p>When you convert a model with &lt;code>coremltools&lt;/code>, it figures out which operations can run on the ANE versus the CPU or GPU, applies optimizations, and compiles the model into an efficient format. At runtime, Core ML then routes each operation to the right compute unit to maximize performance and minimize power use.&lt;/p>
&lt;p>CoreML gives you three ways to run unit neural networks on device:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>CPU Only.&lt;/strong> This is the slowest but most safest option. According to the &lt;a href="https://onnxruntime.ai/docs/execution-providers/CoreML-ExecutionProvider.html#coreml_flag_use_cpu_only" target="_blank" rel="noopener">ONNX Runtime documentation&lt;/a>, CPU-only mode is mainly available for debugging and validation, since it avoids precision differences and guarantees predictable results. Community benchmarks suggest it runs approximately &lt;a href="https://mybyways.com/blog/faster-stable-diffusion-on-mseries-macs" target="_blank" rel="noopener">7-8x slower&lt;/a> than optimal configurations, making it impractical for real-time generation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>CPU and GPU.&lt;/strong> This combination is capable but not recommended. GPUs were originally built for desktops with unlimited power, so theyâ€™re plausible but not ideal for running heavy models on mobile devices. It&amp;rsquo;s typically used for Macs with powerful GPUs or as a fallback for older devices without a Neural Engine.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>CPU ane ANE.&lt;/strong> This is Apple&amp;rsquo;s recommended configuration for deploying intensive models on iPhones and iPads. ANE was specifically designed for ML inference workloads and delivers comparable performance to GPU at a fraction of the power consumption.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In our case, I configured &lt;code>coreml-stable-diffusion-v1-5-palettized&lt;/code> to run primarily on the Neural Engine with CPU fallback for unsupported operations. This hybrid approach maximizes performance where it counts while maintaining graceful degradation for edge cases.&lt;/p>
&lt;h3 id="model-chunking">Model Chunking&lt;/h3>
&lt;p>iOS enforces stricter memory constraints than macOS. As noted in &lt;a href="https://apple.github.io/coremltools/docs-guides/source/opt-stable-diffusion.html" target="_blank" rel="noopener">Apple&amp;rsquo;s CoreML optimization guide&lt;/a>,&lt;/p>
&lt;p>ANE&amp;rsquo;s specialized architecture comes with strict model size constraints. iOS enforces stricter per-file memory mapping limits than macOS. While the exact limit is undocumented, &lt;a href="https://apple.github.io/coremltools/docs-guides/source/opt-stable-diffusion.html" target="_blank" rel="noopener">Apple&amp;rsquo;s optimization guide&lt;/a> suggests it&amp;rsquo;s around 1GB based on their compression targets for mobile deployment. Meaning, attempting to load our U-Net component, which is roughly 1.5GB in float16 precision, will iPhone triggers memory allocation failure, even if it&amp;rsquo;d run perfectly on a Mac.&lt;/p>
&lt;p>This makes &lt;a href="https://github.com/apple/ml-stable-diffusion?tab=readme-ov-file#-converting-models-to-core-ml" target="_blank" rel="noopener">model chunking&lt;/a> essential for mobile deployment. The idea is simple: we split huge weight files into smaller slices that fit within iOSâ€™s memory limits, and let the runtime load each slice on demand. Apple&amp;rsquo;s &lt;code>ml-stable-diffusion&lt;/code> repo handles this automatically with the &lt;a href="https://github.com/apple/ml-stable-diffusion" target="_blank" rel="noopener">&lt;code>--chunk-unet&lt;/code> conversion flag&lt;/a> flag, which divides the U-Net weights into multiple files that stay well under the limit. These chunks are stored in the &lt;code>.mlmodelc&lt;/code> format, a pre-compiled, ANE-optimized layout that improve loading time.&lt;/p>
&lt;p>The beauty of Apple&amp;rsquo;s setup is that developers never have to think about model chunking. Core ML handles this behind the scenes. While there&amp;rsquo;s a small cost to pulling in multiple files, we wouldn&amp;rsquo;t be able to run Stable Diffusion on iOS without this approach.&lt;/p>
&lt;h3 id="attention-variants">Attention Variants&lt;/h3>
&lt;p>Apple&amp;rsquo;s CoreML conversion tools offer two attention implementations that compute identical mathematical operations but differ critically in their kernel implementation.&lt;/p>
&lt;h4 id="1-the-original-attention-mechanism">1. The Original Attention Mechanism&lt;/h4>
&lt;p>This implementation uses the standard batched multi-head attention formula:&lt;/p>
&lt;pre>&lt;code class="language-python"># Shape: [batch, seq_len, heads * head_dim]
Q, K, V = linear_projections(x)
# Reshape to [batch, heads, seq_len, head_dim]
Q = Q.reshape(batch, seq_len, heads, head_dim).transpose(1, 2)
K = K.reshape(batch, seq_len, heads, head_dim).transpose(1, 2)
V = V.reshape(batch, seq_len, heads, head_dim).transpose(1, 2)
# Batched matrix multiplication across all heads
# [batch, heads, seq_len, head_dim]
attention = softmax(Q @ K.transpose(-2, -1) / sqrt(d_k)) @ V
# Reshape back
output = attention.transpose(1, 2).reshape(batch, seq_len, heads * head_dim)
&lt;/code>&lt;/pre>
&lt;p style="text-align: center; font-size: 0.75em; margin-top: 0.5em; color: #66;">
&lt;strong>Figure 6.&lt;/strong> Pseudocode of the &lt;a href="https://arxiv.org/abs/1706.03762">original attention&lt;/a> mechanism.
&lt;/p>
&lt;p>This works well on CPUs and GPUs, which handle dynamic reshaping efficiently. It&amp;rsquo;s &lt;a href="https://github.com/apple/ml-stable-diffusion#performance-benchmark" target="_blank" rel="noopener">faster on Macs with discrete GPUs&lt;/a> (M1 Pro/Max/Ultra) where memory bandwidth isn&amp;rsquo;t the primary bottleneck.&lt;/p>
&lt;h4 id="2-split-einsum-attention">2. Split Einsum Attention&lt;/h4>
&lt;p>ANE penalizes non-contiguous memory access, which makes the reshape/transpose operations shown in Figure 6 computationally expensive. Fortunately, we can rewrite matrix multiplication as a series of &lt;a href="https://mathworld.wolfram.com/EinsteinSummation.html" target="_blank" rel="noopener">Einstein summations&lt;/a> (einsums) as shown in Equation 1 to better utilize ANE.
$$
C_{ik} = \sum_{j} A_{ij} B_{jk} = AB = C
\tag{1}
$$&lt;/p>
&lt;p>By keeping keeps tensors in fixed 3D layouts and using the einsum operation, we avoid generating unnecessary memory copies. The implementation looks something like this:&lt;/p>
&lt;pre>&lt;code class="language-python"># Shape: [batch, seq_len, heads * head_dim]
Q, K, V = linear_projections(x)
# Split into explicit per-head tensors (no reshape)
Q_heads = [Q[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)]
K_heads = [K[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)]
V_heads = [V[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)]
# Compute attention per head using einsum (preserves 3D tensor layout)
outputs = []
for Q_h, K_h, V_h in zip(Q_heads, K_heads, V_heads):
# [batch, seq_len, seq_len]
scores = torch.einsum('bqd,bkd-&amp;gt;bqk', Q_h, K_h) / sqrt(head_dim)
attn = softmax(scores, dim=-1)
out = torch.einsum('bqk,bkd-&amp;gt;bqd', attn, V_h) # [batch, seq_len, head_dim]
outputs.append(out)
# Concatenate (cheap operation)
output = torch.cat(outputs, dim=-1) # [batch, seq_len, heads * head_dim]
&lt;/code>&lt;/pre>
&lt;p style="text-align: center; font-size: 0.75em; margin-top: 0.5em; color: #66;">
&lt;strong>Figure 7.&lt;/strong> Pseudocode of the &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers">einsum attention variant&lt;/a>.
&lt;/p>
&lt;p>The trade-off is lower parallelism, since we&amp;rsquo;re using explicit per-head loops rather than batched operations. This hurts GPU performance, but &lt;a href="https://machinelearning.apple.com/research/neural-engine-transformers" target="_blank" rel="noopener">ANE performance is bottlenecked by memory bandwidth&lt;/a>. Meaning, the memory savings outweigh the costs of reduced parallelism.&lt;/p>
&lt;h3 id="scheduler-optimization">Scheduler Optimization&lt;/h3>
&lt;p>Diffusion models turn random static into art by clearing away noise. The &lt;strong>scheduler&lt;/strong> (also called a sampler or solver) is the control algorithm that orchestrates the denoising loop: it calls the U-Net at each timestep to predict the noise, then uses its mathematical formula to update the image toward a cleaner state. Meaning, the scheduler controls the number of diffusion steps needed for high-quality image generation. If we select a more efficient scheduler, we can improve inference time without degrading quality.&lt;/p>
&lt;p>The &lt;a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" target="_blank" rel="noopener">original Stable Diffusion models&lt;/a> used the &lt;a href="https://arxiv.org/abs/2202.09778" target="_blank" rel="noopener">PNDM (Pseudo Numerical Methods for Diffusion Models)&lt;/a> scheduler, which applies a linear multi-step method:&lt;/p>
&lt;p>$$
x_{t-1} = x_t + \sum_{i=0}^{k-1} \alpha_i \cdot \epsilon_\theta(x_{t-i}, t-i)
\tag{2}
$$&lt;/p>
&lt;p>where $x_t$ is the current noisy image at timestep $t$, $\epsilon_\theta$ predicts what noise to remove, and $\alpha_i$ are coefficients that weight predictions from the past $k$ steps. As seen in Equation 2, PNDM treats each timestep as a discrete prediction problem, where each step uses local information (the last few predictions). In this context, larger jumps (more noise removal per step) risk error accumulation, which lowers image quality. &lt;a href="https://arxiv.org/abs/2202.09778" target="_blank" rel="noopener">PNDM tends to require ~50 diffusion steps to yield acceptable outputs.&lt;/a>&lt;/p>
&lt;p>The &lt;a href="https://arxiv.org/abs/2211.01095" target="_blank" rel="noopener">DPMSolverMultistepScheduler&lt;/a> treats denoising as a continuous process rather than discrete jumps. Since noise is added gradually during training, it can be removed along a smooth, continuous path that written as an Ordinary Differential Equation (ODE):&lt;/p>
&lt;p>$$
\frac{dx_t}{dt} = f(t) x_t + g(t) \epsilon_\theta(x_t, t)
\tag{3}
$$&lt;/p>
&lt;p>This makes diffusion a continuous process and allows the DPM Solver to take larger, more informed steps through the denoising trajectory. As a result, &lt;a href="https://arxiv.org/abs/2211.01095" target="_blank" rel="noopener">25 steps with DPM Solver produces quality comparable&lt;/a> to 50 steps with PNDM, offering a 2Ã— speedup.&lt;/p>
&lt;p>This made it the DPM Solver an obvious choice for Apple&amp;rsquo;s CoreML implementation, where every second of latency matters. The step count creates a direct quality-speed trade-off:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Strategy&lt;/th>
&lt;th>Steps&lt;/th>
&lt;th>Runtime&lt;/th>
&lt;th>Quality Impact&lt;/th>
&lt;th>Use Case&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Aggressive&lt;/strong>&lt;/td>
&lt;td>15-20&lt;/td>
&lt;td>10-15 seconds&lt;/td>
&lt;td>Noticeable artifacts, loss of fine details&lt;/td>
&lt;td>Quick previews, concept iteration&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Balanced&lt;/strong>&lt;/td>
&lt;td>20-30&lt;/td>
&lt;td>15-25 seconds&lt;/td>
&lt;td>High-quality results, minimal artifacts&lt;/td>
&lt;td>Production deployment&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Conservative&lt;/strong>&lt;/td>
&lt;td>50+&lt;/td>
&lt;td>35+ seconds&lt;/td>
&lt;td>Marginal improvement over 25 steps&lt;/td>
&lt;td>Not worth the extra latency on mobile&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p style="text-align: center; font-size: 0.75em; margin-top: 0.5em; color: #66;">
&lt;strong>Table 2.&lt;/strong> Trade-off between diffusion steps and image quality when using the DPMSolverMultistepScheduler.
&lt;/p>
&lt;p>After extensive testing, I settled on 25 steps as my default to properly balance my need for quality and speed.&lt;/p>
&lt;br/>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Apple&amp;rsquo;s CoreML Stable Diffusion represents a masterclass in optimization engineering. With aggressive quantization, ANE-friendly attention kernels, and smart scheduling, Apple squeezed a 6GB model into a 1.5GB package that can generate an image in under 10 seconds on an iPhone. It&amp;rsquo;s a technical flex that&amp;rsquo;s hard to overstate.&lt;/p>
&lt;p>But here&amp;rsquo;s an uncomfortable truth: optimization doesnâ€™t expand capabilities. Apple solved &lt;em>how&lt;/em> to run Stable Diffusion on mobile â€” not whether it&amp;rsquo;s good enough. Strip away the speedups, and we&amp;rsquo;re left with the a 2022-era model:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Poor identity preservation.&lt;/strong> Try img2img at high denoising strength and watch faces dissolve into uncanny abstractions. The model simply can&amp;rsquo;t maintain coherent identity while delivering image transformation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prompt adherence is weak.&lt;/strong> Compared to &lt;a href="https://stablediffusionxl.com/" target="_blank" rel="noopener">SDXL&lt;/a> or &lt;a href="https://bfl.ai/models/flux-pro" target="_blank" rel="noopener">Flux&lt;/a>, SD 1.5 treats our carefully crafted prompt more like a vague suggestion than a clear set of instructions.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>As a result, I can&amp;rsquo;t just port Apple&amp;rsquo;s approach.&lt;/p>
&lt;h3 id="whats-next">What&amp;rsquo;s next?&lt;/h3>
&lt;p>My goal is conditional edge image generation with an explicit need for character consistency. If Apple&amp;rsquo;s optimizations give us the blueprint for mobile deployment, what architecture actually delivers my required capabilities?&lt;/p>
&lt;p>That&amp;rsquo;s what I&amp;rsquo;ll cover in &lt;a href="../edge-diffusion-3/">my next and final blogpost&lt;/a>. My goal isnâ€™t just to run fast; itâ€™s to run fast and look good doing it.&lt;/p></description></item><item><title>Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model</title><link>https://bellanich.github.io/post/edge-diffusion-1/</link><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-diffusion-1/</guid><description>&lt;!-- Welcome ðŸ‘‹ -->
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;!-- - [Table of Contents](#table-of-contents) -->
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem-constraints">Problem Constraints&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#beyond-the-noise-unpacking-the-architecture-of-diffusion-models">Beyond the Noise: Unpacking the Architecture of Diffusion Models&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#model-architecture">Model Architecture&lt;/a>&lt;/li>
&lt;li>&lt;a href="#text-to-image-vs-image-to-image-generation">Text-to-Image vs. Image-to-Image Generation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#nano-banana-todays-state-of-the-art">Nano Banana: Today&amp;rsquo;s State of the Art&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#the-model-search">The Model Search&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#tiny-sd-starting-as-small-as-possible">Tiny SD: Starting as Small as Possible&lt;/a>&lt;/li>
&lt;li>&lt;a href="#coreml-stable-diffusion">CoreML Stable Diffusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#whats-next">What&amp;rsquo;s next?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I have a problem: I love testing out and applying the latest ML research, but I really dislike managing my own cloud infrastructure. That&amp;rsquo;s why I ended up &lt;a href="../../portfolio//edge-llm/">embedding a multimodal LLM on various edge devices&lt;/a> last year. However, generated text doesn&amp;rsquo;t deliver the same immediate, visceral impact that high-quality images do, compelling me to switch domains.&lt;/p>
&lt;p>Unfortunately, deploying a state-of-the-art (SOTA) diffusion model on the edge is far harder than its LLM counterpart. LLMs work in a discrete output space (i.e., tokens). Thus, they tolerate the noise of simple compression algorithms relatively well. In contrast, diffusion models operate on a continuous, dense latent space, causing the same amount of noise to more severely degradate model performance. Attempting to shrink a 4GB model to fit the standard 2GB iOS memory budget is a brutal performance problem. For better or worse, this is my favorite type of problem to solve.&lt;/p>
&lt;h3 id="problem-constraints">Problem Constraints&lt;/h3>
&lt;p>To keep things interesting, I decided to target deployment directly for my iPhone 16 (~8GB of RAM). If this model can run effectively on my phone, I&amp;rsquo;ll always have a tiny, powerful image generator right in my pocket. However, this choice immediately imposed a very strict iOS memory budget. iOS apps encounter a dynamic limit from ~2-4 GB (roughly &lt;a href="https://developer.apple.com/documentation/coreml" target="_blank" rel="noopener">50-70% of total RAM&lt;/a>) that triggers an &lt;code>EXC_RESOURCE RESOURCE_TYPE_MEMORY&lt;/code> termination exception and crashes the app.&lt;/p>
&lt;p>Of course, compressing the model is only half the battle. If it&amp;rsquo;s too slow, any reasonable user will just quit the app, rendering the entire point moot. Hence, I set a 60-second end-to-end limit for the pipeline, allotting 40 seconds for model inference.&lt;/p>
&lt;p>This high bar for speed and precision demanded a pipeline built around conditional image generation (Image-to-Image or Img2Img). This is essential because it:&lt;/p>
&lt;ol>
&lt;li>Provides superior creative control and output fidelity.&lt;/li>
&lt;li>Elevates the ML-side challenge by requiring hands-on control of the model&amp;rsquo;s neural network sub-components&lt;/li>
&lt;li>Delivers a more engaging user experience by actively transforming the source photo&lt;/li>
&lt;/ol>
&lt;p>The final, non-negotiable rule was that the output images needed to be of reasonable quality. Meaning, the model needs to generate easily identifiable objects that are in-line with the provided text prompt.&lt;/p>
&lt;h2 id="beyond-the-noise-unpacking-the-architecture-of-diffusion-models">Beyond the Noise: Unpacking the Architecture of Diffusion Models&lt;/h2>
&lt;p>The path to modern image generation was surprisingly quick. &lt;a href="https://openai.com/index/dall-e-2/" target="_blank" rel="noopener">Open AI released DALL-E in January 2021&lt;/a>, &lt;a href="https://stability.ai/news/stable-diffusion-public-release" target="_blank" rel="noopener">Stable Diffusion democratized the field in August 2022&lt;/a>, and suddenly everyone had access to conditional image synthesis.&lt;/p>
&lt;p>This new era of vision models is powered by &lt;a href="https://arxiv.org/pdf/2006.11239" target="_blank" rel="noopener">diffusion&lt;/a>, where the model learns to destroy images systematically and then reverses the process.&lt;/p>
&lt;figure>
&lt;img src="images/noise_to_image.jpg" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 1.&lt;/strong> A visualization of the diffusion process. Noise is added to (forward pass) or removed from the image (reverse processes) (&lt;a href="https://www.siam.org/publications/siam-news/articles/generalization-of-diffusion-models-principles-theory-and-implications/">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>We start with the original image $x_0$ and progressively corrupt it by adding Gaussian noise over $T$ timesteps. At each step, we keep some fraction of the previous image and add fresh noise:&lt;/p>
&lt;p>$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)$$&lt;/p>
&lt;p>where $\beta_t$ controls the noise intensity. This defines a &lt;strong>Markov chain&lt;/strong>, a sequence of random states where each state $x_t$ depends only on the immediately previous state $x_{t-1}$, nothing earlier.&lt;/p>
&lt;p>This Markov property ensures that while each image follows a different random path to noise, all images at timestep t share identical noise statistics (same signal-to-noise ratio). This predictable structure lets us train a neural network to predict the noise $\epsilon$ added at any timestep, which we can then subtract to reverse the corruption. Once trained, the model generates images by starting with pure noise and iteratively denoising over 20-50 steps, guided by a text prompt.&lt;/p>
&lt;p>During training, we need noisy images at various timesteps to teach the model this denoising function. Stepping through $x_1 \rightarrow x_2 \rightarrow \cdots \rightarrow x_t$ sequentially for every training sample would be computationally infeasible. Fortunately, a key mathematical property of Markov chains with Gaussian transitions is that the entire sequence collapses into a closed-form solution:&lt;/p>
&lt;p>$$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon $$&lt;/p>
&lt;p>where $\epsilon \sim \mathcal{N}(0, I)$ is random noise and $\bar{\alpha}_t = \prod_{i=1}^{t}(1 - \beta_i)$ accumulates all the noise scaling factors up to time $t$. This &lt;strong>reparameterization trick&lt;/strong> lets us jump directly to any timestep in one shot, making model training computationally feasible&lt;/p>
&lt;h3 id="model-architecture">Model Architecture&lt;/h3>
&lt;p>The &lt;a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener">original diffusion paper&lt;/a> introduced the denoising process, but it operated directly on pixels, making it computationally expensive. &lt;a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener">Stable Diffusion&lt;/a> changed the game by running diffusion in a compressed latent space, which dramatically reduces computational costs while maintaining quality.&lt;/p>
&lt;figure>
&lt;img src="images/stable-diffusion-components.png" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 2.&lt;/strong> A visualization of how Stable Diffusion's three neural components work together to do text-to-image generation. In image-to-image generation, a vision encoder also maps the original image to a latent space representation (&lt;a href="https://jalammar.github.io/illustrated-stable-diffusion/">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Stable Diffusion is a composite of four neural networks:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Text encoder.&lt;/strong> We use a &lt;a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener">Contrastive Language-Image Pre-training (CLIP)&lt;/a> network to convert text prompts into $77 \times 768$ embedding vectors. These embeddings semantically link language to visual concepts, allowing our model to &amp;ldquo;understand&amp;rdquo; our text inputs.&lt;/li>
&lt;li>&lt;strong>Image encoder.&lt;/strong> We use a &lt;a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="noopener">Variational Autoencoder Encoder (VAE)&lt;/a> to compress images from pixel space ($3 \times H \times W$) to a compact latent space ($4 \times \frac{H}{8} \times \frac{W}{8}$). This ~64Ã— compression is the key efficiency innovation, since it lets us denoise the compressed representations rather than the full-resolution images.&lt;/li>
&lt;li>&lt;strong>Image denoiser.&lt;/strong> The &lt;a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-Net&lt;/a> is the core component that implements the reverse diffusion process. It accepts noisy latents, the current timestep, and text embeddings (via &lt;a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">cross-attention&lt;/a>) to predict what noise to remove.
&lt;ul>
&lt;li>This is the model&amp;rsquo;s heaviest component, consuming &lt;a href="https://github.com/CompVis/stable-diffusion" target="_blank" rel="noopener">roughly 85% of the Stable Diffusion&amp;rsquo;s total memory footprint&lt;/a>. The immense size is mandatory because the U-Net must model a universal, continuous denoising function spanning all timesteps and image content. This dense predictive complexity is precisely why the U-Net resists compression.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Image decoder.&lt;/strong> The VAE decoder decompresses our latents back to high-resolution pixel-space images. This reconstruction is the final output that the model returns.&lt;/li>
&lt;/ol>
&lt;p>Even with these efficiency gains, deploying Stable Diffusion on mobile devices requires further aggressive but non-destructive U-Net compression. This architecture supports two distinct generation modes, each with different performance characteristics.&lt;/p>
&lt;h3 id="text-to-image-vs-image-to-image-generation">Text-to-Image vs. Image-to-Image Generation&lt;/h3>
&lt;p>Stable Diffusion supports two generation modes, each starting from a different point in the noise spectrum:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Text-to-Image (T2I)&lt;/strong> starts with pure random noise and relies solely on the text prompt to guide generation. This maximizes the model&amp;rsquo;s creative freedom, but means we get little control over the final image&amp;rsquo;s structure or composition.&lt;/li>
&lt;li>&lt;strong>Image-to-Image (Img2Img)&lt;/strong> adds noise to an existing image&amp;rsquo;s latent representation, then denoises it while being guided by both the original image structure and a text prompt. This trades creative flexibility for precise control over image composition.
&lt;ul>
&lt;li>The &lt;a href="https://github.com/CompVis/stable-diffusion/blob/main/README.md#image-modification-with-stable-diffusion" target="_blank" rel="noopener">strength parameter&lt;/a> sets how aggressively the model transforms its input. At $0.0$, the model returns the original image untouched. At $1.0$, the input is completely replaced with noise, making the output nearly identical to Text-to-Image generation.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>These two modes establish the foundational mechanics of image creation, but what happens when you scale that process to an unbelievable level of fidelity and control? That&amp;rsquo;s where Nano Banana Pro enters the frame.&lt;/p>
&lt;h3 id="nano-banana-todays-state-of-the-art">Nano Banana: Today&amp;rsquo;s State of the Art&lt;/h3>
&lt;p>In August 2025, Google DeepMind released &lt;a href="https://aistudio.google.com/models/gemini-2-5-flash-image" target="_blank" rel="noopener">Nano Banana&lt;/a>, a Gemini model natively generates interleaved text and images. It excels at image editing thanks to its strong character-consistency across different image edits. And unlike most image-generation models, which still benefited from long, detailed prompts, Nano Banana performs well with simple instructions.&lt;/p>
&lt;figure>
&lt;img src="images/nano-banana-figurine.png" style="width: 60%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 3.&lt;/strong> An example of the viral "Nano Banana" trend, where users generated figures representations of themselves and their favorite film characters, including James Bond (&lt;a href="https://www.instagram.com/p/DOiy0wciShc/">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>These capabilities were striking enough to &lt;a href="https://www.cnbc.com/2025/11/20/google-nano-banana-pro-gemini-3.html" target="_blank" rel="noopener">spark a viral and global social media trend&lt;/a>, where users turned themselves into figurines (Figure 3).&lt;/p>
&lt;p>Three months later, &lt;a href="https://deepmind.google/models/gemini-image/pro/" target="_blank" rel="noopener">Nano Banana Pro&lt;/a> arrived. It extended its predecessorâ€™s multi-character consistency to handle scenes with 10+ people (Figure 4) and dramatically improved text rendering, enabling designer-level infographics to be generated in minutes.&lt;/p>
&lt;figure>
&lt;img src="images/nano_banana_consistency.png" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 4.&lt;/strong> Nano Banana Pro demonstrates an uncanny level of character consistency with its ability to merge 14 distinct, cute, and fuzzy characters into a cohesive scene (&lt;a href="https://blog.google/technology/ai/nano-banana-pro/">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Figure 5 shows one such example: â€œBest Chocolate Around the World: A Global Taste Odyssey,â€ which illustrates how cocoa is grown, processed, and enjoyed across regions.&lt;/p>
&lt;figure>
&lt;img src="images/chocolate_infographic.jpg" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 5.&lt;/strong> Nano Banana Pro can generate beautifully illustrated and ultra-detailed infographic on demand. Consider this delicious example about where cocoa beans are grown and how they're turned into chocolate.
&lt;/figcaption>
&lt;/figure>
&lt;p>However, both models remain closed-source and proprietary, so we can only infer how they work. We know performs some form of planning-style reasoning for image generation, because &lt;a href="https://x.com/karpathy/status/1992655330002817095" target="_blank" rel="noopener">it can solve university level phsyics and chemistry problems&lt;/a> by generating neatly written, correct solutions directly onto a blank exam page. It is, frankly, impressively capable. And since it is built on Gemini, it&amp;rsquo;s also probably too large to run on edge devices, even with aggressive model compression and optimization.&lt;/p>
&lt;figure>
&lt;img src="images/nano-banana-exam.jpeg" style="width: 100%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 6.&lt;/strong> Nano Banana Pro was able to successfully generate the correct solutions, including doodles, for university-level Physics and Chemistry exam questions (&lt;a href="https://x.com/karpathy/status/1992655330002817095/photo/1">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>This leaves us with a clear goal: replicate as much of this functionality as possible using open-source, on-device alternatives. Unfortunately, current mobile-friendly diffusion models more closely resemble 2021â€“2022 Stable Diffusion systems. They can produce coherent images, but they cannot match Nano Bananaâ€™s full range of abilities.&lt;/p>
&lt;br/>
&lt;h2 id="the-model-search">The Model Search&lt;/h2>
&lt;p>Since Nano Banana Pro&amp;rsquo;s advanced capabilities only emerge at massive scale, we have to accept two harsh realities:&lt;/p>
&lt;ol>
&lt;li>We have to rely on open-source, convertible models that often lag 6-12 months behind industry SOTA; and&lt;/li>
&lt;li>The model we choose won&amp;rsquo;t have the same magical coherence of its cloud-scale counterparts.&lt;/li>
&lt;/ol>
&lt;p>Simply put, I need to select a model that&amp;rsquo;s small enough to run on-device but still capable enough to produce usable results. We can translate our earlier &lt;a href="#problem-constraints">problem constraints&lt;/a> into the model specifications shown in Table 1.&lt;/p>
&lt;figure>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Requirement&lt;/th>
&lt;th>Specification&lt;/th>
&lt;th>Reasoning&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Size&lt;/strong>&lt;/td>
&lt;td>Memory footprint &amp;lt;2GB&lt;/td>
&lt;td>iOS apps face strict memory limits (~2-4GB), allocate 2GB primarily for model usage to prevent crashes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Performance&lt;/strong>&lt;/td>
&lt;td>Inference &amp;lt;40 seconds&lt;/td>
&lt;td>Fits within 60-second end-to-end pipeline budget, leaves room for pre/post-processing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Hardware Support&lt;/strong>&lt;/td>
&lt;td>Apple Neural Engine (ANE) optimization required&lt;/td>
&lt;td>Standard Metal GPU processing will be too slow, need leverage the iPhone's built-in AI accelerator&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Methodology&lt;/strong>&lt;/td>
&lt;td>Separate component access (CLIP, U-Net, VAE Encoder/Decoder)&lt;/td>
&lt;td>Conditional image-to-image generation is inherently modular, components must be accessed separate for img2img tasks&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Quality&lt;/strong>&lt;/td>
&lt;td>Maintain human subject identity with high-fidelity&lt;/td>
&lt;td>Core product requirement: failure to maintain character consistency leads to poor user experience.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;figcaption>
&lt;strong>Table 1.&lt;/strong> Model specifications for my edge conditional image generation application
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="tiny-sd-starting-as-small-as-possible">Tiny SD: Starting as Small as Possible&lt;/h3>
&lt;p>To establish a minimum viable quality baseline, I targeted the smallest available contender: &lt;a href="https://huggingface.co/segmind/tiny-sd" target="_blank" rel="noopener">Segmind&amp;rsquo;s Tiny SD&lt;/a>. As a 55% parameter reduction of Stable Diffusion, it is among the most aggressively compressed models from an established maintainer. Since it only consumed half of my tight 2GB iOS memory ceiling, it was the perfect, low-risk candidate to stress-test the absolute lower bound of acceptable quality and performance.&lt;/p>
&lt;p>My next move was optimizing tiny SD for speed. I used &lt;a href="https://developer.apple.com/documentation/coreml" target="_blank" rel="noopener">CoreML&lt;/a> , Apple&amp;rsquo;s dedicated framework for integrating machine learning models into apps, to convert the weights into an &lt;a href="https://github.com/hollance/neural-engine" target="_blank" rel="noopener">Apple Neural Engine (ANE)&lt;/a> optimized format. The ANE is the dedicated hardware accelerator built into Apple Silicon, specifically designed to run on-device neural network inference with superior power efficiency. Meaning, if this works, I will be able to conditionally generate images without killing my phone&amp;rsquo;s battery.&lt;/p>
&lt;p>In my initial test, I wanted to validate the model&amp;rsquo;s basic text-to-image (T2I) generation. To keep this assessment fair, I used the same text prompt Segmind provided in &lt;a href="https://huggingface.co/segmind/tiny-sd" target="_blank" rel="noopener">their model card&lt;/a>: &lt;code>&amp;quot;Portrait of a pretty girl&amp;quot;&lt;/code>. Despite my best efforts to meet the 40-second deadline (via 25-30 diffusion steps), the model failed quality control. Instead of images, I was left with psychedelic noise and low-fidelity artifacts (Figure 7).&lt;/p>
&lt;figure style="display: flex; flex-direction: column; align-items: center; text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 10px;">
&lt;div style="width: 45%;">
&lt;img src="images/tiny_sd_simple_prompt.png" style="width: 100%;">
&lt;div>(a)&lt;/div>
&lt;/div>
&lt;div style="width: 45%;">
&lt;img src="images/tiny_sd_elaborate_prompt.png" style="width: 100%;">
&lt;div>(b)&lt;/div>
&lt;/div>
&lt;/div>
&lt;figcaption>
&lt;strong> Figure 7. &lt;/strong> The samples illustrate Tiny SD's quality collapse. &lt;strong>(a)&lt;/strong> Default prompt at recommended &lt;a href="https://huggingface.co/docs/diffusers/en/using-diffusers/write_your_own_pipeline#classifier-free-guidance">guidance scale&lt;/a> (7.5). &lt;strong>(b)&lt;/strong> Enhanced prompt and an aggressive guidance (11.0) to force better prompt adherence. Both outputs were generated within the 25â€“30 step limit and exhibit severe artifacts and image distortions.
&lt;/figcaption>
&lt;/figure>
&lt;p>Despite the artifacts, rough semantic alignment remains: Fig. 7A shows a framed &amp;ldquo;portrait&amp;rdquo; of a woman and Fig. 7B renders a woman with &amp;ldquo;flowing hair.&amp;rdquo; Crucially, Figure 8 confirms the original Tiny SD model produces coherent, acceptable (if blurry) outputs. This performance gap strongly suggests our CoreML pipeline is sound, but the model weights are being corrupted during the conversion or loading process.&lt;/p>
&lt;figure>
&lt;img src="images/tiny_sd_readme_outputs.png" style="width: 60%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 8.&lt;/strong> Official examples of Tiny SD outputs. These samples confirm the original Tiny SD is capable of generating coherent, if slightly blurry, portraits of people (e.g., center-left image), establishing an acceptable quality baseline prior to CoreML conversion (&lt;a href="https://huggingface.co/segmind/tiny-sd">source&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>So, where did the weights go wrong? The corruption must have originated from one of these three technical suspects:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Inference step requirements.&lt;/strong> Distilled models often require higher step counts for convergence than their parent models, a detail missing from the Tiny SD documentation. Our current $25-30$ steps may be too few.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>CoreML quantization precision loss.&lt;/strong> CoreML&amp;rsquo;s model packaging applies weight quantization (typically FP16 or mixed precision) that could compound errors in an already-distilled model, potentially degrading performance below acceptable limits.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>VAE decoder corruption during CoreML conversion.&lt;/strong> The VAE decoder is the model component most sensitive to weight corruption . It is a critical single point of failure because it performs the final, irreversible $64\times$ spatial upsampling. CoreML conversion might corrupt its transposed convolution weights, and (unlike the self-correcting U-Net) even the slightest VAE decoder corruption turns perfect latents into unusable outputs.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;!-- Validating this would require testing the decoder in isolation with known-good latents from PyTorch, which we didn't perform. -->
&lt;p>Pinpointing the exact cause of corruption would require a costly series of controlled ablation studies on &lt;a href="https://huggingface.co/SG161222/Realistic_Vision_V4.0_noVAE" target="_blank" rel="noopener">the teacher PyTorch model&lt;/a>: testing step counts, comparing FP32 vs. FP16 precision, and measuring degradation at each CoreML conversion stage.&lt;/p>
&lt;p>However, this investigation isn&amp;rsquo;t needed. This test already validates Tiny SD&amp;rsquo;s non-viability: I need CoreML/ANE compilation and a small number of diffusion steps to meet my strict sub-40-second latency budget. As seen, tiny SD can&amp;rsquo;t deliver under these constraints. It&amp;rsquo;s time to pivot to a model explicitly designed with Apple&amp;rsquo;s silicon in mind.&lt;/p>
&lt;h3 id="coreml-stable-diffusion">CoreML Stable Diffusion&lt;/h3>
&lt;p>The search for a viable replacement led me to Apple&amp;rsquo;s &lt;a href="https://github.com/apple/ml-stable-diffusion" target="_blank" rel="noopener">CoreML Stable Diffusion&lt;/a>. This is a professionally tuned implementation of &lt;a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5" target="_blank" rel="noopener">Runway ML&amp;rsquo;s stable-diffusion-v1-5&lt;/a> (1.3B parameters) that&amp;rsquo;s compressed into ~1.5GB.&lt;/p>
&lt;p>What makes this model viable where Tiny SD collapsed? Its key advantage is co-design with Apple&amp;rsquo;s hardware team. This grants engineers access to proprietary optimizationsâ€”like deep operator fusion and memory layoutâ€”to produce a calibrated FP16 model guaranteeing peak ANE performance unavailable through generic conversions. Crucially, this implementation is also battle-tested for iOS deployment, eliminating the risk of weight corruption I previously faced.&lt;/p>
&lt;figure>
&lt;img src="images/coreml-quick-test.png" style="width: 40%; height: auto;">
&lt;figcaption>
&lt;strong> Figure 9.&lt;/strong> Prototype testing. A generated image from the text prompt &lt;code>"A beautiful landscape with mountains and a lake, golden hour lighting"&lt;/code> using Appleâ€™s CoreML Stable Diffusion model. The output is a high-quality, realistic rendering of a rustic mountain range at sunset.
&lt;/figcaption>
&lt;/figure>
&lt;p>The trade-off is simple: I accept a ~1.5GB footprint (still well within budget) for a solution that guarantees production quality. Sometimes the &amp;ldquo;smallest&amp;rdquo; solution isn&amp;rsquo;t the best one. Hardware-aware optimizations at a reasonable scale beat model over-compression.&lt;/p>
&lt;br/>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this post, we explored the tight constraints required to deliver a Nano Banana-like experience on the edge. Our initial exploration led us to select Apple&amp;rsquo;s CoreML Stable Diffusion model due its aggressive hardware co-design.&lt;/p>
&lt;h3 id="whats-next">What&amp;rsquo;s next?&lt;/h3>
&lt;p>Before we can attempt to replicate the Nano Banana experience on device, we first need to understand what problems Apple&amp;rsquo;s CoreML optimizations truly solved and which technical challenges remain. &lt;a href="../edge-diffusion-2/">My next blog post&lt;/a> covers exactly how Apple safely compressed a diffusion model that is so easy to corrupt.&lt;/p></description></item><item><title>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</title><link>https://bellanich.github.io/post/edge-llm-app/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-llm-app/</guid><description>&lt;!-- Welcome ðŸ‘‹ -->
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#my-game-plan">My Game Plan&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#deployment">Deployment&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#attempt-1-deploying-my-multi-modal-model-as-an-ios-app">Attempt 1: Deploying my multi-modal model as an iOS app&lt;/a>&lt;/li>
&lt;li>&lt;a href="#what-if-the-mlc-chat-engine-supported-uploading-images">What if the MLC Chat Engine supported uploading images?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#changing-directions">Changing Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#attempt-2-deploying-my-multi-modal-model-as-a-python-cli">Attempt 2: Deploying my multi-modal model as a Python CLI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>If you&amp;rsquo;ve been following along in this blog post series, you know that I got a little too inspired at &lt;a href="https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/" target="_blank" rel="noopener">this year&amp;rsquo;s Google I/O Connect event&lt;/a>. After hearing about Google&amp;rsquo;s incredible feat of &lt;a href="https://support.google.com/pixelphone/community-guide/277503565/enable-gemini-nano-on-device-on-google-pixel-8-8a?hl=en" target="_blank" rel="noopener">embedding Gemini Nano into their Pixel 8 phones&lt;/a>, I&amp;rsquo;ve been not-so-patiently waiting for the open source community to release tools that make similar feats possible for solo developers like me. Fortunately, &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">the Machine Learning Compiler (MLC) project&lt;/a> has matured enough that my dreams might just be possible.&lt;/p>
&lt;p>Hence, I&amp;rsquo;ve been a personal quest to assess the limits of this new and exciting technology. To keep things interesting, I&amp;rsquo;ve decided to embed a multi-modal LLM. I want to test a less established setup, but I&amp;rsquo;m also secretly hoping to have a nice souvenir after everything is said and done. (Personally, I could really use a multi-lingual and multi-modal chatbot during my next holidays â€” especially once that doesn&amp;rsquo;t eat up my monthly data plan.)&lt;/p>
&lt;p>At this point, I&amp;rsquo;m very close to this end goal. In &lt;a href="../edge-llm-mlc/">my first blog post in this series&lt;/a>, I introduced you to the MLC framework, which has been doing most of the heavy-lifting. I also gave you a quick crash course in what it takes to embed a large machine learning model on a resource-constrained device. In &lt;a href="../edge-llm-embed-llava/">the 2nd blogpost&lt;/a>, I gave you a primer on vision transformers â€” specifically, what we need to know to embed &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">this tiny and multi-modal LLaVA-OneVision Qwen2 0.5B model&lt;/a>. In &lt;a href="../edge-llm-embed-llava/">my last blog post&lt;/a>, I used this LLaVA model as an example to show you how to extend the MLC framework to support a custom model definition.&lt;/p>
&lt;p>After going through this entire process, I have a fully functional - but not very logical - LLaVA model running on my laptop. As the name suggests, this LLaVA model belongs to the high-performing, open source, and &lt;a href="https://llava-vl.github.io" target="_blank" rel="noopener">multi-modal Large Language and Vision Assistant (LLaVA) model family&lt;/a>. Fortunately, for us, this model has less than 0.5B parameters. Meaning, it should be small enough to squeeze onto an iPhone. Here&amp;rsquo;s what its chat conversations look like:&lt;/p>
&lt;figure>
&lt;img src="images/llava_makes_sense.png" width="60%">
&lt;figcaption>
At the start of a very simple discussion, my embedded LLaVA-OneVision model looks promising.
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="images/llava_doesnt_make_sense.png" width="65%">
&lt;figcaption>
Unfortunately, it doesn't take too long to see the cracks in LLaVA's logical reasoning â€” and to watch them crumble.
&lt;/figcaption>
&lt;/figure>
&lt;p>Given this LLaVA model&amp;rsquo;s lack of coherence, we don&amp;rsquo;t want it talking directly to our end user. Rather, we&amp;rsquo;ll let it stick to what it does best (image annotation) and call in a slightly large LLM for backup.&lt;/p>
&lt;h3 id="my-game-plan">My Game Plan&lt;/h3>
&lt;p>So, what&amp;rsquo;s next? If you recall the diagram that I shared with you in the last time, I need to deploy a two-model ensemble. My plan is to use LLaVa-OneVision&amp;rsquo;s image annotation capabilities to expand &lt;a href="https://arxiv.org/pdf/2408.00118" target="_blank" rel="noopener">Gemma 2B&amp;rsquo;s ridiculously good conversational capabilities&lt;/a>. Essentially, LLaVA will describe the user provided image to Gemma in plain text. Gemma will receive the original user prompt and LLaVA&amp;rsquo;s image description. Between these two, Gemma should have enough information to return a lucid and logical response.&lt;/p>
&lt;figure>
&lt;img src="images/model_flowchart.png">
&lt;figcaption>
My multi-modal chat pipeline consists of two models: &lt;b>(1)&lt;/b> the eloquent and multilingual &lt;a href="https://huggingface.co/google/gemma-2b">Gemma2B model&lt;/a>, and &lt;b>(2)&lt;/b> a &lt;a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">mini LLaVA-OneVision model&lt;/a>. LLaVa will act as a translator for Gemma, generating a text descriptions for any provided images.
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="deployment">Deployment&lt;/h2>
&lt;p>My original vision was to have an edge multi-modal embedded onto my iPhone. That way, I could chat with my personal travel assistant anytime and anywhere â€” no internet connection required.&lt;/p>
&lt;h3 id="attempt-1-deploying-my-multi-modal-model-as-an-ios-app">Attempt 1: Deploying my multi-modal model as an iOS app&lt;/h3>
&lt;p>Before I can package Gemma 2B and LLaVA together as a single model, I first need to figure out how to input images into MLC&amp;rsquo;s simple, pre-built chat iOS application, and this is where I run into my first roadblock.&lt;/p>
&lt;p>If you&amp;rsquo;re observant, you might have noticed the grayed out text &amp;ldquo;Upload picture to chat&amp;rdquo; in my conversation screenshots with LLaVA. One could logically conclude that this means that MLC has some sort of user interface for uploading images. However, after pouring over the Swift code, I couldn&amp;rsquo;t find any such feature. Before I start cobbling something together in Swift, I decide to do a quick sanity check: Can I pass an image URL to the MLC Chat Engine?&lt;/p>
&lt;p>Well&amp;hellip;not really. I&amp;rsquo;d have to go in deep to the MLC&amp;rsquo;s low code and Swift implementations to make that possible. That&amp;rsquo;s a rabbit hole that I don&amp;rsquo;t want to fall into, so it&amp;rsquo;s time for me to pivot directions.&lt;/p>
&lt;h3 id="what-if-the-mlc-chat-engine-supported-uploading-images">What if the MLC Chat Engine supported uploading images?&lt;/h3>
&lt;p>Let&amp;rsquo;s suppose that I was able to the the MLC Chat Engine iOS implementation so that it:&lt;/p>
&lt;ol>
&lt;li>It now supports serving images to models.&lt;/li>
&lt;li>There&amp;rsquo;s a cool and sleek user interface for uploading said images.&lt;/li>
&lt;/ol>
&lt;p>Could I deploy any of the Apple products in my household? Well, I decided to do an initial test on my housemate&amp;rsquo;s iPad Pro 11 inch (V1). I copied over and downloaded my iOS app. While I was able to open the app&amp;rsquo;s homepage and start my conversation with Gemma 2B, the app would crash before I could even ask Gemma a followup question.&lt;/p>
&lt;p>Perplexed, I checked my iOS app&amp;rsquo;s memory consumption in Xcode after I opened a new chat with Gemma. Well, it turns out that Gemma consumes a bit over 2GB of RAM. Given that this borrowed iPad only has 4GB of RAM in total, it&amp;rsquo;s not a surprise that the iOS application was crashing. Even when I closed all other applications, other background processes were consuming this iPad&amp;rsquo;s limited and valuable memory.&lt;/p>
&lt;figure style="text-align: center;">
&lt;div style="margin-bottom: 20px;">
&lt;img src="images/gemma_memory.png" style="width: 95%; max-width: 900px;">
&lt;div style="font-size: 16px; margin-top: 8px;">(a) Gemma 2B&lt;/div>
&lt;/div>
&lt;div>
&lt;img src="images/llava_memory.png" style="width: 95%; max-width: 900px;">
&lt;div style="font-size: 16px; margin-top: 8px;">(b) LLaVA-OneVision&lt;/div>
&lt;/div>
&lt;figcaption style="margin-top: 20px; font-size: 16px;">
The memory footprints of my embedded Gemma 2B and LLaVA-OneVision models on my 8GB M2 MacBook Air.
&lt;/figcaption>
&lt;/figure>
&lt;p>In contrast, the LLaVA-OneVision model has a much smaller memory footprint at 755MB. So, yes, it&amp;rsquo;s technically possible to deploy only the LLaVA-OneVision model on this given iPad; however, it&amp;rsquo;s probably not worth it. As we&amp;rsquo;ve seen from this blog post&amp;rsquo;s introduction, the embedded LLaVA-OneVision model isn&amp;rsquo;t capable of holding a coherent conversation for very long. Meaning, Gemma&amp;rsquo;s memory consumption is a definitive roadblock in my app ambitions.&lt;/p>
&lt;p>As a final check, I tried the same exercise with my iPhone 13 and got similar results. This is expected, given that both devices have the same memory capacity. Of course, all the Apple devices I could find are on the older side. Apple&amp;rsquo;s newer iPhones and iPads have a large enough RAM that I should be able to deploy Gemma 2B and LLaVA-OneVision Qwen2 0.5B together.&lt;/p>
&lt;figure>
&lt;div style="padding-left: 250px; padding-right: 20px;">
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Device&lt;/th>
&lt;th>Available RAM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>iPhone X&lt;/td>
&lt;td>3GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>iPhone 13&lt;/td>
&lt;td>4G&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>iPad Pro V1&lt;/td>
&lt;td>4G&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>iPhone 16&lt;/td>
&lt;td>8G&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>iPad Air 6&lt;/td>
&lt;td>8G&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;figcaption>Table 1: The RAM available on selected Apple products. The top 3 rows describe devices that I have immediate access to, while the last two describe Apple's latest iPhone and iPad offerings (&lt;a href="https://9to5mac.com/2023/11/18/iphone-ram-list/">source&lt;/a>).&lt;/figcaption>
&lt;/figure>
&lt;p>The available RAM on Apple&amp;rsquo;s latest iPhone and iPad models is a testament to its commitment to &lt;a href="https://www.apple.com/newsroom/2024/10/apple-intelligence-is-available-today-on-iphone-ipad-and-mac/" target="_blank" rel="noopener">cram AI tools and features into every part of the Apple user experience&lt;/a>. Of course, it&amp;rsquo;s also wild to think that the latest iPhone is just as computationally strong â€” at least in RAM â€” as my laptop. This also means that if I could feed images into the iOS MLC Chat Engine, then I should be able to very easily deploy my multi-modal model setup on an iPhone â€” just not my phone.&lt;/p>
&lt;p>That being said, I&amp;rsquo;m not in a rush to get the latest iPhone. I just have something to look forward to when its inevitably time to replace my current one.&lt;/p>
&lt;h3 id="changing-directions">Changing Directions&lt;/h3>
&lt;p>At this point, I&amp;rsquo;ve ruled out the possibility of deploying my multi-modal foundation model on any Apple device, especially those within my immediate reach.&lt;/p>
&lt;p>Now, I&amp;rsquo;m trying to contend with two different mysteries:&lt;/p>
&lt;ol>
&lt;li>Why does MLC natively support some LLaVA models, but doesn&amp;rsquo;t support serving images? The main added value of the LLaVA model family is their exceptional ability to handle multi-modal inputs.&lt;/li>
&lt;li>Does the MLC Engine support serving a model images in any other platform-specific chat engine implementation?&lt;/li>
&lt;/ol>
&lt;p>Realistically, I don&amp;rsquo;t think that I&amp;rsquo;m going to get an answer to my first question unless I manage to chase down some of the &lt;a href="https://github.com/mlc-ai/mlc-llm/graphs/contributors" target="_blank" rel="noopener">MLC project&amp;rsquo;s main contributors&lt;/a>. Fortunately, the second question is easier to answer. If you recall from &lt;a href="../edge-llm-mlc/">my first blog post&lt;/a>, MLC implements their own version of &lt;a href="https://platform.openai.com/docs/api-reference/introduction" target="_blank" rel="noopener">OpenAI&amp;rsquo;s Python API&lt;/a>.&lt;/p>
&lt;p>Since the original Python API supports serving images, there&amp;rsquo;s a good chance that the MLC implementation might offer the same functionality.&lt;/p>
&lt;figure>
&lt;img src="images/openai_image_url.png">
&lt;figcaption>
The Machine Learning Compiler bases their Python API off of OpenAI's well-established implementation. As we can see in the official OpenAI Python API documentation, the OpenAI implementation does support serving a model images (&lt;a href="https://platform.openai.com/docs/guides/fine-tuning#vision">source&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Sure enough, I look through when I look through the &lt;code>mlc-llm-cpu&lt;/code> library&amp;rsquo;s source code, I find the same &lt;code>image_url&lt;/code> variable in their conversation protocol definition.&lt;/p>
&lt;pre>&lt;code class="language-python">class Conversation(BaseModel):
...
def as_prompt(self, config=None) -&amp;gt; List[Any]:
...
for item in content:
assert isinstance(item, dict), &amp;quot;Content should be a string or a list of dicts&amp;quot;
assert &amp;quot;type&amp;quot; in item, &amp;quot;Content item should have a type field&amp;quot;
if item[&amp;quot;type&amp;quot;] == &amp;quot;text&amp;quot;:
message = self.role_templates[role].replace(
MessagePlaceholders[role.upper()].value, item[&amp;quot;text&amp;quot;]
)
message_list.append(message)
# MLC supports passing image URLs via its Python API
elif item[&amp;quot;type&amp;quot;] == &amp;quot;image_url&amp;quot;:
assert config is not None, &amp;quot;Model config is required&amp;quot;
image_url = _get_url_from_item(item)
message_list.append(data.ImageData.from_url(image_url, config))
message_list.append(&amp;quot;\n&amp;quot;)
else:
raise ValueError(f&amp;quot;Unsupported content type: {item['type']}&amp;quot;)
message_list.append(separator)
...
return prompt
&lt;/code>&lt;/pre>
&lt;p>Meaning, I still can implement my multi-modal chat pipeline via a Python script.&lt;/p>
&lt;br/>
&lt;h3 id="attempt-2-deploying-my-multi-modal-model-as-a-python-cli">Attempt 2: Deploying my multi-modal model as a Python CLI&lt;/h3>
&lt;p>At this point, I just want to see if I could successfully deploy the embedded multi-modal foundation model on some device that I own. So, I wrote a very simple Python script that lets a user interact with my chat pipeline via the commandline. If the user provides an image filepath, the model will check if it&amp;rsquo;s there. If so, underneath the hood, LLaVA is annotating this image for Gemma 2B. If not, my chatbot will kindly ask the user to check if the image is really there. Otherwise, Gemma 2B will be doing all the talking.&lt;/p>
&lt;p>Here&amp;rsquo;s the end result:&lt;/p>
&lt;figure>
&lt;img src="images/demo.gif">
&lt;figcaption>
My embedded multi-modal model recognizes the cute, cartoon husky in the supplied photo. It's also capable of generating an interesting story synopsis for what could possibly become a new international bestseller.
&lt;/figcaption>
&lt;/figure>
&lt;p>As you can see, the LLaVA model was able to recognize the Husky in the cartoon image and pass this information along to Gemma. Afterwards, Gemma was able to quickly help me draft an initial synopsis for about a day in the life of Barnaby, a sea-faring Husky who travels the world in his tiny yellow submarine. If we had a bit more compute power and a stronger LLaVA model, perhaps we could also extend this system to support image generation. If so, do think you &amp;ldquo;Barnaby&amp;rsquo;s Deep Sea Adventures&amp;rdquo; could become an international bestseller?&lt;/p>
&lt;blockquote>
&lt;p>For the exact source code used in this demo, please refer to &lt;a href="https://github.com/bellanich/pocket-llm/tree/main/src" target="_blank" rel="noopener">this blog post&amp;rsquo;s corresponding GitHub repository&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>It&amp;rsquo;s truly incredible seeing how quickly embedded systems technology has progressed in this domain. The tech giants are definitely in a comfortable place when it comes to deploying sophisticated edge foundation models, but the open source tooling is still in its early phase. Through this project, I&amp;rsquo;ve defined stumbled over a few vague, low-level code errors of my own. Tools like the MLC framework (in their current iteration) well-suited for those of us who are:&lt;/p>
&lt;ol>
&lt;li>Well-versed in the latest deep learning research&lt;/li>
&lt;li>Comfortable debugging low code system errors&lt;/li>
&lt;li>Reasonably proficient in Swift (or similar programming languages)&lt;/li>
&lt;li>Own latest edge devices&lt;/li>
&lt;/ol>
&lt;p>Of course, that&amp;rsquo;s a lot of conditions. Nonetheless, it&amp;rsquo;s incredible seeing how projects like the Machine Learning Compiler have democratized access to foundation models. Whether you&amp;rsquo;re moonlighting as an entrepreneurial or are a curious university student, you can get easily started deploying your own single-modal LLMs on edge today. Maybe in another 6 months, the tooling will have advanced to the point that it fully supports multi-modal chat â€” including image generation.&lt;/p>
&lt;p>In the meantime, I plan to watch these industry development closely. While I&amp;rsquo;ve put my hopes of a private, multi-modal travel assistant on the shelf, I haven&amp;rsquo;t abandoned them entirely. Stay tuned to see what I&amp;rsquo;ll do next. ðŸ‘‹&lt;/p></description></item><item><title>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</title><link>https://bellanich.github.io/post/edge-llm-embed-llava/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-llm-embed-llava/</guid><description>&lt;!-- Welcome ðŸ‘‹ -->
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#selected-architecture">Selected Architecture&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#why-not-only-use-the-llava-model">Why Not Only Use the LLaVA Model?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#the-overall-process">The Overall Process&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#manual-porting-llava-onevision-to-mlc">Manual Porting LLaVA-OneVision to MLC&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#mlcs-clip-vs-our-selected-llava-siglip-implementation">MLC&amp;rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#embedding-aggregation">Embedding Aggregation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#output-features-explained">Output Features Explained&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#gelu-approximation">GELU Approximation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#embedding-normalization">Embedding Normalization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#packaging-our-custom-model">Packaging Our Custom Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#end-result">End Result&lt;/a>&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#whats-next">What&amp;rsquo;s next?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I attended this year&amp;rsquo;s &lt;a href="https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/" target="_blank" rel="noopener">Google I/O Connect in Berlin&lt;/a>, and seeing
&lt;a href="https://ai.google.dev/edge" target="_blank" rel="noopener">Googleâ€™s latest work in Edge AI&lt;/a> was inspiring. Since then, I&amp;rsquo;ve been on a personal mission to deploy my own edge model. Given how rapidly the open source community is catching up with the tech giants in edge foundation models, I&amp;rsquo;ve decided test the limits of what I can realistically achieve as a solo developer. Can I embed a &lt;strong>multi-modal&lt;/strong> foundation model onto my iPhone?&lt;/p>
&lt;p>In my &lt;a href="../edge-llm-mlc/">first blog post&lt;/a>, I&amp;rsquo;ve introduced the &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">Machine Learning Compiler Project&lt;/a> as the open source solution that will make this possible. In my &lt;a href="../edge-llm-embed-llava/">last blog post&lt;/a>, I&amp;rsquo;ve given you the technical background knowledge needed to successfully deploy a multi-modal foundation model.&lt;/p>
&lt;p>Now, we&amp;rsquo;re going to put this theory into practice in a hands-on activity. I&amp;rsquo;m going to show you how to embed a custom foundation model onto an edge device â€” and all the things that can go wrong in the process.&lt;/p>
&lt;h3 id="selected-architecture">Selected Architecture&lt;/h3>
&lt;p>Remember that my end vision is to have a multi-modal foundation model deployed on my iPhone. Ideally, I want it to be multi-lingual so I can get some help the next time that I&amp;rsquo;m lost in a foreign country.&lt;/p>
&lt;p>At a high-level, my implementation will consist of two different models: &lt;strong>(a)&lt;/strong> the smallest &lt;a href="https://llava-vl.github.io" target="_blank" rel="noopener">Large Language and Vision Assistant (LLaVA)&lt;/a> model that I can find, and &lt;strong>(b)&lt;/strong> an &lt;a href="https://huggingface.co/google/gemma-2b" target="_blank" rel="noopener">instruction-tuned version of Gemma 2B&lt;/a>.&lt;/p>
&lt;p>I&amp;rsquo;ve chosen these models, since &lt;strong>(a)&lt;/strong> the MLC Engine natively supports them, &lt;strong>(b)&lt;/strong> they&amp;rsquo;re lightweight enough to be compiled on my laptop, and &lt;strong>(c)&lt;/strong> their respective families have earned a reputation as high-performers.&lt;/p>
&lt;p>Each model will work on a specific task. Whenever the end user shares an image with our multi-modal chatbot, the mini LLaVA model will generate a text description for Gemma. Gemma will then use this information to respond to the user&amp;rsquo;s original prompt. If no image is shared, our user will only interact with Gemma 2B. Of course, this model specialization will be abstracted away. Our user won&amp;rsquo;t be aware that they&amp;rsquo;re actually conversing with two different foundation models rather than just one.&lt;/p>
&lt;figure>
&lt;img src="images/model_flowchart.png">
&lt;figcaption>
My multi-modal chat pipeline consists of two models: &lt;b>(1)&lt;/b> the eloquent and multilingual &lt;a href="https://huggingface.co/google/gemma-2b">Gemma2B model&lt;/a>, and &lt;b>(2)&lt;/b> a mini &lt;a hred="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision model&lt;/a>. LLaVa will act as a translator for Gemma, generating a text description for any user inputted images.
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="why-not-only-use-the-llava-model">Why Not Only Use the LLaVA Model?&lt;/h4>
&lt;p>It may sound a bit strange that we&amp;rsquo;re deploying two models instead of one â€” especially since all LLaVA understand text and images. However, we need to make a distinction between &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">the smallest LLaVa-OneVision model&lt;/a>, which is less than 1B parameters in size, and &lt;a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md" target="_blank" rel="noopener">its much larger 7B+ counterparts&lt;/a>.&lt;/p>
&lt;figure>
&lt;img src="images/llava_model_stats.png" width="60%">
&lt;figcaption>
According to the 0.5B LLaVA-OneVision model card, the LLaVa model is just under 900M parameters in total. Given its 80,000+ model downloads, we can assume it does a decent job at its primary task: annotating images (&lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf">source&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Larger LLaVA models are perfectly capable of holding a coherent conversation and to understanding whatever images we show them. You can find a few interesting examples of large LLaVA models answering questions about images &lt;a href="https://llava-vl.github.io" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;figure>
&lt;img src="images/large_llava_chat.png">
&lt;figcaption>
&lt;a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16">The LLaVA-v1.6 series&lt;/a> contains models ranging from 7B to 34B parameters. At these sizes, a LLaVA model can easily identify Leonardo da Vinci's world famous masterpiece from a screenshot and give us a quick art history lesson. Unfortunately, we can't expect the same from the smallest LLaVA models. (&lt;a href="https://llava-vl.github.io">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>However, we need to adjust our expectations for a 900M parameter model. There&amp;rsquo;s only so much that a small model can do â€” and the LLaVA-OneVision Qwen2 O.5B model has been optimized for image annotation rather than instruction tasks. Meaning, we can probably converse directly with it, but may not be thrilled with the quality of its responses.&lt;/p>
&lt;p>If we wanted to deploy a similar application in a cloud environment, then it would probably just make more sense to quantize a 7B LLaVA model and accept a slightly larger monthly bill. However, since we&amp;rsquo;re working on edge, we have tight resource constraints and need to make do with what we have.&lt;/p>
&lt;h3 id="the-overall-process">The Overall Process&lt;/h3>
&lt;p>For each model, we need to:&lt;/p>
&lt;ol>
&lt;li>Quantize its weights; and&lt;/li>
&lt;li>Apply hardware-specific optimizations to it.&lt;/li>
&lt;/ol>
&lt;p>How simple this process really is comes down to the degree of built-in MLC Engine support. &lt;a href="https://huggingface.co/mlc-ai/gemma-2b-it-q4f32_1-MLC" target="_blank" rel="noopener">MLC has already pre-quantized an instruction-tuned version of Gemma 2B for us&lt;/a>. Hence, deploying Gemma as a stand alone model in an iOS application is relatively straightforward task. Just follow &lt;a href="https://llm.mlc.ai/docs/get_started/quick_start" target="_blank" rel="noopener">MLC&amp;rsquo;s Quick Start Documentation for how to package Gemma 2B&lt;/a> and &lt;a href="https://llm.mlc.ai/docs/deploy/ios.html#deploy-ios" target="_blank" rel="noopener">their iOS Swift SDK instructions&lt;/a>.&lt;/p>
&lt;p>On the other hand, applying the same process to our LLaVA model is a bit trickier. If we go through &lt;a href="https://huggingface.co/mlc-ai" target="_blank" rel="noopener">the list of pre-quantized models offered by MLC&lt;/a>,(as of November 2024) there is no pre-quantized LLaVA model available â€” much less our desired mini LLaVA model.&lt;/p>
&lt;p>Since this process for deployed a pre-quantized model from &lt;a href="https://huggingface.co" target="_blank" rel="noopener">HuggingFace&lt;/a> so well-documented, I&amp;rsquo;m not going to focus on it. Rather, I&amp;rsquo;ll show you how I ported a new model into the MLC framework.&lt;/p>
&lt;h2 id="manual-porting-llava-onevision-to-mlc">Manual Porting LLaVA-OneVision to MLC&lt;/h2>
&lt;p>At this point, you may be a bit confused. I&amp;rsquo;ve stated the MLC Engine supports LLaVA models, but I&amp;rsquo;m also talking about manually porting the 0.5B LLaVA-OneVision model into the MLC Framework.&lt;/p>
&lt;p>What&amp;rsquo;s going on? At an initial glance, it looks like MLC fully supports &lt;a href="https://llava-vl.github.io" target="_blank" rel="noopener">the LLaVa model family&lt;/a>, but that&amp;rsquo;s only partially true. At the time of writing (November 2024), MLC only natively supports specific LLaVA implementations, more specifically those that use &lt;strong>(a)&lt;/strong> use &lt;a href="https://www.llama.com" target="_blank" rel="noopener">Llama&lt;/a> or &lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" target="_blank" rel="noopener">Mistral model&lt;/a> as its text decoder, and &lt;strong>(b)&lt;/strong> use a CLIP-trained vision encoder. Unfortunately, all LLaVA variants that meet these requirements are at 7B+ parameters. Meaning, they&amp;rsquo;re too large for my laptop â€” muchless my smartphone â€” to handle.&lt;/p>
&lt;p>As a result, I need to manually port &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">the much smaller &lt;code>llava-onevision-qwen2-0.5b-ov-hf&lt;/code> model definition&lt;/a> into the MLC framework. Practically speaking, this means defining this model in the style used in the &lt;code>mlc-llm-cpu&lt;/code> Python library, which is only available through &lt;a href="https://mlc.ai/wheels" target="_blank" rel="noopener">MLC AI&amp;rsquo;s own Python code repository&lt;/a>. Afterwards, we need to then recompile this library locally so that it contains our new model definition.&lt;/p>
&lt;p>Once that&amp;rsquo;s done, I can quantize and hardware-optimize my selected LLaVA model just like any other MLC-supported model. As its full name suggests, the 0.5B LLaVA-OneVision model uses the &lt;a href="https://qwenlm.github.io/blog/qwen2/" target="_blank" rel="noopener">Qwen2 0.5B LLM&lt;/a> as its text decoder. Fortunately for us, MLC already supports Qwen2 0.5B implementation. Meaning, this change is quite easy. We just copy and paste MLC&amp;rsquo;s definition of LLaVA, rename the files, and change a few key-value pairs:&lt;/p>
&lt;p>The original MLC LLaVA model definition looks like this:&lt;/p>
&lt;pre>&lt;code class="language-python">from ..llama.llama_model import LlamaConfig, LlamaForCausalLM
from ..mistral.mistral_model import MistralConfig, MistralForCasualLM
CONFIG_MAP = {
&amp;quot;LlamaForCausalLM&amp;quot;: LlamaConfig,
&amp;quot;MistralForCausalLM&amp;quot;: MistralConfig,
}
ARCHITECTURE_MAP = {
&amp;quot;LlamaForCausalLM&amp;quot;: LlamaForCausalLM,
&amp;quot;MistralForCausalLM&amp;quot;: MistralForCasualLM,
}
&lt;/code>&lt;/pre>
&lt;p>Now, we just rewrite our LLaVA-OneVision model definition to support &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf/blob/main/config.json" target="_blank" rel="noopener">the LLaVA-OneVision Qwen2 0.5B model&amp;rsquo;s architecture definitions&lt;/a>:&lt;/p>
&lt;pre>&lt;code class="language-python"># Defined by MLC
from ..qwen2.qwen2_model import QWen2Config, QWen2LMHeadModel
CONFIG_MAP = {
&amp;quot;QWen2LMHeadModel&amp;quot;: QWen2Config,
&amp;quot;Qwen2ForCausalLM&amp;quot;: QWen2Config
}
ARCHITECTURE_MAP = {
&amp;quot;QWen2LMHeadModel&amp;quot;: QWen2LMHeadModel,
&amp;quot;Qwen2ForCausalLM&amp;quot;: QWen2LMHeadModel,
}
&lt;/code>&lt;/pre>
&lt;p>So far, so good. Let&amp;rsquo;s take a closer look at the &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf/blob/main/config.json" target="_blank" rel="noopener">the 0.5B LLaVA-OneVision &lt;code>config.json&lt;/code> file&lt;/a>. We see that this LLaVA model&amp;rsquo;s vision encoder was trained using SigLIP â€” rather than MLC&amp;rsquo;s natively supported CLIP training framework.&lt;/p>
&lt;figure>
&lt;img src="images/llava_siglip.png" width="70%">
&lt;figcaption>
As seen in the LLaVA-OnVision Qwen2 0.5B model's configuration file, the vision encoder was trained using SigLIP. As of now (November 2024), MLC's only natively supports CLIP-trained vision encoder (&lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf">source&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Meaning, there&amp;rsquo;s no MLC definition for us to just import. We need to write some custom Python model definition using MLC wrappers. Luckily, we already dived into the details of the SigLIP vision encoder in &lt;a href="../edge-llm-vision-encoders/">the previous blog post&lt;/a>. So, we&amp;rsquo;re ready to get started.&lt;/p>
&lt;p>I&amp;rsquo;ve included the final SigLIP vision encoder definition on &lt;a href="https://github.com/bellanich/pocket-llm/tree/main/models/patches" target="_blank" rel="noopener">in this blogpost series&amp;rsquo;s corresponding GitHub repository&lt;/a>. For the sake of brevity, I&amp;rsquo;m just going to focus on the technical differences between these two vision encoders and how these changes translate into code.&lt;/p>
&lt;h3 id="mlcs-clip-vs-our-selected-llava-siglip-implementation">MLC&amp;rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation&lt;/h3>
&lt;p>Once again, we&amp;rsquo;re going to reference our LLaVA model&amp;rsquo;s trusty &lt;code>config.json&lt;/code> file to get some clues about where to start. In particular, we see the key-value pair: &lt;code>&amp;quot;vision_feature_layer&amp;quot;: -1&lt;/code>, whereas the original LLaVA config uses &lt;code>&amp;quot;vision_feature_layer&amp;quot;: -2&lt;/code>. This is a hint about how a vision encoder aggregates its sequence of embeddings into a single vector.&lt;/p>
&lt;h3 id="embedding-aggregation">Embedding Aggregation&lt;/h3>
&lt;p>Both LLaVA models use a Vision Transformer, which outputs a sequence of embeddings. For the training of CLIP and SigLIP, we need a single vector. In this specific Huggingface implementation, CLIP and SigLIP do this aggregation in different ways.&lt;/p>
&lt;p>CLIP uses a &lt;em>class embedding&lt;/em>, similar to common adaptations of the &lt;a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html" target="_blank" rel="noopener">Vision Transformer for classification&lt;/a>. Here, we add another token to each image sequence, which is fed through the Transformer along with the image tokens. In the end, we pick the aggregated image feature vector as the output of this classification token. By having the class token part of the self-attention layers, it gives the transformer the ability to aggregate information of the image in this token across its layers. In the implementation, we see this class embedding token being added to the image token sequence:&lt;/p>
&lt;pre>&lt;code class="language-python">class CLIPVisionEmbeddings(Module):
def __init__(self, config: CLIPVisionConfig):
super().__init__()
# Class Embedding Token, added to each image token sequence.
self.class_embedding = nn.Parameter((self.embed_dim,))
...
def forward(self, pixel_values: Tensor) -&amp;gt; Tensor:
patch_embeds = self.patch_embedding(pixel_values)
...
class_embeds = broadcast_to(
self.class_embedding, shape=(batch_size, 1, self.embed_dim)
)
# Add class embedding token to image token sequence.
embeddings = concat([class_embeds, patch_embeds], dim=1)
...
return embeddings
&lt;/code>&lt;/pre>
&lt;p>In contrast, SigLIP pools its output features. Hence, the sequence of image tokens remains unchanged in the input and is fed through the layers. We add on top a pooling layer over the sequence dimension to aggregate all the feature information. This can either be done by a simple averaging, or, in case our specific case, with a multi-head attention pooling. This is similar to our self-attention layers, but just with a fixed query.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Note.&lt;/strong> We&amp;rsquo;ve removed the code parts that are common to both models. This simplifies the code and allows us to highlight key differences.&lt;/p>
&lt;/blockquote>
&lt;p>In our SigLIP implementation, shown below, we see as a difference to CLIP that there is no class embedding.&lt;/p>
&lt;pre>&lt;code class="language-python">class SiglipVisionEmbeddings(Module):
def __init__(self, config: SiglipVisionConfig):
super().__init__()
...
def forward(self, pixel_values: Tensor, interpolate_pos_encoding: bool = False) -&amp;gt; Tensor:
patch_embeds = self.patch_embedding(pixel_values)
embeddings = patch_embeds
...
return embeddings
&lt;/code>&lt;/pre>
&lt;h4 id="output-features-explained">Output Features Explained&lt;/h4>
&lt;p>In &lt;a href="../edge-llm-vision-encoders/">the previous blog post&lt;/a>, we discuss how we need a whole sequence of image embeddings for LLaVA rather than a single image feature vector. This provides more detailed information to the decoder.&lt;/p>
&lt;p>While we do not make use of the output heads of CLIP and SigLIP respectively, it does affect which layer we select our features from. This is what the config argument &lt;code>vision_feature_layer&lt;/code> ($-1$ for SigLIP and $-2$ for CLIP).&lt;/p>
&lt;p>In other words, we choose the last layer in SigLIP, since the model was trained with image embeddings that are literally the weighted average of all image sequence tokens. Thus, the training process ensures that all these image embeddings have valuable information in them.&lt;/p>
&lt;figure>
&lt;img src="images/attention_example.svg">
&lt;figcaption>
Attention pooling represents a weighted average pooling, where the weights are determined by the normalized dot product between a static query and the keys per token. While this example shows the averaging for text tokens, it has the same idea with image patch tokens in our vision encoder (&lt;a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;pre>&lt;code class="language-python">class SiglipVisionModel(Module):
def __init__(self, config: SiglipVisionConfig):
super().__init__()
self.vision_model = SiglipVisionTransformer(config)
def forward(self, pixel_values: Tensor) -&amp;gt; Tensor:
# Siglip Qwen2 is using last layer, CLIP pre-last due to different
# Transformer encoder.
return self.vision_model(pixel_values)[-1]
&lt;/code>&lt;/pre>
&lt;p>For CLIP, choosing the last layer is suboptimal, because of the usage of a class token. In the CLIP loss, only the output features of the class token are used. Thus, the output features of all other tokens, namely our image embeddings, were not used. In other words, these features did not receive any gradients during training, and we cannot be sure that the model has stored useful information in them. Most likely, the model has specialized the last layer specifically for the class embedding token, making the outputs of the other tokens (possibly) meaningless.&lt;/p>
&lt;p>Hence, we need to go back one more layer (i.e, the pre-last layer), because these tokens did receive gradients during the training by their dependency on the class token in the last self-attention layer. This ensures that these embeddings have strong features and makes them usable in our LLaVA model implementation.&lt;/p>
&lt;pre>&lt;code class="language-python">class CLIPVisionModel(Module):
def __init__(self, config: CLIPVisionConfig):
super().__init__()
self.vision_model = CLIPVisionTransformer(config)
def forward(self, pixel_values: Tensor) -&amp;gt; Tensor:
return self.vision_model(pixel_values)[-2]
&lt;/code>&lt;/pre>
&lt;h3 id="gelu-approximation">GELU Approximation&lt;/h3>
&lt;p>The &lt;a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">Gaussian Error Linear Unit (GELU)&lt;/a> is a very popular activation function for Transformers, and is used in both of our CLIP and SigLIP implementations. However, there are some specific details in about how we can implement the GELU activation.&lt;/p>
&lt;p>The &amp;ldquo;true&amp;rdquo;, precise implementation of GELU involves the cumulative distribution function (CDF) of the Gaussian distribution $\Phi(x)$:&lt;/p>
&lt;center>
&lt;p>$\text{gelu}(x)=x\cdot\Phi(x)$&lt;/p>
&lt;/center>
&lt;p>This CDF is, however, expensive to implement and in particular for edge-devices, where every inference optimization counts, it&amp;rsquo;s sub-optimal. Instead, people commonly use GeLU approximations that are good enough. The standard approximation, often used during training and in frameworks like &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.gelu.html" target="_blank" rel="noopener">JAX&lt;/a> and &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html" target="_blank" rel="noopener">PyTorch&lt;/a>, is the tanh-approximation:&lt;/p>
&lt;center>
&lt;p>$\text{gelu}(x)\approx 0.5x\left(1+\tanh\left[\sqrt{\frac{2}{\pi}}(x+0.044715\cdot x^3)\right]\right)$&lt;/p>
&lt;/center>
&lt;p>This is also being used in the Huggingface implementation for SigLIP, and we port it over as shown below:&lt;/p>
&lt;pre>&lt;code class="language-python">class QuickGELU(Module): # SigLIP implementation
def forward(self, input_tensor: Tensor) -&amp;gt; Tensor:
c = (2 / math.pi)**0.5
return 0.5 * input_tensor * (
1 + tanh(c * (input_tensor + 0.044715 * input_tensor**3))
)
&lt;/code>&lt;/pre>
&lt;p>In the MLC implementation of CLIP, another approximation is used. This one involved the sigmoid function, and is simply:&lt;/p>
&lt;center>
&lt;p>$\text{gelu}(x)\approx x\cdot \sigma(1.702x)$&lt;/p>
&lt;/center>
&lt;p>CLIP&lt;/p>
&lt;pre>&lt;code class="language-python">class QuickGELU(Module):
def forward(self, input_tensor: Tensor) -&amp;gt; Tensor:
return input_tensor * sigmoid(input_tensor * 1.702)
&lt;/code>&lt;/pre>
&lt;p>While the Sigmoid GeLU approximation is simpler and even cheaper to calculate, it is also less accurate. Thus, we have to make a tradeoff between efficiency and accuracy. Since we our selected SigLIP vision encoder was trained using the tanh-approximation, we&amp;rsquo;ll stick with it. Differences between &lt;a href="https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/" target="_blank" rel="noopener">the GeLU function implementation during training and inference time can cause a slight but noticeable drop in performance&lt;/a>.&lt;/p>
&lt;figure style="display: flex; flex-direction: column; align-items: center; text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 10px;">
&lt;img src="images/gelu_approximation_comparison.png" style="width: 45%;">
&lt;img src="images/gelu_approximation_comparison_zoomed.png" style="width: 45%;">
&lt;/div>
&lt;figcaption>
A visualization of the different implementations of the GELU activation function. The original GeLU function is in red, the tanh approximation is in purple, and the sigmoid approximation is in green. As you can see, all of them are quite similar. For the sigmoid activation, there is a noticeable difference for the negative range -1.5 and -4. However, for the tanh approximation, we need to zoom in closely to see the difference, showcasing why the tanh approximation is often used as a close match.
&lt;/figcaption>
&lt;/figure>
&lt;h3 id="embedding-normalization">Embedding Normalization&lt;/h3>
&lt;p>Another minor design choice is whether we normalize the embedding features before feeding them into the main Transformer model. Both models use the pre-activation Transformer implementation, which applies a &lt;code>LayerNorm &lt;/code> before each Self-Attention and MLP layer. However, we can also apply a LayerNorm on the embeddings themselves, or leave the model to learn the scaling of the residual part.&lt;/p>
&lt;p>In the CLIP implementation, we find a LayerNorm applied to the embeddings before feeding it through the layers.&lt;/p>
&lt;pre>&lt;code class="language-python">class CLIPVisionTransformer(Module):
def __init__(self, config: CLIPVisionConfig):
super().__init__()
embed_dim = config.hidden_size
self.embeddings = CLIPVisionEmbeddings(config)
self.pre_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)
self.encoder = CLIPEncoder(config)
self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)
def forward(self, pixel_values: Tensor) -&amp;gt; Tensor:
hidden_states = self.embeddings(pixel_values)
hidden_states = self.pre_layernorm(hidden_states)
encoder_outputs = self.encoder(inputs_embeds=hidden_states)
return encoder_outputs
&lt;/code>&lt;/pre>
&lt;p>In contrast, in the SigLIP implementation, this normalization is missing. However, it is not expected to cause a major performance difference.&lt;/p>
&lt;pre>&lt;code class="language-python">
class SiglipVisionTransformer(Module):
def __init__(self, config: SiglipVisionConfig):
super().__init__()
embed_dim = config.hidden_size
self.embeddings = SiglipVisionEmbeddings(config)
self.encoder = SiglipEncoder(config)
# Defined but not actually used.
self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)
def forward(self, pixel_values: Tensor) -&amp;gt; Tensor:
hidden_states = self.embeddings(pixel_values)
encoder_outputs = self.encoder(inputs_embeds=hidden_states)
return encoder_outputs
&lt;/code>&lt;/pre>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Overall, our SigLIP implementation has some small, but crucial differences compared to MLC&amp;rsquo;S CLIP implementation. The table below summarizes the differences that we needed to account for.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>MLC LLaVA Model - CLIP&lt;/th>
&lt;th>0.5B LLaVA-OneVision Qwen2 0.5B Model - SigLIP&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Output Feature Aggregation&lt;/td>
&lt;td>Class Token&lt;/td>
&lt;td>Attention Pooling&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Feature Layer&lt;/td>
&lt;td>Pre-Last Layer&lt;/td>
&lt;td>Last Layer&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Normalization&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GELU Implementation&lt;/td>
&lt;td>Sigmoid Approximation&lt;/td>
&lt;td>Tanh Approximation&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now that we&amp;rsquo;ve implemented SigLIP in the MLC framework, it is straight forward to integrate the LLaVA-OneVision models into the MLC framework. We can now proceed with quantizing and optimizing our model for deployment on an edge device.&lt;/p>
&lt;h2 id="packaging-our-custom-model">Packaging Our Custom Model&lt;/h2>
&lt;p>Once we&amp;rsquo;ve extended the &lt;code>mlc-llm-cpu&lt;/code> library to include our custom model definition, then we can proceed as normally.&lt;/p>
&lt;p>First, we want to quantize it our newly ported LLaVA model. To do so, we run the following commands in our MLC LLM project&amp;rsquo;s repo directory:&lt;/p>
&lt;pre>&lt;code class="language-bash"># Create directory
mkdir -p dist/models &amp;amp;&amp;amp; cd dist/models
# Clone Original LLaVA model's weights from HuggingFace
git lfs install
git clone https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf
# Apply the `q4f16_1` quantization method to our model
mlc_llm convert_weight ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \
--quantization q4f16_1 \
-o dist/llava-onevision-qwen2-0.5b-ov-hf
--model-type llava_onevision
&lt;/code>&lt;/pre>
&lt;p>Fortunately, the command ran successfully.&lt;/p>
&lt;figure>
&lt;img src="images/quantize_llava.png" width="100%">
&lt;figcaption>
After defining my target LLaVA-OneVision model as a new MLC `model-type`, I was able to easily quantize this model.
&lt;/figcaption>
&lt;/figure>
&lt;p>Next, we need to apply some hardware-specific optimizations to our model.&lt;/p>
&lt;pre>&lt;code class="language-bash"># Generate a MLC config file for our quantized model
mkdir dist/libs
mlc_llm gen_config ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \
--quantization q4f16_1 \
--conv-template redpajama_chat \
--context-window-size 768 \
-o dist/llava-onevision-qwen2-0.5b-ov-hf
# Optimize LLaVA OneVision for an iOS app implementation
mlc_llm compile ./dist/llava-onevision-qwen2-0.5b-ov-hf/mlc-chat-config.json \
--device iphone \
-o dist/libs/llava-onevision-qwen2-0.5b-ov-hf-iphone.tar
&lt;/code>&lt;/pre>
&lt;p>Finally, we need to package the quantized and optimized model for my iOS App. To make my life easier, I&amp;rsquo;ve uploaded the pre-quantized LLaVA-OneVision Qwen2 0.5B model to &lt;a href="https://huggingface.co/bella-nich/llava-onevision-qwen2-0.5b-ov-q4f16_1-mlc" target="_blank" rel="noopener">my personal HuggingFace account&lt;/a>.&lt;/p>
&lt;p>My &lt;code>mlc-package-config.json file&lt;/code> located in the &lt;code>ios/MLCChat/&lt;/code> subdirectory (following &lt;a href="https://github.com/mlc-ai/mlc-llm" target="_blank" rel="noopener">MLC LLM&amp;rsquo;s project structure&lt;/a>) now looks like this:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;device&amp;quot;: &amp;quot;iphone&amp;quot;,
&amp;quot;model_list&amp;quot;: [
{
&amp;quot;model&amp;quot;: &amp;quot;HF://bella-nich/llava-onevision-qwen2-0.5b-ov-q4f16_1-mlc&amp;quot;,
&amp;quot;model_id&amp;quot;: &amp;quot;llava-onevision-qwen2-0.5b-ov-hf&amp;quot;,
&amp;quot;estimated_vram_bytes&amp;quot;: 1000000000,
&amp;quot;overrides&amp;quot;: {
&amp;quot;prefill_chunk_size&amp;quot;: 128
}
},
]
}
&lt;/code>&lt;/pre>
&lt;p>So, I&amp;rsquo;m going to package my quantized and optimized LLaVA-OneVision model into a proper iOS app.&lt;/p>
&lt;pre>&lt;code class="language-bash"># Words
cd /path/to/MLCChat # e.g., &amp;quot;ios/MLCChat&amp;quot;
export MLC_LLM_SOURCE_DIR=/path/to/mlc-llm # e.g., &amp;quot;../..&amp;quot;
mlc_llm package
&lt;/code>&lt;/pre>
&lt;figure>
&lt;img src="images/package_llava.png">
&lt;figcaption>
I was able to successfully package my newly ported LLaVA-OneVision Qwen2 0.5B model without any errors. Meaning, there's a chance that everything was quantized and compiled correctly.
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="end-result">End Result&lt;/h2>
&lt;p>While the lack of compilation errors is promising, the only way to validate this entire process is talk to the embedded LLaVA model. So, I built and deployed my packaged iOS application.&lt;/p>
&lt;p>Once I do, I&amp;rsquo;m greeted with a few strange but mostly understandable sentences.&lt;/p>
&lt;figure>
&lt;img src="images/llava_makes_sense.png" width="60%">
&lt;figcaption>
My embedded LLaVA-OneVision Qwen2 0.5B model is able to form mostly coherent sentences. However, its sense of humor doesn't seem fully developed.
&lt;/figcaption>
&lt;/figure>
&lt;p>In other words, LLaVA isn&amp;rsquo;t pure spouting gibberish. I take this as a sign that the quantization and model compilation processes have gone well. Of course, as the longer the conservation goes on, the less coherent LLaVA becomes. Pretty soon LLaVA is giving me random responses strung together.&lt;/p>
&lt;figure>
&lt;img src="images/llava_doesnt_make_sense.png" width="65%">
&lt;figcaption>
After my dissatisfaction with the embedded model's sense of humor, I decided to see if LLaVA can tell me a fun fact. To my surprise, I'm greeted with a sudden and odd request.
&lt;/figcaption>
&lt;/figure>
&lt;p>The chat snippets of &lt;code>&amp;lt;im_start&amp;gt;&lt;/code>, &lt;code>&amp;lt;im_end&amp;gt;&lt;/code>, and &lt;code>assistant&lt;/code> give us clues about how this LLaVA model was tuned for image annotation tasks. More specifically, this tells us about the structure of LLaVA-OneVision Qwen2 0.5 B&amp;rsquo;s &lt;a href="https://huggingface.co/docs/transformers/main/en/chat_templating" target="_blank" rel="noopener">chat template&lt;/a>. A chat template restructures our current conversation, which is a list of string, into a single, tokenizable format that the model expects. Here, we can see that the chat template &lt;code>assistant&lt;/code> role is prompting LLaVA to continue but in ways that are completely disconnected from my original text-only prompts.&lt;/p>
&lt;p>The good news is that chat template &lt;code>assistant&lt;/code> role should be more useful when we provide LLaVA image inputs. However, this LLaVA model&amp;rsquo;s current performance (in a task that it wasn&amp;rsquo;t fine-tuned for) highlights the differences between &amp;gt;1B and a 7B+ parameter models. Larger foundation models are simply more versatile than smaller ones.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this blogpost, I&amp;rsquo;ve shown you everything that it takes to embed an unsupported model onto an edge device using the Machine Learning Engine Compiler framework. As you can see, the devil is in the details. You need to be well-versed in the different permutations of a given neural architecture&amp;rsquo;s implementation and able to spot those differences in the wild.&lt;/p>
&lt;p>Of course, the only thing that&amp;rsquo;s more important than getting this process right is to choose the correct model to embed. The smaller we go in size, the more portable our foundation model becomes, but that portability comes at the cost of performance.&lt;/p>
&lt;h3 id="whats-next">What&amp;rsquo;s next?&lt;/h3>
&lt;p>While embedding a custom model is an exciting milestone, we&amp;rsquo;re not done yet. As you can see, the embedded LLaVA model works but it doesn&amp;rsquo;t make for a scintillating conversation partner. Hence, we need to get Gemma 2B and the LLaVA-OneVision Qwen2 0.5B model to work together â€” which is exactly what I do in &lt;a href="../edge-llm-app/">my next (and final) blogpost in this series&lt;/a>. Stay tuned!&lt;/p></description></item><item><title>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</title><link>https://bellanich.github.io/post/edge-llm-vision-encoders/</link><pubDate>Sat, 30 Nov 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-llm-vision-encoders/</guid><description>&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#llava-chatbots-that-can-see">LLaVA: Chatbots That Can &amp;ldquo;See&amp;rdquo;&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#visual-instruction-tuning-of-text-decoders">Visual Instruction Tuning of Text Decoders&lt;/a>&lt;/li>
&lt;li>&lt;a href="#vision-encoders">Vision Encoders&lt;/a>&lt;/li>
&lt;li>&lt;a href="#training-the-vision-encoder">Training the Vision Encoder&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#clip">CLIP&lt;/a>&lt;/li>
&lt;li>&lt;a href="#siglip">SigLIP&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#inference">Inference&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#machine-learning-compiler-implementation">Machine Learning Compiler Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-llava-model-family-on-mlc">The LLaVA Model Family on MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="#llava-onevision">LLaVA OneVision&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#whats-next">What&amp;rsquo;s next?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In &lt;a href="../edge-llm-mlc/">my last blog post&lt;/a>, I introduced you to the fascinating world of edge foundation models. I was dreaming big, imagining a foundation model that could &amp;ldquo;see&amp;rdquo; â€” well, at least understand the photos I share with it. Let&amp;rsquo;s be honest, Iâ€™ll probably need some help when Iâ€™m lost in a new city on my next vacation. A model that understands both text and images? Way more useful when I&amp;rsquo;m wandering around than a single-modal chatbot!&lt;/p>
&lt;p>Recently, things have been getting pretty exciting in the world of multi-modal models. Beyond just text and images, &lt;a href="https://openai.com/index/chatgpt-can-now-see-hear-and-speak/" target="_blank" rel="noopener">ChatGPT can now &amp;ldquo;see, hear, and speak&amp;rdquo;&lt;/a> â€” processing text, images, and audio. There&amp;rsquo;s also &lt;a href="https://deepmind.google/technologies/gemini/" target="_blank" rel="noopener">Gemini&lt;/a>, Googleâ€™s latest powerhouse, which can process everything from text to images to audio â€” &lt;a href="https://deepmind.google/technologies/gemini/pro/" target="_blank" rel="noopener">and even long movies, thanks to its multi-million token context window&lt;/a>. Sounds pretty impressive, right? But here&amp;rsquo;s the catch: these models are so large and computationally demanding that itâ€™s nearly impossible to run them on edge devices (like phones and laptops).&lt;/p>
&lt;p>In this blog post, weâ€™ll explore some of the latest advancements in small, efficient multi-modal models that can actually be deployed on edge devices. We&amp;rsquo;ll introduce the &lt;a href="https://llava-vl.github.io/" target="_blank" rel="noopener">LLaVA&lt;/a> &lt;a href="https://huggingface.co/llava-hf" target="_blank" rel="noopener">model family&lt;/a>, which combines vision encoders and text decoders to provide a general-purpose multi-modal chatbot.&lt;/p>
&lt;figure>
&lt;img src="images/llava_v1_5_performance.jpg" width="70%">
&lt;figcaption>
The LLaVA model family stands out as a high-performance, open-source collection of models. Upon its release, the LLaVA-1.5 series achieved state-of-the-art (SoTA) performance across 11 benchmarks (&lt;a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Weâ€™ll also take a closer look at the architecture behind LLaVAâ€”specifically the vision encoders and text decoders that make it work. This includes the popular CLIP and SigLIP training frameworks.&lt;/p>
&lt;p>Understanding how these models work lays the groundwork for &lt;a href="../edge-llm-embed-llava/">my next blog post&lt;/a>.&lt;/p>
&lt;br/>
&lt;h2 id="llava-chatbots-that-can-see">LLaVA: Chatbots That Can &amp;ldquo;See&amp;rdquo;&lt;/h2>
&lt;p>The LLaVA model family is a collection of vision-language models that use pre-trained Vision Encoders to give pre-trained Large Language Models (LLMs) the ability to understand images. Why all the pre-training? Well, pre-trained models help keep training costs low while still leveraging the latest in foundation model technology.&lt;/p>
&lt;p>This combination of pre-trained models has made LLaVA models increasingly popular for multi-modal tasks. These models â€” &lt;a href="https://huggingface.co/llava-hf/bakLlava-v1-hf" target="_blank" rel="noopener">some named more creatively than others&lt;/a> â€” are open-sourced in various sizes, ranging from 0.5B to 13B parameters.&lt;/p>
&lt;figure>
&lt;img src="images/bakllava_model_card.png" width="80%">
&lt;figcaption>
Most LLaVA models are named after their base transformer architectures, but this one has been named after a beloved Turkish delicacy (&lt;a href="https://huggingface.co/llava-hf/bakLlava-v1-hf">source&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Donâ€™t forget that the &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">Machine Learning Compiler (MLC) Engine&lt;/a> (introduced in &lt;a href="../edge-llm-mlc/">my last blog post&lt;/a>) supports quantizing LLaVA models. In other words, the smallest LLaVA models are promising candidates for my edge multi-modal ambitions.&lt;/p>
&lt;h3 id="visual-instruction-tuning-of-text-decoders">Visual Instruction Tuning of Text Decoders&lt;/h3>
&lt;p>Before we jump into how LLaVA works its magic, letâ€™s take a quick look at how plain text-based LLMs operate. A typical Large Language Model (LLM) begins by breaking your input text into discrete units called &lt;strong>tokens&lt;/strong> using a tokenizer. These tokens are then transformed into high-dimensional embeddings â€” essentially numerical representations the model can &amp;ldquo;understand&amp;rdquo;. The embeddings pass through multiple layers of self-attention mechanisms and feed-forward neural networks, which process the information and predict the next token in the sequence. This process continues iteratively until the model generates the desired output text.&lt;/p>
&lt;p>But hereâ€™s the challenge: when it comes to multi-modal tasks, like combining images and text, your traditional text-based LLM hits a wall â€” it simply canâ€™t â€œseeâ€. To fix this, we bring in vision encoders. Vision encoders translate images into embeddings, a deep learning model&amp;rsquo;s &amp;ldquo;native language&amp;rdquo;. Afterwards, a text decoder translates these embeddings into an output text based on both the image and the text input. We align the feature space of the vision encoder with the LLM by adding a trainable linear projection layer on top of the vision embeddings. By fine-tuning the text decoder on a multi-modal dataset, the model learns to generate text that is relevant to the input image.&lt;/p>
&lt;figure>
&lt;center>
&lt;img src="images/llava_onevision.png"/>
&lt;/center>
&lt;figcaption> The LLaVA model family combines a vision encoder with a text decoder to generate human-like text based on images. The vision encoder converts images into embeddings, which are then fed into the text decoder as a visual instruction to generate the output text (&lt;a href="[url](https://arxiv.org/abs/2408.03326)">source&lt;/a>).&lt;/figcaption>
&lt;/figure>
&lt;p>This approach is called &lt;strong>&lt;a href="https://arxiv.org/abs/2304.08485" target="_blank" rel="noopener">Visual Instruction Tuning&lt;/a>&lt;/strong>. If youâ€™re familiar with Instruction Tuning, itâ€™s the same idea with a multi-modal twist. In regular instruction tuning, you give the model a text instruction (â€œSummarize this paragraphâ€) and train it to produce the desired text output. Visual Instruction Tuning swaps that text instruction for an image. The goal? Train the model to generate text that describes or explains the image.&lt;/p>
&lt;p>For example, LLaVA models are trained on datasets that pair images with captions or multi-modal Q&amp;amp;A examples. This forces them to become fluent in both visual and linguistic cues. In these finetuning setups, we commonly keep the vision encoder fixed and only fine-tune the text decoder and projection layer on the multi-modal dataset. his way, we can leverage the pre-trained vision encoder to understand images without the need for additional training data, while also benefiting from the power of the pre-trained LLM to generate human-like text.&lt;/p>
&lt;h3 id="vision-encoders">Vision Encoders&lt;/h3>
&lt;p>Essentially, Visual Instruction Tuning swaps out text instructions for images and teach the model to generate text based on what it â€œsees.â€ But thereâ€™s a big question here: how do we get an image - essentially a 2D grid of pixels â€” to become compatible with a token-consuming text-based model? While Convolutional Neural Networks (CNNs) have traditionally excelled at extracting features from images, Vision Transformers have shown promising results in processing images as sequences of tokens, similar to text.&lt;/p>
&lt;p>Hereâ€™s how it works: we divide up an image into smaller, fixed-size patches rather processing it all at once. Each patch is then flattened and mapped into a high-dimensional feature space â€” a numerical representation that captures the patchâ€™s visual characteristics. To make sure the model doesnâ€™t lose track of where these patches belong in the image, we add positional encodings. A Vision Transformer then consumes these location-annotated image patches, processing them through multiple layers of self-attention and feed-forward neural networks.&lt;/p>
&lt;p>This process allows the model to understand the relationships between different parts of the image and distill its content into a sequence of semantically meaningful embeddings. This sequence of embeddings is particularly compatible with classical LLMs because it mirrors the token-like structure LLMs expect for text. Thus, the combination of Vision Transformers with Text Decoders gives us high-functioning multi-modal models.&lt;/p>
&lt;p>If you want to learn about Vision Transformers in more detail, I recommend taking a closer look at the &lt;a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformers.html" target="_blank" rel="noopener">University of Amsterdam&amp;rsquo;s Deep Learning Tutorials&lt;/a>. For now, let&amp;rsquo;s focus on the training frameworks for vision encoders, as they are crucial for the performance of the final LLaVA model.&lt;/p>
&lt;h3 id="training-the-vision-encoder">Training the Vision Encoder&lt;/h3>
&lt;p>The quality of the final multi-modal LLaVA model heavily depends on the quality of its vision encoder. This is why it is crucial to train the vision encoder on a diverse and large-scale dataset to ensure that it can understand a wide range of images. Two popular training frameworks for vision encoders are CLIP and SigLIP. We describe both of them in a bit more detail here, since we need this knowledge to properly imbed a LLaVA model onto an edge device.&lt;/p>
&lt;h4 id="clip">CLIP&lt;/h4>
&lt;p>&lt;a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener">Contrastive Language-Image Pre-training (CLIP)&lt;/a> is a pre-training framework that teaches the vision encoder to understand images by associating them with text descriptions. Hereâ€™s the gist: CLIP trains the vision encoder to predict what the image means using a text description, and vice versa, to predict the image from a text description. By doing this, the model essentially learns a shared â€œlanguageâ€ that lets it understand both images and text.&lt;/p>
&lt;p>CLIP has been shown to achieve state-of-the-art performance on a wide range of vision tasks, making it a popular choice for vision encoders in multi-modal models, in particular due to its alignment of the vision and text feature spaces.&lt;/p>
&lt;figure>
&lt;center>
&lt;img src="images/clip_diagram.svg"/>
&lt;/center>
&lt;figcaption> CLIP trains vision encoders to predict image representations that align with text representations of their respective caption. By doing so contrastively over many image-text pairs in a batch, the vision encoder learns strong, semantic image embeddings (&lt;a href="[url](https://arxiv.org/abs/2103.00020)">source&lt;/a>).&lt;/figcaption>
&lt;/figure>
&lt;center>
&lt;/center>
&lt;p>A key aspect in CLIP is its use of &lt;strong>contrastive learning&lt;/strong>, where the model is trained to maximize the similarity between positive pairs (image-text pairs that belong together) and minimize the similarity between negative pairs (image-text pairs that do not belong together).&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Example.&lt;/strong> The vision encoders learns to match positive pairs (like a picture of a dog with a caption saying &amp;ldquo;a dog&amp;rdquo;) and push apart negative pairs (like a picture of a dog with a caption saying &amp;ldquo;a cat&amp;rdquo;).&lt;/p>
&lt;/blockquote>
&lt;p>The similarity is measured by a softmax, which is applied over the dot products between the representation spaces. This allows the model to learn a discriminative representation space that captures the semantic content of images and text.&lt;/p>
&lt;p>While this approach is intuitive, it doesn&amp;rsquo;t scale well. For large representation spaces, we need large batch sizes to effectively capture a large variety of (negative) image-text pairs and improve the training process. This raises challenges in CLIP, in particular with the softmax. To calculate the CLIP loss, we need to apply the batch-level softmax twice to normalize the pairwise similarity scores across all images for training the text encoder, and across all texts for training the image encoder. These passes over the full batch size can be computationally expensive, especially when the batch size is sharded across multiple devices.&lt;/p>
&lt;p>In a distribution training setup, we are often using data parallelism, where each device processes a part of the batch. While commonly, each device can do the forward and backward pass independently and only the final gradients are communicated, CLIP needs to already communicate the softmax statistics for the loss between all devices. This creates an efficiency bottleneck, especially when we scale to many devices. This bottleneck prevents CLIP from being scaled efficiently, and it&amp;rsquo;s the exact problem SigLIP solves.&lt;/p>
&lt;h4 id="siglip">SigLIP&lt;/h4>
&lt;p>The &lt;a href="https://arxiv.org/abs/2303.15343" target="_blank" rel="noopener">Sigmoid Loss for Language Image Pre-Training (SigLIP)&lt;/a> replaces the softmax in CLIP with a sigmoid loss, which is applied element-wise to the dot products between the image and text representations. The loss objective is then closer to standard &lt;em>predictive learning&lt;/em> with binary cross entropy, training the positive pairs to be $1$ while negative ones are pushed closer to $0$. This allows the model to train on a large batch size without the need for full-batch softmax computation, making it more efficient and scalable.&lt;/p>
&lt;p>SigLIP has been shown to achieve similar or even better performance to CLIP, in particular for smaller datasets of image-caption pairs. Furthermore, SigLIP is more computationally efficient at scale, making it a popular choice for training strong vision encoders in multi-modal models.&lt;/p>
&lt;h3 id="inference">Inference&lt;/h3>
&lt;p>Once the vision encoder is trained, we can use it to generate embeddings for images. However, both CLIP and SigLIP train single-vector representations, meaning that the image is represented by a single vector. This is not ideal for multi-modal models, as we want to generate a sequence of embeddings that can be fed into the text decoder. Furthermore, a single vector representation may not capture the full content of the image.&lt;/p>
&lt;p>To address this, we can use the internal representations of the Vision Encoder, which is commonly a Vision Transformer, to extract a sequence of embeddings for the image. This allows us to capture a more detailed representation of the image, which can be used by the text decoder to generate more accurate and detailed text descriptions. This is a key aspect of the LLaVA model family, which combines the power of Vision Transformers with Large Language Models to generate human-like text based on images.&lt;/p>
&lt;p>In &lt;a href="../edge-llm-embed-llava/">the next blogpost&lt;/a>, we will see that there are actually several variations on how a Vision Transformer can be implemented in the CLIP and SigLIP frameworks (e.g. using a separate class embedding, pooling, etc.). These variations can have a significant impact on the performance of the final LLaVA model and require adjusting the model architecture accordingly. Hence, it is important to carefully consider them when implementing your own LLaVA model.&lt;/p>
&lt;br/>
&lt;h2 id="machine-learning-compiler-implementation">Machine Learning Compiler Implementation&lt;/h2>
&lt;p>Now that we are familiar with the LLaVA model family and the vision encoders used in these models, let&amp;rsquo;s discuss how we can deploy these models on edge devices. As we have introduced in &lt;a href="../edge-llm-mlc/">Part 1 of this blog post series&lt;/a>, we use &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">the Machine Learning Compiler (MLC) project&lt;/a> for on-edge deployment. MLC aims to optimize and compile deep learning models for edge devices, enabling efficient and fast inference on resource-constrained hardware. MLC supports a wide range of deep learning models, including LLaVA models, making it a powerful tool for deploying multi-modal models on edge devices.&lt;/p>
&lt;h3 id="the-llava-model-family-on-mlc">The LLaVA Model Family on MLC&lt;/h3>
&lt;p>Out of the box, the MLC LLM supports the quantization of the LLaVA family of vision encoders and text decoders. For this, the LLaVA implementation of the &lt;a href="https://huggingface.co/docs/transformers/en/model_doc/llava" target="_blank" rel="noopener">Hugging Face Transformers library&lt;/a> has been integrated into the MLC framework using the TVM stack. At the time of writing (November 2024), the default supported text decoders are Llama and Mistral, with a CLIP-trained vision encoder. However, the sizes of these models are often around 7B parameters and larger, making them unsuitable for deployment on small edge devices (and even my M2 MacBook Air). This is why we need to consider smaller models, which are more suitable for our travel recommendation chatbot.&lt;/p>
&lt;h3 id="llava-onevision">LLaVA OneVision&lt;/h3>
&lt;p>One of the smallest LLaVA models is the &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">LLaVA OneVision Qwen2 0.5B&lt;/a>, which, as the name suggests, uses the &lt;a href="https://qwenlm.github.io/blog/qwen2/" target="_blank" rel="noopener">Qwen2 language model&lt;/a> with 0.5B parameters.&lt;/p>
&lt;figure>
&lt;img src="images/llava_one_vision_performance.png">
&lt;figcaption>
LLaVA-OneVision models tackle even the trickiest secondary school math problems with ease. In this example, the model demonstrates its ability to interpret multiple images and coherently apply deductive reasoning (&lt;a href="https://arxiv.org/pdf/2408.03326">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>The LLaVA OneVision Qwen2 0.5B model is particularly suitable for deployment on edge devices, as it is small and lightweight. While we can&amp;rsquo;t expect the same performance that larger models offer, this small LLaVA OneVision model is a good starting point for our travel recommendation chatbot and allows a fast iteration cycle for model development.&lt;/p>
&lt;p>In contrast to the original LLaVA models, the &lt;a href="https://arxiv.org/abs/2408.03326" target="_blank" rel="noopener">LLaVA OneVision model&lt;/a> uses a SigLIP-pretrained vision encoder, as it has demonstrated higher multi-modal LLM performance among open vision encoders. While we mentioned that in theory, there are no differences between SigLIP and CLIP during inference, the encoders slighlty differ in their Huggingface Transformers implementation. This is why we need to first port the LLaVA OneVision model and its SigLIP vision encoder to the MLC framework. Once again, I&amp;rsquo;ll walk you through how to do in &lt;a href="../edge-llm-embed-llava/">the next blog post&lt;/a>.&lt;/p>
&lt;br/>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Multi-modal foundation models allow us to apply state-of-the-art models to new and more complex applications. We&amp;rsquo;ve introduced how we can use a pre-trained vision encoder and text decoder architecture to build a general-purpose vision-language chatbot. The Machine Learning Compiler Project currently supports embedding the LLaVA vision-to-text models on edge devices. However, due to our restricted local device, we will use the smallest LLaVA OneVision model with only 0.5B, which needs to be manually ported to the MLC framework. Thus, we&amp;rsquo;ve taken some time to understand the intricacies between LlaVA model implementations and what this means during model training and testing time.&lt;/p>
&lt;h3 id="whats-next">What&amp;rsquo;s next?&lt;/h3>
&lt;p>This blog post provides you with the background knowledge needed to deploy a multi-modal vision-to-text model on edge devices. In &lt;a href="../edge-llm-embed-llava/">the next blog post of this series&lt;/a>, I&amp;rsquo;ll walk you through how to put this knowledge into practice.&lt;/p></description></item><item><title>Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC</title><link>https://bellanich.github.io/post/edge-llm-mlc/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/post/edge-llm-mlc/</guid><description>&lt;!-- Welcome ðŸ‘‹ -->
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;!-- - [Table of Contents](#table-of-contents) -->
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#project-inspiration">Project Inspiration&lt;/a>&lt;/li>
&lt;li>&lt;a href="#why-edge-foundation-models-matter">Why Edge Foundation Models Matter&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges-of-running-foundation-models-on-edge-devices">Challenges of Running Foundation Models on Edge Devices&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#my-gemma-debacle">My Gemma Debacle&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-memory-struggle-is-real">The Memory Struggle Is Real&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#mlc-llm-a-quantum-leap-in-deploying-edge-foundation-models">MLC LLM: A Quantum Leap in Deploying Edge Foundation Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#how-does-mlc-llm-work">How does MLC LLM work?&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#quantization">Quantization&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#what-is-quantization">What is Quantization?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#quantization-methods">Quantization Methods&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#1-post-training-quantization-ptq">1. Post-Training Quantization (PTQ)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-quantization-aware-training">2. Quantization-Aware Training&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#quantizing-transformers">Quantizing Transformers&lt;/a>&lt;/li>
&lt;li>&lt;a href="#out-of-the-box-mlc-llm-solutions">Out of the Box MLC LLM Solutions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#custom-solutions">Custom Solutions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#hardware-optimizations">Hardware Optimizations&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#just-in-time-model-compilation">Just-in-Time Model Compilation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mlc-llm-implementation">MLC LLM Implementation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#whats-next">What&amp;rsquo;s next?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Ever since ChatGPT went mainstream, Iâ€™ve been captivated by the rapid advancements in large language models (LLMs). As a machine learning engineer, Iâ€™ve been eagerly awaiting my chance to experiment with these groundbreaking models. Yet, the reality of deploying and managing the required infrastructure â€” and its massive cost â€” always made me pause.&lt;/p>
&lt;!-- the reality of deploying and managing the required infrastructure â€” not to mention the potential drain on my wallet (looking at you, AWS) â€” was a major roadblock. -->
&lt;h3 id="project-inspiration">Project Inspiration&lt;/h3>
&lt;p>This June at the &lt;a href="https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/" target="_blank" rel="noopener">Google I/O Connect Event in Berlin&lt;/a>, I realized my vision of working with server-free LLMs wasnâ€™t as far-fetched as Iâ€™d thought. Google showcased &lt;a href="https://deepmind.google/technologies/gemini/nano/" target="_blank" rel="noopener">Geminin nano, a powerful LLM integrated directly into their latest Android devices&lt;/a>. While it wasnâ€™t accessible to developers yet, it was a glimpse of what might soon be possible.&lt;/p>
&lt;figure>
&lt;img src="images/gemini_nano.png" width="75%">
&lt;figcaption>
Google originally launched Gemini Nano on the Pixel 8 Pro in December 2023. This edge LLM has been gradually rolled out to other popular Android phones, including Samsung's Galaxy S24 series (&lt;a href="https://www.yahoo.com/tech/google-making-gemini-nano-available-152409469.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAHbJFvaHe7eD1PbQpKUqhsrT8SThokdhZahaoc7cwPDe_CDZhRjsXmtWYQBrMe6qSuBdqUYns1O1ykdkfbAzILy3JmKegzVfSvByqwDsrx7YXxFfNXvM9-z5gPBhkVHS4I6eFneAMIGbStXAkKunwr-kqduoZ5jQb8CaGeTrpVNe">source&lt;/a>,
&lt;a href="https://deepmind.google/technologies/gemini/nano/">image credit&lt;/a>).
&lt;/figcaption>
&lt;/figure>
&lt;p>Inspired by this progress, I set out to test the limits of edge LLMs. Could I, as a solo enthusiast, deploy a LLM on an edge device like my iPhone? To make the challenge even more intriguing, I decided to aim for a multi-modal LLM. After all, who wouldnâ€™t want a private chatbot that understands your phoneâ€™s photo gallery and keeps your secrets safe?&lt;/p>
&lt;p>In this 4-part blog series, I document my experiment to prove that you donâ€™t need a sprawling server farm or a high-end workstation to dive into the latest AI technology. With a bit of machine learning knowledge, solid documentation, and plenty of determination, you can get started with state-of-the-art models without a painful cloud bill (looking at you, AWS).&lt;/p>
&lt;!-- * Link to 2nd blog post
* Link to 3rd blog post
* Link to 4th blog post -->
&lt;h3 id="why-edge-foundation-models-matter">Why Edge Foundation Models Matter&lt;/h3>
&lt;p>Foundation models have traditionally been too massive to run on edge devices like smartphones, IoT gadgets, or embedded systems. As a result, theyâ€™re typically hosted on centralized servers, which introduces several challenges:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Cost barriers.&lt;/strong> Deploying and serving large-scale models (think 10B+ parameters) in the cloud is prohibitively expensive, often costing millions in infrastructure and energy. This creates a significant barrier for students, hobbyists (like me), and smaller organizations looking to experiment with AI. By running models locally on edge devices, the need for expensive server infrastructure disappears, democratizing access to this cutting-edge technology.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Unreliable performance.&lt;/strong> Cloud-based inference depends on a steady internet connection to send data to servers and retrieve results. This back-and-forth can cause frustrating delays, especially in areas with poor connectivity. Edge models, which run directly on local devices, bypass these issues. They deliver faster responses and work well even without an internet connection.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Security concerns.&lt;/strong> Cloud-based systems inherently require sending data to remote servers, which comes with inherent risks. For users, their personal chat data could be exposed in a security breach or misused without consent. Businesses, meanwhile, must navigate strict regulations like GDPR or HIPAA when transferring sensitive data off-device. By processing data locally, edge models eliminate these risks, ensuring that your personal information stays private.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In short, edge foundation models break down cost barriers, improve reliability, and address privacy concerns. They make AI more accessible for curious minds and businesses alike while offering end users more peace of mind.&lt;/p>
&lt;h3 id="challenges-of-running-foundation-models-on-edge-devices">Challenges of Running Foundation Models on Edge Devices&lt;/h3>
&lt;p>By now, you might be thinking, Edge foundation models sound amazing! Why isnâ€™t everyone using them? Well, as with most things in life (and AI), thereâ€™s a catch. Running foundation models on edge devices isnâ€™t exactly a walk in the park. Let me walk you through some of the challenges, starting with my own cautionary tale.&lt;/p>
&lt;h4 id="my-gemma-debacle">My Gemma Debacle&lt;/h4>
&lt;p>About six months ago, I got my hands on Googleâ€™s shiny new &lt;a href="https://huggingface.co/google/gemma-2-2b-it" target="_blank" rel="noopener">instruction-tuned Gemma 2B model&lt;/a>. Gemma, for those unfamiliar, is the â€œbabyâ€ version of &lt;a href="https://deepmind.google/technologies/gemini/" target="_blank" rel="noopener">DeepMindâ€™s Gemini family&lt;/a> â€” a lightweight, open-weight LLM designed for resource-constrained environments.&lt;/p>
&lt;figure>
&lt;img src="images/gemma2b_ranking.png" width="100%">
&lt;figcaption>
Why Gemma2B? In a nutshell, it's very impressive for its size. In benchmarks tasks, &lt;a href="https://arxiv.org/pdf/2408.00118">Gemma 2B's performance is comparable those of 7B - 9B LLMs&lt;/a>. It also &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">consistently tops the HuggingFace leaderboard for smaller LLMs&lt;/a>.
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="images/gemma2b_languages.png" width="60%">
&lt;figcaption>
Gemma 2B calls itself a polyglot, claiming fluency in a bunch of languages. If that checks out, itâ€™s a pretty exciting pick for global business apps â€” just the kind of thing a solo developer (like me) could put to work.
&lt;/figcaption>
&lt;/figure>
&lt;p>Basically, Gemma 2B was designed for laptops, desktops, and modest cloud setups. It sounded perfect for my trusty MacBook Air M2 (8GB of RAM).&lt;/p>
&lt;p>Spoiler alert: it wasnâ€™t.&lt;/p>
&lt;p>I excitedly set up Gemma and attempted to serve my first request. My MacBook? It practically waved a white flag and crashed halfway through.&lt;/p>
&lt;p>Letâ€™s do the math to see why this happened. The Gemma 2B model has (you guessed it) 2 billion parameters. Using the standard float32 data type, the model parameters alone would require $2B\text{ parameters} * \frac{4 \text{ bytes}}{\text{parameter}} = 8 \text{ billion bytes} = 8 \text{ GB of RAM}$.&lt;/p>
&lt;p>But thatâ€™s not all my machine needs to handle:&lt;/p>
&lt;ul>
&lt;li>I need extra memory for activations (the intermediate calculations during inference).&lt;/li>
&lt;li>The operating system (in my case, macOS) also needs a hefty chunk of RAM to do its thing&lt;/li>
&lt;/ul>
&lt;p>In short, my poor MacBook was way out of its depth. Even with more efficient data types like float16 or bfloat16 (which halve memory usage), the combined memory demands of the model, activations, and system processes were just too much. Now, imagine trying to squeeze this kind of workload onto a smartphone with even less RAM. Youâ€™d be lucky if your phone didnâ€™t catch fire (kidding&amp;hellip;mostly).&lt;/p>
&lt;h4 id="the-memory-struggle-is-real">The Memory Struggle Is Real&lt;/h4>
&lt;p>Edge devices are, by design, resource-constrained. Theyâ€™re great for portability, but they arenâ€™t built to handle the sheer memory and compute demands of large language models. Even lightweight models like Gemma, which aim to close this gap, can still overwhelm devices with limited RAM or processing power.&lt;/p>
&lt;p>But donâ€™t despair! Engineers and researchers are tackling these challenges head-on. By using model compression techniques like quantization, pruning, and distillation, theyâ€™ve managed to shrink memory and compute requirements significantly. Add to this a new wave of hardware optimization techniques, and edge deployment is more feasible than ever.&lt;/p>
&lt;p>Now, innovative tools are building on these advancements to make edge LLMs not just possible, but practical for a wide range of devices. Curious about how these breakthroughs are unfolding in real-world applications? Let me introduce you to one powerful solution: the MLC LLM framework.&lt;/p>
&lt;h3 id="mlc-llm-a-quantum-leap-in-deploying-edge-foundation-models">MLC LLM: A Quantum Leap in Deploying Edge Foundation Models&lt;/h3>
&lt;p>Fast forward to November 2024, I decided to try the same task as before but with &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">the Machine Learn Compiler (MLC) LLM Engine&lt;/a>. This time, I deployed &lt;a href="https://huggingface.co/mlc-ai/gemma-2b-it-q4f16_1-MLC" target="_blank" rel="noopener">a pre-quantized version of this Gemma 2B model&lt;/a> onto an edge device â€” specifically, an iOS app. The results blew me away.&lt;/p>
&lt;p>For starters, I encountered zero performance issues on my MacBook Air. The pre-quantized and hardware-optimized Gemma model ran smoothly and efficiently, without any of the lag or crashes I had faced with six months earlier.&lt;/p>
&lt;p>But here&amp;rsquo;s where things really got exciting: the quality of the responses. They were practically indistinguishable from the likes of massive, cloud-based LLMs in an everyday conversation. Curious to see how well this mini-model handled other languages, I threw some Spanish and German at it. To my non-discerning eye, the results looked spot-on. (Iâ€™d love to hear what native speakers think, though.)&lt;/p>
&lt;figure>
&lt;img src="images/gemma2b_spanish.png" width="100%">
&lt;figcaption>
I decided to practice my very rusty Spanish and seek vacation inspiration in one-go. Here are Gemma's recommendations for a visit to Valencia, Spain ðŸ‡ªðŸ‡¸ in Winter.
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="images/gemma2b_german.png" width="80%">
&lt;figcaption>
Christmas time means Christmas Markets ðŸŽ„ â€” especially when you live so close to Germany ðŸ‡©ðŸ‡ª. So, I decided to see if Gemma could give me any fun suggestions in German.
&lt;/figcaption>
&lt;/figure>
&lt;p>Now, you might be wondering: How did MLC manage to pull this off? Letâ€™s take a step back and dive into the tech behind this feat.&lt;/p>
&lt;br/>
&lt;h2 id="how-does-mlc-llm-work">How does MLC LLM work?&lt;/h2>
&lt;p>At a high-level, &lt;a href="https://github.com/mlc-ai/mlc-llm" target="_blank" rel="noopener">the MLC LLM project&lt;/a> makes it possible to embed smaller LLMs (under 10B parameters) on edge devices through a streamlined, three-step process:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Quantize LLM weights&lt;/strong> as the model is downloaded. This prevented my machine from crashing due to insufficient memory.&lt;/li>
&lt;li>&lt;strong>Embed the quantized model&lt;/strong> with hardware-specific optimizations applied during the model compilation stage.&lt;/li>
&lt;li>&lt;strong>Provide a simple, pre-built user interface&lt;/strong> to interact with your newly embedded foundation model.&lt;/li>
&lt;/ol>
&lt;figure>
&lt;img src="./images/mlc_llm_workflow.png">
&lt;figcaption> The MLCEngine embeds LLMs across different software platforms through model quantization and hardware-specific optimization (&lt;a href="[url](https://llm.mlc.ai)">image credit&lt;/a>)&lt;/figcaption>
&lt;/figure>
&lt;p>MLC offers a user-friendly, open-source chat application for both Android and iOS. Alternatively, it implements its own version of &lt;a href="https://platform.openai.com/docs/api-reference/introduction" target="_blank" rel="noopener">OpenAI&amp;rsquo;s Python API&lt;/a>, making it easy to integrate the optimized LLM into your own existing projects.&lt;/p>
&lt;h3 id="quantization">Quantization&lt;/h3>
&lt;p>&lt;a href="https://llm.mlc.ai/docs/get_started/introduction.html#chat-cli" target="_blank" rel="noopener">MLC LLM caches pre-quantized model weights and compiled model library locally&lt;/a>, which means you &lt;strong>only&lt;/strong> need to &lt;strong>download and quantize the model once&lt;/strong>. After that, the quantized model is ready to run on your device without requiring repeated downloads. This saves both time and bandwidth, making the process smoother and more efficient.&lt;/p>
&lt;h4 id="what-is-quantization">What is Quantization?&lt;/h4>
&lt;p>In simple terms, quantization is the process of reducing the precision of the numbers that represent a modelâ€™s parameters. The goal? Shrink the modelâ€™s memory footprint while keeping its performance as close to the original as possible. The real magic happens when you see the cost savingsâ€”quantization can cut your cloud compute bills by half, a quarter, or even a sixth, without any noticeable drop in performance. For massive models like LLMs, those savings can really add up.&lt;/p>
&lt;blockquote>
&lt;p>Take the example of &lt;a href="https://www.yurts.ai" target="_blank" rel="noopener">yurts&lt;/a>, a contractor for the U.S. government. They &lt;a href="https://www.yurts.ai/blog/enhancing-enterprise-efficiency-quantization-for-cost-effective-llm-deployment" target="_blank" rel="noopener">slashed its monthly cloud computing bill from USD 24,000 to USD 4,000 for a 70B parameter LLM&lt;/a> by using a quantization method called &lt;a href="https://huggingface.co/docs/transformers/main/en/quantization/awq" target="_blank" rel="noopener">Activation-aware Weight Quantization (AWQ)&lt;/a>. Pretty impressive, right?&lt;/p>
&lt;/blockquote>
&lt;h4 id="quantization-methods">Quantization Methods&lt;/h4>
&lt;p>When it comes to quantizing a model, there are a few common methods, but the two main approaches are:&lt;/p>
&lt;h5 id="1-post-training-quantization-ptq">1. Post-Training Quantization (PTQ)&lt;/h5>
&lt;p>After a model is trained, you can apply quantization to reduce the bit-width of its weights. The best part? Itâ€™s quick, easy, and requires minimal changes to the original model, while still offering significant memory savings.&lt;/p>
&lt;p>One common PTQ technique is grouped quantization, where the modelâ€™s weights are grouped based on features like their layer or importance. Each group is quantized separately, making the process more tailored and efficient. This method has been around since the late 1990s and continues to evolve as a way to balance performance and memory efficiency.&lt;/p>
&lt;p>Some weight groups are more sensitive to quantization errors and need higher precision (more bits) to maintain accuracy. Others can handle lower precision without a noticeable hit to performance.&lt;/p>
&lt;p>With the rise of foundation models, more specific implementations of grouped quantization have emerged. For an in-depth look, check out &lt;a href="https://arxiv.org/abs/2212.09720" target="_blank" rel="noopener">&amp;ldquo;The case for 4-bit precision: k-bit Inference Scaling Laws&amp;rdquo;&lt;/a> and &lt;a href="https://arxiv.org/abs/2206.09557" target="_blank" rel="noopener">&amp;ldquo;The LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>More recently, techniques like &lt;a href="https://huggingface.co/docs/transformers/en/quantization/awq" target="_blank" rel="noopener">Activation-aware Weight Quantization (AWQ)&lt;/a> have taken this dynamic quantization approach even further. AWQ uses activation statistics to pinpoint the most important individual weights and ensures they arenâ€™t over-quantized, allowing for better compression without sacrificing performance.&lt;/p>
&lt;h5 id="2-quantization-aware-training">2. Quantization-Aware Training&lt;/h5>
&lt;p>This method goes a step further by training the model with lower precision in mind from the start. By optimizing the model for reduced precision during training, you often get better results than you would with post-training quantization. Essentially, it allows the model to â€œlearnâ€ how to perform well with less precision, resulting in better overall performance. However, as we focus on deploying pre-trained models, we wonâ€™t explore this method further.&lt;/p>
&lt;h4 id="quantizing-transformers">Quantizing Transformers&lt;/h4>
&lt;p>When quantizing Transformers, itâ€™s not just the weights that need attentionâ€”activations play a big role too. Activations are the intermediate values generated during the model&amp;rsquo;s forward pass as it processes the input data. In a Transformer, these are the values produced at each layer as it handles each token. Just like with weights, activations can also be compressed during quantization, which further reduces memory usage.&lt;/p>
&lt;p>But memory management doesnâ€™t end with weights and activations. For Transformers, thereâ€™s also &lt;a href="https://huggingface.co/docs/transformers/kv_cache#what-is-cache-and-why-we-should-care" target="_blank" rel="noopener">the key-value (KV) cache&lt;/a> â€” this stores the context of the input sequence as the model processes longer inputs. As the model processes longer and longer inputs, it needs more memory to store the increasing number of keys and values. To keep things efficient,&lt;a href="https://llm.mlc.ai/docs/compilation/compile_models.html#generate-mlc-chat-config" target="_blank" rel="noopener"> MLC LLM provides additional memory optimization techniques, like sliding windows&lt;/a>, which help manage memory usage even when dealing with longer sequences.&lt;/p>
&lt;figure>
&lt;img src="./images/kv_cache.gif">
&lt;figcaption> The key-value (KV) cache in Transformers preserves the context of processed tokens, enabling the model to "remember" earlier parts of a conversation. Yet, as the conversation grows, the cache scales up rapidly, incurring risks of out-of-memory errors on edge devices if not handled properly. (&lt;a href="https://jalammar.github.io/illustrated-gpt2/">image credit&lt;/a>)&lt;/figcaption>
&lt;/figure>
&lt;h4 id="out-of-the-box-mlc-llm-solutions">Out of the Box MLC LLM Solutions&lt;/h4>
&lt;p>As you can probably guess, the MLC Engine only implements post-training quantization (since we have no control over an open sourced LLM&amp;rsquo;s training process). In particular, &lt;a href="https://github.com/mlc-ai/mlc-llm/blob/main/docs/compilation/configure_quantization.rst" target="_blank" rel="noopener">MLC LLM implements the grouping quantization methods&lt;/a> shown below.&lt;/p>
&lt;figure>
&lt;div style="padding-left: 50px; padding-right: 20px;">
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method Name&lt;/th>
&lt;th>Weight Quantization&lt;/th>
&lt;th>Activation Quantization&lt;/th>
&lt;th>Version No.&lt;/th>
&lt;th>Stable?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>q0f16&lt;/td>
&lt;td>None&lt;/td>
&lt;td>16 bits&lt;/td>
&lt;td>-&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q0f32&lt;/td>
&lt;td>None&lt;/td>
&lt;td>32 bits&lt;/td>
&lt;td>-&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q3f16_1&lt;/td>
&lt;td>3 bits&lt;/td>
&lt;td>16 bits&lt;/td>
&lt;td>1&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q4f16_1&lt;/td>
&lt;td>4 bits&lt;/td>
&lt;td>16 bits&lt;/td>
&lt;td>1&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q4f32_1&lt;/td>
&lt;td>4 bits&lt;/td>
&lt;td>32 bits&lt;/td>
&lt;td>1&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q4f16_awq&lt;/td>
&lt;td>4 bits&lt;/td>
&lt;td>16 bits&lt;/td>
&lt;td>1&lt;/td>
&lt;td>&lt;strong>No&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;figcaption>Table 1: An Overview of MLC's implemented quantization methods (&lt;a href="https://github.com/mlc-ai/mlc-llm/blob/main/docs/compilation/configure_quantization.rst">source&lt;/a>).&lt;/figcaption>
&lt;/figure>
&lt;p>MLC Enginer also offers an AWQ implementation (called &lt;code>q4f16_awq&lt;/code>), but it&amp;rsquo;s currently &lt;strong>unstable&lt;/strong> so use it at your own risk.&lt;/p>
&lt;p>Of course, the folks behind MLC have already gone and quantized most of the very popular open-source LLMs. You can download these pre-quantized model weights from &lt;a href="https://huggingface.co/mlc-ai" target="_blank" rel="noopener">their official MLC AI&amp;rsquo;s HuggingFace account&lt;/a>.&lt;/p>
&lt;p>If you want to quantize a new model, then there&amp;rsquo;s a little more work involved. MLC right now supports quantization of these model types: &lt;code>baichuan&lt;/code>, &lt;code>bert&lt;/code>, &lt;code>chatglm3&lt;/code>, &lt;code>cohere&lt;/code>, &lt;code>eagle&lt;/code>, &lt;code>gemma&lt;/code>, &lt;code>gemma2&lt;/code>, &lt;code>gpt2&lt;/code>, &lt;code>gpt_bigcode&lt;/code>, &lt;code>gpt_neox&lt;/code>, &lt;code>internlm&lt;/code>, &lt;code>internlm2&lt;/code>, &lt;code>llama&lt;/code>, &lt;code>llava&lt;/code>, &lt;code>medusa&lt;/code>, &lt;code>minicpm&lt;/code>, &lt;code>mistral&lt;/code>, &lt;code>mixtral&lt;/code>, &lt;code>orion&lt;/code>, &lt;code>phi&lt;/code>, &lt;code>phi3&lt;/code>, &lt;code>phi3v&lt;/code>, &lt;code>qwen&lt;/code>, &lt;code>qwen2&lt;/code>, &lt;code>qwen2_moe&lt;/code>, &lt;code>rwkv5&lt;/code>, &lt;code>rwkv6&lt;/code>, &lt;code>stable_lm&lt;/code>, and &lt;code>starcoder2&lt;/code>.&lt;/p>
&lt;p>So, if you want to quantize of these model types yourself, then all you have to do is &lt;a href="https://llm.mlc.ai/docs/get_started/introduction.html#id8" target="_blank" rel="noopener">run a few simple commands&lt;/a>.&lt;/p>
&lt;h4 id="custom-solutions">Custom Solutions&lt;/h4>
&lt;p>If you want to quantize an unsupported model type, you&amp;rsquo;ll need to extend MLC LLM&amp;rsquo;s source code. This involves inferring your target model&amp;rsquo;s architecture from its source &lt;code>config.json&lt;/code> file &lt;a href="https://huggingface.co" target="_blank" rel="noopener">on HuggingFace&lt;/a> and wrapping its original Python definition (e.g., from &lt;a href="https://pypi.org/project/transformers/" target="_blank" rel="noopener">the &lt;code>transformers&lt;/code> Python library&lt;/a>) with MLC LLM&amp;rsquo;s wrappers. I ended up having to do this to support multi-modal functional in &lt;a href="../edge-llm-embed-llava/">the 3rd blog post in this series&lt;/a>.&lt;/p>
&lt;br/>
&lt;h3 id="hardware-optimizations">Hardware Optimizations&lt;/h3>
&lt;p>Quantization is just one part of MLCâ€™s bag of tricks. The other? Squeezing every last drop of performance out of your hardware through smart optimizations. See, your LLM model might start as high-level Python code, but it doesnâ€™t interact directly with your deviceâ€™s hardware. Thereâ€™s a crucial middle step where MLC translates that model into something your CPU or GPU can actually understandâ€”and it does this in the most efficient way possible.&lt;/p>
&lt;h4 id="just-in-time-model-compilation">Just-in-Time Model Compilation&lt;/h4>
&lt;p>&lt;strong>Just-in-Time (JIT) model compilation&lt;/strong> is the secret sauce behind MLCâ€™s stellar efficiency. Instead of pre-compiling everything in advance or running the model eagerly line-by-line, JIT optimizes your model right before it executes, ensuring itâ€™s perfectly suited to your specific hardware.&lt;/p>
&lt;p>JIT strikes a balance between two compiler approaches:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Interpreted execution&lt;/strong> processes code step-by-step as it runs. This makes the code super flexible and easy to debug, but leaves no room for optimizations. In other words, it&amp;rsquo;s painfully slow.&lt;/li>
&lt;li>&lt;strong>Ahead-of-Time (AOT) compilation&lt;/strong> pre-compiles everything into a fixed version before execution. This is much faster, but comes with a catch: we assume a one-size-fits-all solution. If the model encounters unexpected conditions or hardware variations, AOTâ€™s rigid approach can leave performance on the table because it canâ€™t take full advantage of the specific device running the code.&lt;/li>
&lt;/ol>
&lt;p>JIT avoids these pitfalls by waiting until runtime to optimize. It tailors the modelâ€™s code to your hardware and execution context just before runtime, ensuring maximum efficiency. Here&amp;rsquo;s how this process works:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Tracing or scripting.&lt;/strong> First, the engine analyzes your modelâ€™s high-level code and maps out its computation graph and operations. Think of it as creating a blueprint for what the model will do&lt;/li>
&lt;li>&lt;strong>Optimization.&lt;/strong> Next, the engine gets to work refining that blueprint. It fuses operations, removes redundancies, and inlines functions, streamlining execution wherever possible. (Itâ€™s like an architect revising a blueprint for a more efficient construction process.)&lt;/li>
&lt;li>&lt;strong>Low-level code generation.&lt;/strong> Once the optimizations are done, the graph is compiled into low-level machine code tailored to your specific hardwareâ€”whether thatâ€™s a CPU, GPU, or something fancier.&lt;/li>
&lt;li>&lt;strong>Execution.&lt;/strong> Finally, the optimized code is executed, running faster and using less memory thanks to all the pre-launch optimizations.&lt;/li>
&lt;/ol>
&lt;p>MLC uses JIT model compilation to get the most out of your edge device&amp;rsquo;s limited resources. And the best part? This process is abstracted away into &lt;a href="https://llm.mlc.ai/docs/compilation/compile_models.html" target="_blank" rel="noopener">a few simple CLI commands&lt;/a>.&lt;/p>
&lt;h4 id="mlc-llm-implementation">MLC LLM Implementation&lt;/h4>
&lt;p>Deep neural networks are computationally demanding. Hence, most deep learning frameworks include built-in JIT compilation extensions. For example, &lt;a href="https://github.com/openxla/xla?tab=readme-ov-file" target="_blank" rel="noopener">Accelerated Linear Algebra (XLA)&lt;/a>, the backbone of JAX, offers &lt;a href="https://openxla.org/xla" target="_blank" rel="noopener">cross-framework JIT support&lt;/a>. Looking specifically at PyTorch, &lt;a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" target="_blank" rel="noopener">&lt;code>torch.compile&lt;/code> provides a general-purpose solution that supports both training and inference&lt;/a>.&lt;/p>
&lt;p>However, MLC takes it a step further by leveraging &lt;a href="https://tvm.apache.org" target="_blank" rel="noopener">Apache&amp;rsquo;s Tensor Virtual Machine (TVM)&lt;/a> for even deeper hardware-level optimizations.&lt;/p>
&lt;figure>
&lt;img src="./images/tvm_flexible.png">
&lt;figcaption> We can think of TVM as an improvement on PyTorch's optimizations, offering advanced optimizations and hardware-specific tuning that PyTorch's JIT lacks. Additionally, TVM is easy to use due to the separation of its compiler and runtime components. This makes it possible for me to compile a ML model on one machine (e.g., a MacBook) and deploy it on another (e.g., a Raspberry Pi). (&lt;a href="https://tvm.apache.org/docs/how_to/deploy/index.html">image credit&lt;/a>)&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;a href="https://mlc.ai/chapter_integration/index.html" target="_blank" rel="noopener">TVM works by providing a Python API for tensor operations like matrix multiplications, sums, and type conversions.&lt;/a> It also makes it a breeze to port models from PyTorch. Once we have the model in TVM, we can translate it into C++ as we optimize it for execution.&lt;/p>
&lt;p>Hereâ€™s how exactly TVM supercharges model optimization:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Operation Fusion.&lt;/strong> TVM combines smaller operations (like element-wise additions or multiplications) into larger, more efficient ones.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Example.&lt;/strong> Instead of calculating ReLU(x) followed by Add(x, y), TVM can combine them into a single, efficient kernel, saving memory and time.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Memory Layout Optimization:.&lt;/strong> TVM fine-tunes memory access patterns to align with the hardwareâ€™s strengths. For example, GPUs perform better when accessing data in large, coalesced blocks, while CPUs benefit from loop optimizations that prevent cache misses.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Kernel Selection and Tuning.&lt;/strong> A &amp;ldquo;kernel&amp;rdquo; is a specialized function designed to perform specific operations, like matrix multiplication. TVM either selects the best pre-tuned kernels or auto-tunes them for maximum performance on the target hardware.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>These optimizations make it possible to (hypothetically) fit a 7B+ parameter model onto an iPhone. But of course, thereâ€™s a trade-off: the more optimizations we apply, the less flexible the model becomes. Debugging also gets trickier â€” any issues that arise are often low-level errors, especially when input sizes change.&lt;/p>
&lt;p>Despite these challenges, the benefits far outweigh the costs. Without TVM, deploying models on edge devices would be much more difficult.&lt;/p>
&lt;br/>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In the past six months, the AI research community has made groundbreaking strides in optimizing foundation models for edge devices. Back in June 2024, my personal machine crashed when I tried to run the Gemma 2B model locally â€” without quantization or hardware optimizations. But thanks to the rapid progress in this field, even solo enthusiasts like myself can now, as of November 2024, easily deploy the same model (or even larger ones) locallyâ€”without needing to become compiler engineers.&lt;/p>
&lt;p>In this blog post, Iâ€™ve introduced the Machine Learning Compiler (MLC) as a powerful new tool to make this possible. Iâ€™ve also walked you through its inner workings and provided essential background knowledge to help you get started.&lt;/p>
&lt;h3 id="whats-next">What&amp;rsquo;s next?&lt;/h3>
&lt;p>In &lt;a href="../edge-llm-vision-encoders">my next blog post&lt;/a>, weâ€™ll dive into how we can extend the MLC Engine to support embedding LLMs that arenâ€™t natively supported. After all, our goal is to deploy a &lt;strong>multi-modal&lt;/strong> LLM on an iPhone.&lt;/p></description></item></channel></rss>