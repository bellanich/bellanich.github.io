[{"authors":null,"categories":null,"content":"As an ML Engineer at Netflix, I focus on making foundation models faster and more efficient. I evaluate these models from a systems perspective and optimize our evaluation workflows so that we can rapidly improve recommendations for our 300 million users worldwide.\nI’ve spent my career moving between ML algorithms and the hardware to streamline performance. I’ve stabilized homepage recommendation engines at scale, optimized AI assistant workloads to slash costs, and implemented holistic ML system improvements. Looking ahead, I’m increasingly interested in how hardware-software co-design can push the boundaries of performance, whether in large-scale distributed systems or resource-constrained edge environments.\nMy foundation includes a master’s degree in Artificial Intelligence from the University of Amsterdam and early professional experience in edge computer vision in Spain.\n","date":1764115200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1764115200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"As an ML Engineer at Netflix, I focus on making foundation models faster and more efficient. I evaluate these models from a systems perspective and optimize our evaluation workflows so that we can rapidly improve recommendations for our 300 million users worldwide.","tags":null,"title":"Bella Nicholson","type":"authors"},{"authors":null,"categories":null,"content":"Running state-of-the-art computer vision entirely on-device isn’t easy — and that’s exactly why I had to try it. During last year’s adventure in embedding multimodal LLMs on edge devices, the visceral impact of images really stood out to me. Unlike sequential language, the human brain processes imagery almost instantly, making its effects uniquely potent.\nIn recent years, the gravitational pull of Large Language Models (LLMs) has dominated the AI space, thanks to the compounding force of attention and scaling laws. That focus began to shift when Google DeepMind released Nano Banana in August 2025, and turned the painstaking process of photography edits into a simple text prompt. Three months later, Nano Banana Pro demonstrated a massive leap in generative computer vision capabilities by rendering perfectly legible text pixel by pixel. The implication? The power to instantly generate designer-quality infographics and slide decks.\nSeeing that level of capability running smoothly in the cloud made me wonder: how powerful could a tiny conditional image-generation model be while still fitting into my iPhone (and not crashing it)?\nFigure 1. My hyper-optimized three-stage pipeline uses Apple’s CoreML Stable Diffusion model for on-device conditional image generation. By isolating the heavy diffusion step to background regeneration, the system preserves identity consistency despite the tiny model’s quality constraints. The result is lightweight, fully on-device image generation averaging ~27 seconds end-to-end. That curiosity quickly launched this hands-on experiment. I challenged myself to run high-capacity, conditional image generation entirely on my iPhone\u0026rsquo;s hardware. Unfortunately, generative quality often breaks down at smaller scales: a tiny model is more likely to turn you into a distant cousin than your doppelgänger. To solve this, I engineered a two-step workflow that preserves subject identity while regenerating complex backgrounds and enforcing visual consistency (with some fun filters included). I initially benchmarked Segmind\u0026rsquo;s Tiny (~1GB) stable diffusion model due to its tiny memory footprint, but it couldn\u0026rsquo;t generate high-quality outputs under our strict timing and hardware constraints.\nThe result is a fully on-device image transformation playground that tests the best of open-source conditional image generation.\n(a) (b) Figure 2. My custom iOS app. In the original image, Sabrina Carpenter is performing at the 2024 Governors Ball in Queens, New York. My stable diffusion pipeline successfully transports her to the interior of Balboa Park in San Diego, California. One final filter applies a stylized aesthetic, transforming the result into an album-cover candidate. For the full story, including technical details, check out my corresponding \u0026ldquo;Frames Per Second\u0026rdquo; blog series:\nPart 1: The Hunt for a Tiny, High-Quality Diffusion Model Part 2: Quantization, Kernels, and the Path to On-Device Diffusion Part 3: Turning a Tiny Diffusion Model into a Traveling Photobooth The complete source code, benchmarks, and project notes are available on GitHub. Sometimes it only takes a few frames per second to generate the image you want: no cloud, no fuss, no hassle.\n","date":1764460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764460800,"objectID":"a654479dcc308e6ea1f469779b7e78b1","permalink":"https://bellanich.github.io/portfolio/edge-diffusion/","publishdate":"2025-11-30T00:00:00Z","relpermalink":"/portfolio/edge-diffusion/","section":"portfolio","summary":"A study in pushing state-of-the-art computer vision models to their performance limits on iOS, balancing model capacity against real-world edge constraints","tags":["Code"],"title":"Frames Per Second: Low-Latency Conditional Image Generation on a 2GB Memory Budget","type":"portfolio"},{"authors":["Bella Nicholson"],"categories":null,"content":"Table of Contents Table of Contents Introduction A Three-Stage Approach Stage 1: Person Segmentation (~1s) Stage 2: Conditional Background Generation Prompt Engineering Thread Safety and Process Survival Stage 3: Compositing \u0026amp; Style Filters (\u0026lt;1s) Lessons from the Edge 1. At small scales, hardware-aware wins 2. If your prompt loses focus, Stable Diffusion will too 3. Architecture solves capability gaps Conclusion Future Directions The Joy of Building Small Introduction I\u0026rsquo;m on a personal mission to recreate the Nano Banana experience on the edge\u0026hellip;or at least get as close as physics and open-source tools will allow. In my first blogpost, I explained why Apple\u0026rsquo;s CoreML Stable Diffusion (SD) is my best bet. In the last post, I broke down how Apple squeezed a 6GB model down to 1.5GB while still delivering sub-10-second generation on iOS.\nBut there\u0026rsquo;s catch: Apple’s implementation breaks my use case. If I try a simple img2img portrait edit, the subject\u0026rsquo;s identity collapses. As seen in Figure 1, once I make the denoising strength high enough to change the background, my subject morphs into a loosely related stranger who just happens to be wearing a similar outfit.\nFigure 1. Apple's CoreML Stable Diffusion models fails to maintain character consistency. We attempt to transport the lovely Sabrina Carpenter from the 2025 MTV Video Music Awards (image credit) to a festive holiday backdrop. The background and prop swaps are convincing, but the resulting blonde is definitely not Sabrina. If I rely solely on Apple\u0026rsquo;s CoreML Stable Diffusion model, I run into an impossible trade-off:\nLow strength (0.3-0.5): Character consistency is maintained, but the background barely changes High strength (0.7-0.9): Background transforms perfectly to align with the given text prompt; however, the person pictures just becomes unrecognizable. This is hardly a surprise, since (a) the original Nano Banana model (released in August 2025) broke the internet for its ability to maintain character consistency and (b) we\u0026rsquo;re working with a hyper-optimized version of a 2022 model. The problem with our approach is that Stable Diffusion can\u0026rsquo;t distinguish between \u0026ldquo;keep this\u0026rdquo; and \u0026ldquo;change that\u0026rdquo;. It\u0026rsquo;s trying to equally transform each pixel. Hence, it\u0026rsquo;s trying to do two conflicting jobs at once: preserve user identity and dramatically transform the background.\nThe lesson? Stop asking Stable Diffusion to multitask. I need to handle identity preservation and scene transformation separately. This blogpost shows how to do this on a shoestring compute budget.\nA Three-Stage Approach I’m a fan of simple solutions, especially under a tight runtime budget. So, I started with the simplest move possible: I isolated the subject and focused Stable Diffusion\u0026rsquo;s efforts on background generation. This created a lightweight, three-stage pipeline:\nSegment. I used Apple\u0026rsquo;s Vision framework to perform person segmentation. This process yields (a) a cutout of the person with transparent background, and (b) an inverted mask marking which pixels need regeneration.\nGenerate. I feed the inverted mask and text prompt into Stable Diffusion\u0026rsquo;s img2img pipeline. SD regenerates only the masked background regions while leaving the subject\u0026rsquo;s pixels untouched.\nComposite. I then layer the original subject cutout over the newly generated background. In order to deliver a photo booth-like user experience, I also added optional Instagram-style filters to make the final outputs more shareable.\nFigure 2. My hyper-optimized three-stage pipeline uses Apple's CoreML Stable Diffusion model for on-device conditional image generation. By isolating the heavy diffusion step to background regeneration, the system preserves identity consistency despite the tiny model's quality constraints. The final result is a lean, fully on-device conditional image generation pipeline that runs within an average of ~27 seconds. This puts me safely below my 60 second limit.\nNow, let\u0026rsquo;s dive into the details. To demonstrate each stage\u0026rsquo;s output, we\u0026rsquo;ll successfully transport Sabrina Carpenter from the concert stage to a Winter Wonderland.\nFigure 3. Let's assume this concert photo of Sabrina Carpenter, performing onstage during the 2024 Coachella Valley Music and Arts Festival, as our running example. Our goal is to transport her to a Winter Wonderland (image credit). Stage 1: Person Segmentation (~1s) First, I extract the subject from the background using Apple\u0026rsquo;s Vision framework, specifically the VNGeneratePersonSegmentationRequest API. This built-in segmentation model ships with iOS and is already optimized for the Neural Engine.\nDeploying Apple\u0026rsquo;s off the shelf solution let\u0026rsquo;s me focus on core problem without getting distracted by additional deployment overhead. Apple has already hyper-optimized this image segmentation model for their Apple Neural Engine (ANE) hardware accelerator. Meaning, even when I set the preferred quality level to high, the segmentation model still returns a result within ~1 second. I\u0026rsquo;ve alloted about ~67% of my inference time budget to Stable Diffusion (40 seconds) and the remainder to everything else. Keeping the segmentation step leaves me with plenty of breathing room.\nFigure 3 shows an example of this model\u0026rsquo;s outputs, where its separates the subject from her surroundings.\n(a) (b) Figure 4. Sample person segmentation results. (a) The subject is now isolated against an alpha transparency background. (b) A background mask for subsequent conditional image generation, where only white pixels will be repainted. Now, let’s whisk Sabrina Carpenter off the Coachella stage and drop her straight into a glittery Winter Wonderland for a festive, snow-dusted performance.\nStage 2: Conditional Background Generation This step is where the magic happens — and where most of my runtime budget disappears. I feed the background mask from Stage 1 (see Figure 4C) and the text prompt into the Stable Diffusion. The mask acts like a stencil: white regions get regenerated, black pixels (the subject) stay untouched. Everything gets resized to 512×512 before inference, since that’s SD’s native training resolution.\nFor denoising strength, I stayed within the commonly recommended 0.65–0.85 range: low enough to preserve subject boundaries, high enough to meaningfully transform the background. I used the standard 25 DPM-Solver steps and set the default guidance scale to 7.5.\nPrompt Engineering Prompt engineering took longer than I\u0026rsquo;d like to admit. I wanted to create a visually striking background, so I started with maximalist prompts (\u0026quot;A winter wonderland with snow-covered pine trees, twinkling fairy lights, ice sculptures, frosted windows...\u0026quot;). CoreML Stable Diffusion got overwhelmed and returned incoherent mush. Then I went ultra-minimal (\u0026quot;A winter scene\u0026quot;) and got a bleak, featureless white void.\nThe sweet spot was photography-style phrasing with a few concrete details, like \u0026quot;A glittery winter wonderland with snow, twinkling lights, warm glow\u0026quot;. Enough direction, not enough to overwhelm. Along the way, I learned:\nStable Diffusion trims anything past ~75 tokens Evocative scene vibes are better than itemized lists Lighting cues, like “warm orange glow” vs. “blue hour twilight”, can set the entire mood Now, let’s see what all that work actually produces. Here’s the raw background Stable Diffusion generated before the subject gets composited back in (Figure 5).\nFigure 5. Stable Diffusion’s raw output for the prompt “outside in magical winter village at blue hour lighting, charming snow-covered cottage with glowing windows.” The scene is coherent, though not perfect, details like the opaque “window/door” remain ambiguous. Thread Safety and Process Survival Running a Stable Diffusion pipeline on-device means juggling two hard problems:\nThread safety. Segmentation, SD inference, and UI updates all touch the same shared state, creating the perfect incubator for race conditions. Process survival. I need to keep the UI responsive while SD runs for ~27 seconds in the background. At the same time, iOS locks the screen after 30 seconds of inactivity and suspends the app, which kills image generation. In short, I had to choose between concurrency or chaos. I enabled Swift 6\u0026rsquo;s strict concurrency to catch threading bugs at compile time rather than dealing with surprises in production. With strict concurrency, everything needs explicit actor boundaries. The UI state (@Published properties, view model updates) runs on the main thread, while Stable Diffusion inference runs on background threads to prevent freezing the entire app.\n// Simplified coordinator pattern func generateBackground() { isProcessing = true // MainActor UI update Task.detached { // Background thread for heavy work let result = await pipeline.generate(...) await MainActor.run { // Back to MainActor for UI self.outputImage = result self.isProcessing = false } } } Figure 6. Actor coordination pattern in Swift. The thread hopping pattern runs inference on a background thread, then returns to the main thread (@MainActor) for UI updates. I also registered Stable Diffusion as a background task to ensure image generation continues if the screen locks. Without it, we\u0026rsquo;d be left with half a cottage and no Winter Wonderland magic. Once the final image is composited, the background task is released.\nStage 3: Compositing \u0026amp; Style Filters (\u0026lt;1s) With the background generated, I\u0026rsquo;m ready to layer the isolated subject (Fig. 4a) into the new scene. I use Core Graphics Apple\u0026rsquo;s low-level 2D rendering framework, to composite these two layers together. This process is fast, clean, and basically free in terms of runtime.\nFigure 7. Let's assume this concert photo of Sabrina Carpenter, performing onstage during the 67th Annual GRAMMY Awards, as our running example. Our goal is to transport her to a Winter Wonderland (image credit). Originally, I planned to chain multiple Stable Diffusion generations together to create a flexible style transfer experience, allowing the user to further personalize their images, but each extra pass incurs another ~25 seconds. No one is going to wait 2+ minutes to try on a different look.\nSo I switched to Core Image filters, which run instantly. I added four curated styles plus an intensity slider, letting users experiment in real time, turning the whole system into a fully on-device, pocket-sized photobooth. Figure 8 highlights a few results, any of which could slide neatly onto Sabrina’s holiday-themed merch.\n(a) Vintage (b) Pop Art (c) Posterize (d) Mosaic Figure 8. The same composited image with different Core Image filter styles applied. Each filter applies instantly (\u0026lt;1s) with adjustable intensity. Of course, this system isn’t a one-hit wonder. Figure 9 shows the same pipeline dropping Sabrina underneath a Christmas tree, into San Diego’s Balboa Park, and even into a groovy reimagining of La Jolla Cove, proving that this pocket-sized photobooth travels just as well as she does.\n(a) Gradient Gift Descent (b) Balboa Park (c) La Jolla Cove Figure 9. Sabrina Carpenter, re-imagined in three different scenes. Show both the raw Stable Diffusion generated backgrounds (top) and the final composites (bottom). Lessons from the Edge When I started this project, I knew bringing the Nano Banana experience to mobile would be tough, but I just didn’t realize how tough. I learned four valuable lessons along the way:\n1. At small scales, hardware-aware wins Users won’t accept bad images just because the model runs fast. Once we shrink diffusion, quality depends heavily on hardware we don’t control or fully understand. Apple’s vertical integration makes some optimizations look effortless, but replicating them from the outside is anything but.\n2. If your prompt loses focus, Stable Diffusion will too I learned quickly that mixing themes (e.g., Winter Wonderland + robots) just produces incoherent mush. Chaining prompts or tweaking denoising strength didn’t help either: high strength erased the scene, low strength lead to incoherent, blurry transformation.\nBlending multiple semantic concepts is a completely different problem, one tackled by disentangled-control methods like ControlNet and IP-Adapter. But those techniques rely on extra conditioning modules that add hundreds of megabytes and several seconds of latency. That\u0026rsquo;s fine on a workstation, disastrous for a sub-30-second mobile experience.\n3. Architecture solves capability gaps Splitting the process into Segment → Generate → Composite avoided the pitfalls of an all-in-one model. Segmentation preserved identity, SD produced high-quality backgrounds, and fast filters enabled rapid style iteration. Even if on-device disentangled-control were possible, the gains for the end user would be minimal. The modular workflow already delivers strong speed and consistency. This model has its limitations, but good system design works around them.\nOf course, as base model improves, so does the space for smarter systems built around its new limits.\nConclusion I accomplished my original goal of building a Nano-Banana–style photobooth that runs entirely on-device. Along the way, I also ended up with a compact computer-vision playground (Figure 10) ready for whatever comes next.\n(a) (b) Figure 10. My custom iOS app acts as an edge computer vision playground. In this example, I transform Sabrina Carpenter's 2024 Governors Ball performance (image credit) into an album-cover candidate. All image generation occurs on my iPhone 16's dedicated Neural Processing Unit. Future Directions From here, my options are boundless but they include: automating the tedious prompt engineering process using reinforcement learning techniques like DDPO, trying to recreate some aspects of identity-preserving style transfer on-device, or fine-tuning my tiny Stable Diffusion model with LoRA adapters so it knows why Sabrina Carpenter is \u0026ldquo;Man\u0026rsquo;s Best Friend\u0026rdquo;. I now have a solid foundation and free to explore what’s genuinely interesting.\nThe Joy of Building Small A GPU cluster would brute-force most of the problems I hit, edge constraints push me to invent better solutions. With tiny, local models, I skip cloud overhead, iterate faster, and avoid unwanted surprise bills. Starting at the bottom of the scale curve is liberating: any capability I unlock here will only get stronger as the model scales. And in the end, the setup stays small, but the possibilities don’t.\nI’m excited to keep iterating on this work. If you want to dig into the details, the full implementation is on GitHub. Questions, feedback, or wild ideas? Drop a comment or reach out on LinkedIn. I always enjoy meeting people working at the edge of what\u0026rsquo;s possible — on-device or in the cloud.\n","date":1764115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764115200,"objectID":"2d74878170dd98f0b217cd9920cca988","permalink":"https://bellanich.github.io/post/edge-diffusion-3/","publishdate":"2025-11-26T00:00:00Z","relpermalink":"/post/edge-diffusion-3/","section":"post","summary":"A tiny diffusion model, a mobile device, and a surprising amount of magic — here’s how I built a pocket-sized photobooth that can whisk real people into new worlds in under 30 seconds.","tags":["Generative AI","Edge ML","Computer Vision","Article"],"title":"Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction Apple\u0026rsquo;s Optimization Strategy Model Compression Phase 1: Quantization Phase 2: Palettization Neural Engine Optimization Model Chunking Attention Variants 1. The Original Attention Mechanism 2. Split Einsum Attention Scheduler Optimization Conclusion What\u0026rsquo;s next? Introduction After tackling text with my edge multi-modal LLM project last year, I\u0026rsquo;ve become fascinated with the image side of foundation models. The media frenzy ignited by Google DeepMind\u0026rsquo;s initial Nano Banana release acts as a testament to how image generation just hits differently than text.\nThis begs the ultimate question: Can we deliver a Nano Banana-like experience on the edge? The answer isn\u0026rsquo;t simple. Diffusion models are brutally sensitive to noise. In my last blog post, my naive port of Tiny SD failed spectacularly, yielding noisy and psychedelic outputs.\nAs a result, I\u0026rsquo;ve turned my attention to Apple\u0026rsquo;s CoreML Stable Diffusion model, betting its proprietary, hardware-aware design will save this project. How did Apple\u0026rsquo;s engineers successfully squeeze a 6GB model into a sub-2GB, sub-10-second iOS package while maintaining quality? Their secret lies in the careful orchestration of quantization, hardware co-design, and architectural compromises.\nFigure 1. Prototyping testing. A generated image from the text prompt \"A beautiful landscape with mountains and a lake, golden hour lighting\" using Apple’s CoreML Stable Diffusion model. The output is a high-quality, realistic rendering of a rustic mountain range at sunset. This post dissects exactly how Apple pulled off that optimization miracle. By examining which layers got quantized, how Neural Engine constraints shaped the architecture, and where the remaining quality trade-offs live, we\u0026rsquo;ll be ready to build our edge conditional image generation experience in this blogpost series\u0026rsquo;s conclusion.\nApple\u0026rsquo;s Optimization Strategy Let\u0026rsquo;s start with the numbers. Apple took Runway ML\u0026rsquo;s 6GB Stable Diffusion implementation, compressed it by over 70%, and then embedded it onto their proprietary Apple Neural Engine (ANE) hardware. The ANE is the third processor in Apple Silicon, sitting alongside the standard CPU and GPU. It\u0026rsquo;s a dedicated Neural Processing Unit (NPU) purpose-built for the high-throughput matrix operations that define neural network inference.\nFigure 2. An image by generated by the Runway ML's 6GB Stable Diffusion model, which served as the base model for Apple's CoreML implementation (image credit). Figure 3. Sample images from Apple's official CoreML Stable Diffusion demo. As seen, there's some quality degradation when compared to Fig. 2, but the majority of model quality is preserved (image credit). This hyper-specialized hardware enabled Apple to achieve blisteringly fast results: each denoising step takes only ~0.37–0.39 seconds and a high-quality images are generated within 20 steps (documented here). Of course, Apple didn\u0026rsquo;t use a single optimization technique to achieves these impressive runtimes; rather, they employed an entire optimization playbook:\nModel precision reduction. Apple applies a crucial two-step precision reduction process of (a) initial quantization from float32 to float16 to halve memory consumption and (b) followed by aggressive palettization to 6-bit weights to yield another ~40-50% reduction. That\u0026rsquo;s how we went from a 6GB model to a 1.5GB one.\nNeural Engine optimization. CoreML\u0026rsquo;s ANE-specific compilation pipeline fuses common machine learning operators and optimizes tensor memory layouts for the ANE\u0026rsquo;s specialized compute units. Apple\u0026rsquo;s own benchmarks show that ANE-optimized models achieve up to 10× speedup with 14× reduction in peak memory consumption compared to unoptimized implementations on the iPhone 13.\nModel chunking. The pipeline is split into four independently loadable components (.mlmodelc files). iOS dynamically swaps these components in the reduceMemory option to keep peak memory usage below the 2GB limit. The trade-off is increased end-to-end latency due to this just-in-time loading overhead as seen in their official benchmarks.\nAttention implementation. Apple uses a SPLIT_EINSUM attention variant. It breaks multi-head attention into explicit single-head functions and relies on einsum operations to avoid the reshape and transpose steps that trigger memory copies. Because the ANE excels at fixed 4D tensor layouts, keeping data in this shape is crucial. Combined with Apple’s other optimizations, this approach delivered up to 10× faster inference and 14× lower peak memory on their distilbert benchmark.\nScheduler Optimization. The original model was used the PNDM (Pseudo-numerical methods for Denoising Models) scheduler, which requires 50+ computational steps. Apple swapped this for the modern DPMSolverMultistepScheduler. PNDM uses estimates from recent steps to predict the denoising direction, allowing it to learn from recent history to make smarter jumps. This drastically reduces the required denoising steps from 50+ to 20-25 without sacrificing image quality.\nEach optimization technique targets a different performance bottleneck, whether it be memory footprint, inference latency, or peak memory usage. Let\u0026rsquo;s now examine each technique in detail.\nModel Compression Apple engineers pulled off an impressive 70% size reduction through a two-phase compression strategy. They started with a straightforward precision reduction from float32 to float16 and followed up with palettization to jump from float16 to just 6-bit.\nPhase 1: Quantization Model parameters in standard PyTorch models consume 32 bits (float32), making the 1.3B parameter Stable Diffusion consume 5.2GB of RAM. In quantization, we lower the bit-precision used to store each weight, sacrificing some information for a smaller memory footprint.\nThe first step is to halve the precision to float16. CoreML achieved this using simple datatype casting (no algorithmic quantization, clustering, or lookup tables), which instantly halves memory with minimal accuracy loss. This simple technique works because float16 is considered the \u0026ldquo;safe\u0026rdquo; floor for neural networks. This precision level retains enough dynamic range (the span of representable values) and precision to accurately encode most model weights.\nThis reduces memory consumption from 5.2GB to 2.6GB, which is a great start but above my sub-2GB target. If we naively lower the precision any further, we risk catastrophic information loss and would get corrupted outputs like those from my Tiny SD port. Meaning, we need a more clever quantization technique.\nPhase 2: Palettization Apple wanted to build a generalizable framework for compress any Stable Diffusion checkpoint, including community fine-tunes. Hence, they needed a flexible approach that didn\u0026rsquo;t depend on access to the original training data. This immediately rules out state-of-the-art methods like AWQ or GPTQ, which require calibration data to analyze activations and identify the most salient (important) weights.\nPalettization offers a perfect alternative. It achieves aggressive compression without calibration data by using simple, interpretable k-means clustering. This technique actually originates from color quantization in computer graphics. Essentially, during image compression, we needed to map millions of possible colors in an image to a smaller fixed set of representative values.\nFigure 4. An example of color quantization, where the original photograph palette was reduced seven distinct colors (image credit). The same logic can be applied to model weights. Here\u0026rsquo;s how it works:\nAnalyze the distribution of weights in each layer Cluster similar weights using k-means to create a \u0026ldquo;palette\u0026rdquo; of representative values (e.g., 64 values for 6-bit palettization, since $2^6 = 64$) Replace each original weight with an index pointing to its nearest palette entry Store the compact palette plus the many small indices instead of full-precision weights The bit-width determines how many palette entries you get, and thus your compression ratio. Table 1 summarizes the trade-offs in palette entry number selection.\nBit Width Palette Entries Compression vs Float16 Quality Impact 8-bit 256 ~2× Minimal quality loss 6-bit 64 ~2.67× Acceptable quality trade-off 4-bit 16 ~4× Noticeable degradation 2-bit 4 ~8× Severe quality issues Table 1. Palettization bit-width options and their associated trade-offs. For our selected CoreML Stable Diffusion variant, Apple used 6-bit palettization to achieve a final ~1.5GB model size. For even larger models like SDXL (6.94 GB), Apple used mixed-bit palettization for stronger model compression. Here, we assign different bit-widths (1, 2, 4, 6, or 8 bits) to different layers based on a sensitivity analysis.\nNeural Engine Optimization Apple\u0026rsquo;s Neural Engine (NE) debuted in 2017 inside the iPhone X\u0026rsquo;s A11 Bionic chip to power Face ID. The TrueDepth camera fires over 30,000 infrared dots to map your face, and handles that data in real time. That stream in real time was too slow and too power-hungry for the GPU, pushing Apple to develop their own NPU.\nThat first-gen ANE delivered 0.6 teraflops of float16 compute. By 2018, with release of the A12 chip and Core ML, Apple opened the Neural Engine to developers, and today it’s baked into every modern iOS device.\nFigure 5. The evolution of the Apple Neural Engine from 2017 to 2021. The 16-core Neural Engine on on the A15 Bionic chip (iPhone 13 Pro) has a peak throughput 26 times higher than its original counterpart. (image credit). But where does Core ML fit into all this? Since ANE is proprietary hardware, there’s no public API to program it directly. Its architecture, instruction set, and compiler are all trade secrets. With no official documentation on ANE-supported operations or optimization methods, most developer knowledge comes from trial-and-error and reverse engineering. Core ML is the only way iOS developers can access the Neural Engine.\nIt consists of two parts:\ncoremltools is an open source Python package that converts models from frameworks like PyTorch and TensorFlow into Core ML\u0026rsquo;s optimized format The on-device Core ML framework that loads these compiled models and executes them. When you convert a model with coremltools, it figures out which operations can run on the ANE versus the CPU or GPU, applies optimizations, and compiles the model into an efficient format. At runtime, Core ML then routes each operation to the right compute unit to maximize performance and minimize power use.\nCoreML gives you three ways to run unit neural networks on device:\nCPU Only. This is the slowest but most safest option. According to the ONNX Runtime documentation, CPU-only mode is mainly available for debugging and validation, since it avoids precision differences and guarantees predictable results. Community benchmarks suggest it runs approximately 7-8x slower than optimal configurations, making it impractical for real-time generation.\nCPU and GPU. This combination is capable but not recommended. GPUs were originally built for desktops with unlimited power, so they’re plausible but not ideal for running heavy models on mobile devices. It\u0026rsquo;s typically used for Macs with powerful GPUs or as a fallback for older devices without a Neural Engine.\nCPU ane ANE. This is Apple\u0026rsquo;s recommended configuration for deploying intensive models on iPhones and iPads. ANE was specifically designed for ML inference workloads and delivers comparable performance to GPU at a fraction of the power consumption.\nIn our case, I configured coreml-stable-diffusion-v1-5-palettized to run primarily on the Neural Engine with CPU fallback for unsupported operations. This hybrid approach maximizes performance where it counts while maintaining graceful degradation for edge cases.\nModel Chunking iOS enforces stricter memory constraints than macOS. As noted in Apple\u0026rsquo;s CoreML optimization guide,\nANE\u0026rsquo;s specialized architecture comes with strict model size constraints. iOS enforces stricter per-file memory mapping limits than macOS. While the exact limit is undocumented, Apple\u0026rsquo;s optimization guide suggests it\u0026rsquo;s around 1GB based on their compression targets for mobile deployment. Meaning, attempting to load our U-Net component, which is roughly 1.5GB in float16 precision, will iPhone triggers memory allocation failure, even if it\u0026rsquo;d run perfectly on a Mac.\nThis makes model chunking essential for mobile deployment. The idea is simple: we split huge weight files into smaller slices that fit within iOS’s memory limits, and let the runtime load each slice on demand. Apple\u0026rsquo;s ml-stable-diffusion repo handles this automatically with the --chunk-unet conversion flag flag, which divides the U-Net weights into multiple files that stay well under the limit. These chunks are stored in the .mlmodelc format, a pre-compiled, ANE-optimized layout that improve loading time.\nThe beauty of Apple\u0026rsquo;s setup is that developers never have to think about model chunking. Core ML handles this behind the scenes. While there\u0026rsquo;s a small cost to pulling in multiple files, we wouldn\u0026rsquo;t be able to run Stable Diffusion on iOS without this approach.\nAttention Variants Apple\u0026rsquo;s CoreML conversion tools offer two attention implementations that compute identical mathematical operations but differ critically in their kernel implementation.\n1. The Original Attention Mechanism This implementation uses the standard batched multi-head attention formula:\n# Shape: [batch, seq_len, heads * head_dim] Q, K, V = linear_projections(x) # Reshape to [batch, heads, seq_len, head_dim] Q = Q.reshape(batch, seq_len, heads, head_dim).transpose(1, 2) K = K.reshape(batch, seq_len, heads, head_dim).transpose(1, 2) V = V.reshape(batch, seq_len, heads, head_dim).transpose(1, 2) # Batched matrix multiplication across all heads # [batch, heads, seq_len, head_dim] attention = softmax(Q @ K.transpose(-2, -1) / sqrt(d_k)) @ V # Reshape back output = attention.transpose(1, 2).reshape(batch, seq_len, heads * head_dim) Figure 6. Pseudocode of the original attention mechanism. This works well on CPUs and GPUs, which handle dynamic reshaping efficiently. It\u0026rsquo;s faster on Macs with discrete GPUs (M1 Pro/Max/Ultra) where memory bandwidth isn\u0026rsquo;t the primary bottleneck.\n2. Split Einsum Attention ANE penalizes non-contiguous memory access, which makes the reshape/transpose operations shown in Figure 6 computationally expensive. Fortunately, we can rewrite matrix multiplication as a series of Einstein summations (einsums) as shown in Equation 1 to better utilize ANE. $$ C_{ik} = \\sum_{j} A_{ij} B_{jk} = AB = C \\tag{1} $$\nBy keeping keeps tensors in fixed 3D layouts and using the einsum operation, we avoid generating unnecessary memory copies. The implementation looks something like this:\n# Shape: [batch, seq_len, heads * head_dim] Q, K, V = linear_projections(x) # Split into explicit per-head tensors (no reshape) Q_heads = [Q[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)] K_heads = [K[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)] V_heads = [V[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)] # Compute attention per head using einsum (preserves 3D tensor layout) outputs = [] for Q_h, K_h, V_h in zip(Q_heads, K_heads, V_heads): # [batch, seq_len, seq_len] scores = torch.einsum('bqd,bkd-\u0026gt;bqk', Q_h, K_h) / sqrt(head_dim) attn = softmax(scores, dim=-1) out = torch.einsum('bqk,bkd-\u0026gt;bqd', attn, V_h) # [batch, seq_len, head_dim] outputs.append(out) # Concatenate (cheap operation) output = torch.cat(outputs, dim=-1) # [batch, seq_len, heads * head_dim] Figure 7. Pseudocode of the einsum attention variant. The trade-off is lower parallelism, since we\u0026rsquo;re using explicit per-head loops rather than batched operations. This hurts GPU performance, but ANE performance is bottlenecked by memory bandwidth. Meaning, the memory savings outweigh the costs of reduced parallelism.\nScheduler Optimization Diffusion models turn random static into art by clearing away noise. The scheduler (also called a sampler or solver) is the control algorithm that orchestrates the denoising loop: it calls the U-Net at each timestep to predict the noise, then uses its mathematical formula to update the image toward a cleaner state. Meaning, the scheduler controls the number of diffusion steps needed for high-quality image generation. If we select a more efficient scheduler, we can improve inference time without degrading quality.\nThe original Stable Diffusion models used the PNDM (Pseudo Numerical Methods for Diffusion Models) scheduler, which applies a linear multi-step method:\n$$ x_{t-1} = x_t + \\sum_{i=0}^{k-1} \\alpha_i \\cdot \\epsilon_\\theta(x_{t-i}, t-i) \\tag{2} $$\nwhere $x_t$ is the current noisy image at timestep $t$, $\\epsilon_\\theta$ predicts what noise to remove, and $\\alpha_i$ are coefficients that weight predictions from the past $k$ steps. As seen in Equation 2, PNDM treats each timestep as a discrete prediction problem, where each step uses local information (the last few predictions). In this context, larger jumps (more noise removal per step) risk error accumulation, which lowers image quality. PNDM tends to require ~50 diffusion steps to yield acceptable outputs.\nThe DPMSolverMultistepScheduler treats denoising as a continuous process rather than discrete jumps. Since noise is added gradually during training, it can be removed along a smooth, continuous path that written as an Ordinary Differential Equation (ODE):\n$$ \\frac{dx_t}{dt} = f(t) x_t + g(t) \\epsilon_\\theta(x_t, t) \\tag{3} $$\nThis makes diffusion a continuous process and allows the DPM Solver to take larger, more informed steps through the denoising trajectory. As a result, 25 steps with DPM Solver produces quality comparable to 50 steps with PNDM, offering a 2× speedup.\nThis made it the DPM Solver an obvious choice for Apple\u0026rsquo;s CoreML implementation, where every second of latency matters. The step count creates a direct quality-speed trade-off:\nStrategy Steps Runtime Quality Impact Use Case Aggressive 15-20 10-15 seconds Noticeable artifacts, loss of fine details Quick previews, concept iteration Balanced 20-30 15-25 seconds High-quality results, minimal artifacts Production deployment Conservative 50+ 35+ seconds Marginal improvement over 25 steps Not worth the extra latency on mobile Table 2. Trade-off between diffusion steps and image quality when using the DPMSolverMultistepScheduler. After extensive testing, I settled on 25 steps as my default to properly balance my need for quality and speed.\nConclusion Apple\u0026rsquo;s CoreML Stable Diffusion represents a masterclass in optimization engineering. With aggressive quantization, ANE-friendly attention kernels, and smart scheduling, Apple squeezed a 6GB model into a 1.5GB package that can generate an image in under 10 seconds on an iPhone. It\u0026rsquo;s a technical flex that\u0026rsquo;s hard to overstate.\nBut here\u0026rsquo;s an uncomfortable truth: optimization doesn’t expand capabilities. Apple solved how to run Stable Diffusion on mobile — not whether it\u0026rsquo;s good enough. Strip away the speedups, and we\u0026rsquo;re left with the a 2022-era model:\nPoor identity preservation. Try img2img at high denoising strength and watch faces dissolve into uncanny abstractions. The model simply can\u0026rsquo;t maintain coherent identity while delivering image transformation.\nPrompt adherence is weak. Compared to SDXL or Flux, SD 1.5 treats our carefully crafted prompt more like a vague suggestion than a clear set of instructions.\nAs a result, I can\u0026rsquo;t just port Apple\u0026rsquo;s approach.\nWhat\u0026rsquo;s next? My goal is conditional edge image generation with an explicit need for character consistency. If Apple\u0026rsquo;s optimizations give us the blueprint for mobile deployment, what architecture actually delivers my required capabilities?\nThat\u0026rsquo;s what I\u0026rsquo;ll cover in my next and final blogpost. My goal isn’t just to run fast; it’s to run fast and look good doing it.\n","date":1764028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764028800,"objectID":"3dda4703ea71ed8f30ada2a96d75d30b","permalink":"https://bellanich.github.io/post/edge-diffusion-2/","publishdate":"2025-11-25T00:00:00Z","relpermalink":"/post/edge-diffusion-2/","section":"post","summary":"This post unpacks how quantization, ANE-optimized kernels, and smart schedulers shrink a 6GB diffusion model into a fast, mobile-ready package.","tags":["Generative AI","Edge ML","Computer Vision","Article"],"title":"Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction Problem Constraints Beyond the Noise: Unpacking the Architecture of Diffusion Models Model Architecture Text-to-Image vs. Image-to-Image Generation Nano Banana: Today\u0026rsquo;s State of the Art The Model Search Tiny SD: Starting as Small as Possible CoreML Stable Diffusion Conclusion What\u0026rsquo;s next? Introduction I have a problem: I love testing out and applying the latest ML research, but I really dislike managing my own cloud infrastructure. That\u0026rsquo;s why I ended up embedding a multimodal LLM on various edge devices last year. However, generated text doesn\u0026rsquo;t deliver the same immediate, visceral impact that high-quality images do, compelling me to switch domains.\nUnfortunately, deploying a state-of-the-art (SOTA) diffusion model on the edge is far harder than its LLM counterpart. LLMs work in a discrete output space (i.e., tokens). Thus, they tolerate the noise of simple compression algorithms relatively well. In contrast, diffusion models operate on a continuous, dense latent space, causing the same amount of noise to more severely degradate model performance. Attempting to shrink a 4GB model to fit the standard 2GB iOS memory budget is a brutal performance problem. For better or worse, this is my favorite type of problem to solve.\nProblem Constraints To keep things interesting, I decided to target deployment directly for my iPhone 16 (~8GB of RAM). If this model can run effectively on my phone, I\u0026rsquo;ll always have a tiny, powerful image generator right in my pocket. However, this choice immediately imposed a very strict iOS memory budget. iOS apps encounter a dynamic limit from ~2-4 GB (roughly 50-70% of total RAM) that triggers an EXC_RESOURCE RESOURCE_TYPE_MEMORY termination exception and crashes the app.\nOf course, compressing the model is only half the battle. If it\u0026rsquo;s too slow, any reasonable user will just quit the app, rendering the entire point moot. Hence, I set a 60-second end-to-end limit for the pipeline, allotting 40 seconds for model inference.\nThis high bar for speed and precision demanded a pipeline built around conditional image generation (Image-to-Image or Img2Img). This is essential because it:\nProvides superior creative control and output fidelity. Elevates the ML-side challenge by requiring hands-on control of the model\u0026rsquo;s neural network sub-components Delivers a more engaging user experience by actively transforming the source photo The final, non-negotiable rule was that the output images needed to be of reasonable quality. Meaning, the model needs to generate easily identifiable objects that are in-line with the provided text prompt.\nBeyond the Noise: Unpacking the Architecture of Diffusion Models The path to modern image generation was surprisingly quick. Open AI released DALL-E in January 2021, Stable Diffusion democratized the field in August 2022, and suddenly everyone had access to conditional image synthesis.\nThis new era of vision models is powered by diffusion, where the model learns to destroy images systematically and then reverses the process.\nFigure 1. A visualization of the diffusion process. Noise is added to (forward pass) or removed from the image (reverse processes) (image credit). We start with the original image $x_0$ and progressively corrupt it by adding Gaussian noise over $T$ timesteps. At each step, we keep some fraction of the previous image and add fresh noise:\n$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$$\nwhere $\\beta_t$ controls the noise intensity. This defines a Markov chain, a sequence of random states where each state $x_t$ depends only on the immediately previous state $x_{t-1}$, nothing earlier.\nThis Markov property ensures that while each image follows a different random path to noise, all images at timestep t share identical noise statistics (same signal-to-noise ratio). This predictable structure lets us train a neural network to predict the noise $\\epsilon$ added at any timestep, which we can then subtract to reverse the corruption. Once trained, the model generates images by starting with pure noise and iteratively denoising over 20-50 steps, guided by a text prompt.\nDuring training, we need noisy images at various timesteps to teach the model this denoising function. Stepping through $x_1 \\rightarrow x_2 \\rightarrow \\cdots \\rightarrow x_t$ sequentially for every training sample would be computationally infeasible. Fortunately, a key mathematical property of Markov chains with Gaussian transitions is that the entire sequence collapses into a closed-form solution:\n$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon $$\nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$ is random noise and $\\bar{\\alpha}_t = \\prod_{i=1}^{t}(1 - \\beta_i)$ accumulates all the noise scaling factors up to time $t$. This reparameterization trick lets us jump directly to any timestep in one shot, making model training computationally feasible\nModel Architecture The original diffusion paper introduced the denoising process, but it operated directly on pixels, making it computationally expensive. Stable Diffusion changed the game by running diffusion in a compressed latent space, which dramatically reduces computational costs while maintaining quality.\nFigure 2. A visualization of how Stable Diffusion's three neural components work together to do text-to-image generation. In image-to-image generation, a vision encoder also maps the original image to a latent space representation (image credit). Stable Diffusion is a composite of four neural networks:\nText encoder. We use a Contrastive Language-Image Pre-training (CLIP) network to convert text prompts into $77 \\times 768$ embedding vectors. These embeddings semantically link language to visual concepts, allowing our model to \u0026ldquo;understand\u0026rdquo; our text inputs. Image encoder. We use a Variational Autoencoder Encoder (VAE) to compress images from pixel space ($3 \\times H \\times W$) to a compact latent space ($4 \\times \\frac{H}{8} \\times \\frac{W}{8}$). This ~64× compression is the key efficiency innovation, since it lets us denoise the compressed representations rather than the full-resolution images. Image denoiser. The U-Net is the core component that implements the reverse diffusion process. It accepts noisy latents, the current timestep, and text embeddings (via cross-attention) to predict what noise to remove. This is the model\u0026rsquo;s heaviest component, consuming roughly 85% of the Stable Diffusion\u0026rsquo;s total memory footprint. The immense size is mandatory because the U-Net must model a universal, continuous denoising function spanning all timesteps and image content. This dense predictive complexity is precisely why the U-Net resists compression. Image decoder. The VAE decoder decompresses our latents back to high-resolution pixel-space images. This reconstruction is the final output that the model returns. Even with these efficiency gains, deploying Stable Diffusion on mobile devices requires further aggressive but non-destructive U-Net compression. This architecture supports two distinct generation modes, each with different performance characteristics.\nText-to-Image vs. Image-to-Image Generation Stable Diffusion supports two generation modes, each starting from a different point in the noise spectrum:\nText-to-Image (T2I) starts with pure random noise and relies solely on the text prompt to guide generation. This maximizes the model\u0026rsquo;s creative freedom, but means we get little control over the final image\u0026rsquo;s structure or composition. Image-to-Image (Img2Img) adds noise to an existing image\u0026rsquo;s latent representation, then denoises it while being guided by both the original image structure and a text prompt. This trades creative flexibility for precise control over image composition. The strength parameter sets how aggressively the model transforms its input. At $0.0$, the model returns the original image untouched. At $1.0$, the input is completely replaced with noise, making the output nearly identical to Text-to-Image generation. These two modes establish the foundational mechanics of image creation, but what happens when you scale that process to an unbelievable level of fidelity and control? That\u0026rsquo;s where Nano Banana Pro enters the frame.\nNano Banana: Today\u0026rsquo;s State of the Art In August 2025, Google DeepMind released Nano Banana, a Gemini model natively generates interleaved text and images. It excels at image editing thanks to its strong character-consistency across different image edits. And unlike most image-generation models, which still benefited from long, detailed prompts, Nano Banana performs well with simple instructions.\nFigure 3. An example of the viral \"Nano Banana\" trend, where users generated figures representations of themselves and their favorite film characters, including James Bond (image credit). These capabilities were striking enough to spark a viral and global social media trend, where users turned themselves into figurines (Figure 3).\nThree months later, Nano Banana Pro arrived. It extended its predecessor’s multi-character consistency to handle scenes with 10+ people (Figure 4) and dramatically improved text rendering, enabling designer-level infographics to be generated in minutes.\nFigure 4. Nano Banana Pro demonstrates an uncanny level of character consistency with its ability to merge 14 distinct, cute, and fuzzy characters into a cohesive scene (image credit). Figure 5 shows one such example: “Best Chocolate Around the World: A Global Taste Odyssey,” which illustrates how cocoa is grown, processed, and enjoyed across regions.\nFigure 5. Nano Banana Pro can generate beautifully illustrated and ultra-detailed infographic on demand. Consider this delicious example about where cocoa beans are grown and how they're turned into chocolate. However, both models remain closed-source and proprietary, so we can only infer how they work. We know performs some form of planning-style reasoning for image generation, because it can solve university level phsyics and chemistry problems by generating neatly written, correct solutions directly onto a blank exam page. It is, frankly, impressively capable. And since it is built on Gemini, it\u0026rsquo;s also probably too large to run on edge devices, even with aggressive model compression and optimization.\nFigure 6. Nano Banana Pro was able to successfully generate the correct solutions, including doodles, for university-level Physics and Chemistry exam questions (image credit). This leaves us with a clear goal: replicate as much of this functionality as possible using open-source, on-device alternatives. Unfortunately, current mobile-friendly diffusion models more closely resemble 2021–2022 Stable Diffusion systems.\nThe Model Search Since Nano Banana Pro\u0026rsquo;s advanced capabilities only emerge at massive scale, we have to accept two harsh realities:\nWe have to rely on open-source, convertible models that often lag 6-12 months behind industry SOTA; and The model we choose won\u0026rsquo;t have the same magical coherence of its cloud-scale counterparts. Simply put, I need to select a model that\u0026rsquo;s small enough to run on-device but still capable enough to produce usable results. We can translate our earlier problem constraints into the model specifications shown in Table 1.\nRequirement Specification Reasoning Size Memory footprint \u0026lt;2GB iOS apps face strict memory limits (~2-4GB), allocate 2GB primarily for model usage to prevent crashes Performance Inference \u0026lt;40 seconds Fits within 60-second end-to-end pipeline budget, leaves room for pre/post-processing Hardware Support Apple Neural Engine (ANE) optimization required Standard Metal GPU processing will be too slow, need leverage the iPhone's built-in AI accelerator Methodology Separate component access (CLIP, U-Net, VAE Encoder/Decoder) Conditional image-to-image generation is inherently modular, components must be accessed separate for img2img tasks Quality Maintain human subject identity with high-fidelity Core product requirement: failure to maintain character consistency leads to poor user experience. Table 1. Model specifications for my edge conditional image generation application Tiny SD: Starting as Small as Possible To establish a minimum viable quality baseline, I targeted the smallest available contender: Segmind\u0026rsquo;s Tiny SD. As a 55% parameter reduction of Stable Diffusion, it is among the most aggressively compressed models from an established maintainer. Since it only consumed half of my tight 2GB iOS memory ceiling, it was the perfect, low-risk candidate to stress-test the absolute lower bound of acceptable quality and performance.\nMy next move was optimizing tiny SD for speed. I used CoreML , Apple\u0026rsquo;s dedicated framework for integrating machine learning models into apps, to convert the weights into an Apple Neural Engine (ANE) optimized format. The ANE is the dedicated hardware accelerator built into Apple Silicon, specifically designed to run on-device neural network inference with superior power efficiency. Meaning, if this works, I will be able to conditionally generate images without killing my phone\u0026rsquo;s battery.\nIn my initial test, I wanted to validate the model\u0026rsquo;s basic text-to-image (T2I) generation. To keep this assessment fair, I used the same text prompt Segmind provided in their model card: \u0026quot;Portrait of a pretty girl\u0026quot;. Despite my best efforts to meet the 40-second deadline (via 25-30 diffusion steps), the model failed quality control. Instead of images, I was left with psychedelic noise and low-fidelity artifacts (Figure 7).\n(a) (b) Figure 7. The samples illustrate Tiny SD's quality collapse. (a) Default prompt at recommended guidance scale (7.5). (b) Enhanced prompt and an aggressive guidance (11.0) to force better prompt adherence. Both outputs were generated within the 25–30 step limit and exhibit severe artifacts and image distortions. Despite the artifacts, rough semantic alignment remains: Fig. 7A shows a framed \u0026ldquo;portrait\u0026rdquo; of a woman and Fig. 7B renders a woman with \u0026ldquo;flowing hair.\u0026rdquo; Crucially, Figure 8 confirms the original Tiny SD model produces coherent, acceptable (if blurry) outputs. This performance gap strongly suggests our CoreML pipeline is sound, but the model weights are being corrupted during the conversion or loading process.\nFigure 8. Official examples of Tiny SD outputs. These samples confirm the original Tiny SD is capable of generating coherent, if slightly blurry, portraits of people (e.g., center-left image), establishing an acceptable quality baseline prior to CoreML conversion (source). So, where did the weights go wrong? The corruption must have originated from one of these three technical suspects:\nInference step requirements. Distilled models often require higher step counts for convergence than their parent models, a detail missing from the Tiny SD documentation. Our current $25-30$ steps may be too few.\nCoreML quantization precision loss. CoreML\u0026rsquo;s model packaging applies weight quantization (typically FP16 or mixed precision) that could compound errors in an already-distilled model, potentially degrading performance below acceptable limits.\nVAE decoder corruption during CoreML conversion. The VAE decoder is the model component most sensitive to weight corruption . It is a critical single point of failure because it performs the final, irreversible $64\\times$ spatial upsampling. CoreML conversion might corrupt its transposed convolution weights, and (unlike the self-correcting U-Net) even the slightest VAE decoder corruption turns perfect latents into unusable outputs.\nPinpointing the exact cause of corruption would require a costly series of controlled ablation studies on the teacher PyTorch model: testing step counts, comparing FP32 vs. FP16 precision, and measuring degradation at each CoreML conversion stage.\nHowever, this investigation isn\u0026rsquo;t needed. This test already validates Tiny SD\u0026rsquo;s non-viability: I need CoreML/ANE compilation and a small number of diffusion steps to meet my strict sub-40-second latency budget. As seen, tiny SD can\u0026rsquo;t deliver under these constraints. It\u0026rsquo;s time to pivot to a model explicitly designed with Apple\u0026rsquo;s silicon in mind.\nCoreML Stable Diffusion The search for a viable replacement led me to Apple\u0026rsquo;s CoreML Stable Diffusion. This is a professionally tuned implementation of Runway ML\u0026rsquo;s stable-diffusion-v1-5 (1.3B parameters) that\u0026rsquo;s compressed into ~1.5GB.\nWhat makes this model viable where Tiny SD collapsed? Its key advantage is co-design with Apple\u0026rsquo;s hardware team. This grants engineers access to proprietary optimizations—like deep operator fusion and memory layout—to produce a calibrated FP16 model guaranteeing peak ANE performance unavailable through generic conversions. Crucially, this implementation is also battle-tested for iOS deployment, eliminating the risk of weight corruption I previously faced.\nFigure 9. Prototype testing. A generated image from the text prompt \"A beautiful landscape with mountains and a lake, golden hour lighting\" using Apple’s CoreML Stable Diffusion model. The output is a high-quality, realistic rendering of a rustic mountain range at sunset. The trade-off is simple: I accept a ~1.5GB footprint (still well within budget) for a solution that guarantees production quality. Sometimes the \u0026ldquo;smallest\u0026rdquo; solution isn\u0026rsquo;t the best one. Hardware-aware optimizations at a reasonable scale beat model over-compression.\nConclusion In this post, we explored the tight constraints required to deliver a Nano Banana-like experience on the edge. Our initial exploration led us to select Apple\u0026rsquo;s CoreML Stable Diffusion model due its aggressive hardware co-design.\nWhat\u0026rsquo;s next? Before we can attempt to replicate the Nano Banana experience on device, we first need to understand what problems Apple\u0026rsquo;s CoreML optimizations truly solved and which technical challenges remain. My next blog post covers exactly how Apple safely compressed a diffusion model that is so easy to corrupt.\n","date":1763942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1763942400,"objectID":"bad2d28c53bebad6b17319d11ca5fcfa","permalink":"https://bellanich.github.io/post/edge-diffusion-1/","publishdate":"2025-11-24T00:00:00Z","relpermalink":"/post/edge-diffusion-1/","section":"post","summary":"How I chased a diffusion model small enough for the iPhone, fast enough for real use, and resilient enough to avoid corruption—unpacking what works, what doesn’t, and why.","tags":["Generative AI","Edge ML","Computer Vision","Article"],"title":"Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ve had a whirlwind introduction to world of LLMs. Between my adventure in embedding a multi-modal foundation model on various iOS devices and jetting of to NeurIPS 2024 in Vancouver 🇨🇦, I’ve been immersed in all things related to LLMs.\nThe Neural Information Processing Systems (NeurIPS) 2024 Conference took place at the beautiful Vancouver Convention Center, from December 10th to 15th (image credit). I\u0026rsquo;ve spent countless hours talking to the researchers building state-of-the-art LLMs and even more time pouring over online resources to understand the fundamentals. To save you the trouble, I\u0026rsquo;ve organized everything I\u0026rsquo;ve learned into a handy study guide: \u0026ldquo;Transformers Decoded: A Quick Guide to ML Research\u0026rdquo; .\nThis guide explains key concepts, industry trends, and the critical optimization techniques that make LLMs so performant. We pay close attention to crucial inference optimization techniques, including:\nSpeculative decoding: Inference is accelerated by generating multiple candidate outputs with a smaller LLM and verifying them with a larger, more accurate LLM. Flash attention: By restructuring the attention operations to minimize memory reads and writes — the bottleneck in classical attention — we achieve significant speedups. Continuous batching: Incoming requests are dynamically batched to maximize hardware utilization and throughput, especially in online serving. Further details on these (and other) optimization techniques can be found the PDF below. Let\u0026rsquo;s decode the latest in AI research, together!\n","date":1737936e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737936e3,"objectID":"6cf7e7561687f4d58bef57d871ef7043","permalink":"https://bellanich.github.io/post/transformers-handbook/","publishdate":"2025-01-27T00:00:00Z","relpermalink":"/post/transformers-handbook/","section":"post","summary":"For the past two months, I've been intensely studying the state-of-the-art in LLM research. This guide distills my findings into a practical resource for understanding the latest AI research.","tags":["News"],"title":"Decoding Transformers: A Concise Guide to Modern ML Research","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ve had a whirlwind introduction to world of LLMs. Between my adventure in embedding a multi-modal foundation model on various iOS devices and jetting of to NeurIPS 2024 in Vancouver 🇨🇦, I’ve been immersed in all things related to LLMs. I\u0026rsquo;ve spent countless hours talking to the researchers building state-of-the-art LLMs and even more time pouring over online resources to understand the fundamentals.\nTo save you the trouble, I\u0026rsquo;ve organized everything I\u0026rsquo;ve learned into a handy study guide: \u0026ldquo;Transformers Decoded: A Quick Guide to ML Research\u0026rdquo; . This guide explains key concepts, industry trends, and the critical optimization techniques that make LLMs so performant. We pay close attention to crucial inference optimization techniques, including:\nSpeculative decoding: Inference is accelerated by generating multiple candidate outputs with a smaller LLM and verifying them with a larger, more accurate LLM. Flash attention: By restructuring the attention operations to minimize memory reads and writes — the bottleneck in classical attention — we achieve significant speedups. Continuous batching: Incoming requests are dynamically batched to maximize hardware utilization and throughput, especially in online serving. Further details on these (and other) optimization techniques can be found the PDF below. Let\u0026rsquo;s decode the latest in AI research, together!\n","date":1737936e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737936e3,"objectID":"5f0404e99b1aaea285d55103c545ba14","permalink":"https://bellanich.github.io/portfolio/transformers-handbook/","publishdate":"2025-01-27T00:00:00Z","relpermalink":"/portfolio/transformers-handbook/","section":"portfolio","summary":"A practical guide to understanding the latest in LLM research — from fundamental concepts to optimization techniques","tags":["Notes"],"title":"Transformers Decoded: A Quick Guide to ML Research","type":"portfolio"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction My Game Plan Deployment Attempt 1: Deploying my multi-modal model as an iOS app What if the MLC Chat Engine supported uploading images? Changing Directions Attempt 2: Deploying my multi-modal model as a Python CLI Conclusion Introduction If you\u0026rsquo;ve been following along in this blog post series, you know that I got a little too inspired at this year\u0026rsquo;s Google I/O Connect event. After hearing about Google\u0026rsquo;s incredible feat of embedding Gemini Nano into their Pixel 8 phones, I\u0026rsquo;ve been not-so-patiently waiting for the open source community to release tools that make similar feats possible for solo developers like me. Fortunately, the Machine Learning Compiler (MLC) project has matured enough that my dreams might just be possible.\nHence, I\u0026rsquo;ve been a personal quest to assess the limits of this new and exciting technology. To keep things interesting, I\u0026rsquo;ve decided to embed a multi-modal LLM. I want to test a less established setup, but I\u0026rsquo;m also secretly hoping to have a nice souvenir after everything is said and done. (Personally, I could really use a multi-lingual and multi-modal chatbot during my next holidays — especially once that doesn\u0026rsquo;t eat up my monthly data plan.)\nAt this point, I\u0026rsquo;m very close to this end goal. In my first blog post in this series, I introduced you to the MLC framework, which has been doing most of the heavy-lifting. I also gave you a quick crash course in what it takes to embed a large machine learning model on a resource-constrained device. In the 2nd blogpost, I gave you a primer on vision transformers — specifically, what we need to know to embed this tiny and multi-modal LLaVA-OneVision Qwen2 0.5B model. In my last blog post, I used this LLaVA model as an example to show you how to extend the MLC framework to support a custom model definition.\nAfter going through this entire process, I have a fully functional - but not very logical - LLaVA model running on my laptop. As the name suggests, this LLaVA model belongs to the high-performing, open source, and multi-modal Large Language and Vision Assistant (LLaVA) model family. Fortunately, for us, this model has less than 0.5B parameters. Meaning, it should be small enough to squeeze onto an iPhone. Here\u0026rsquo;s what its chat conversations look like:\nAt the start of a very simple discussion, my embedded LLaVA-OneVision model looks promising. Unfortunately, it doesn't take too long to see the cracks in LLaVA's logical reasoning — and to watch them crumble. Given this LLaVA model\u0026rsquo;s lack of coherence, we don\u0026rsquo;t want it talking directly to our end user. Rather, we\u0026rsquo;ll let it stick to what it does best (image annotation) and call in a slightly large LLM for backup.\nMy Game Plan So, what\u0026rsquo;s next? If you recall the diagram that I shared with you in the last time, I need to deploy a two-model ensemble. My plan is to use LLaVa-OneVision\u0026rsquo;s image annotation capabilities to expand Gemma 2B\u0026rsquo;s ridiculously good conversational capabilities. Essentially, LLaVA will describe the user provided image to Gemma in plain text. Gemma will receive the original user prompt and LLaVA\u0026rsquo;s image description. Between these two, Gemma should have enough information to return a lucid and logical response.\nMy multi-modal chat pipeline consists of two models: (1) the eloquent and multilingual Gemma2B model, and (2) a mini LLaVA-OneVision model. LLaVa will act as a translator for Gemma, generating a text descriptions for any provided images. Deployment My original vision was to have an edge multi-modal embedded onto my iPhone. That way, I could chat with my personal travel assistant anytime and anywhere — no internet connection required.\nAttempt 1: Deploying my multi-modal model as an iOS app Before I can package Gemma 2B and LLaVA together as a single model, I first need to figure out how to input images into MLC\u0026rsquo;s simple, pre-built chat iOS application, and this is where I run into my first roadblock.\nIf you\u0026rsquo;re observant, you might have noticed the grayed out text \u0026ldquo;Upload picture to chat\u0026rdquo; in my conversation screenshots with LLaVA. One could logically conclude that this means that MLC has some sort of user interface for uploading images. However, after pouring over the Swift code, I couldn\u0026rsquo;t find any such feature. Before I start cobbling something together in Swift, I decide to do a quick sanity check: Can I pass an image URL to the MLC Chat Engine?\nWell\u0026hellip;not really. I\u0026rsquo;d have to go in deep to the MLC\u0026rsquo;s low code and Swift implementations to make that possible. That\u0026rsquo;s a rabbit hole that I don\u0026rsquo;t want to fall into, so it\u0026rsquo;s time for me to pivot directions.\nWhat if the MLC Chat Engine supported uploading images? Let\u0026rsquo;s suppose that I was able to the the MLC Chat Engine iOS implementation so that it:\nIt now supports serving images to models. There\u0026rsquo;s a cool and sleek user interface for uploading said images. Could I deploy any of the Apple products in my household? Well, I decided to do an initial test on my housemate\u0026rsquo;s iPad Pro 11 inch (V1). I copied over and downloaded my iOS app. While I was able to open the app\u0026rsquo;s homepage and start my conversation with Gemma 2B, the app would crash before I could even ask Gemma a followup question.\nPerplexed, I checked my iOS app\u0026rsquo;s memory consumption in Xcode after I opened a new chat with Gemma. Well, it turns out that Gemma consumes a bit over 2GB of RAM. Given that this borrowed iPad only has 4GB of RAM in total, it\u0026rsquo;s not a surprise that the iOS application was crashing. Even when I closed all other applications, other background processes were consuming this iPad\u0026rsquo;s limited and valuable memory.\n(a) Gemma 2B (b) LLaVA-OneVision The memory footprints of my embedded Gemma 2B and LLaVA-OneVision models on my 8GB M2 MacBook Air. In contrast, the LLaVA-OneVision model has a much smaller memory footprint at 755MB. So, yes, it\u0026rsquo;s technically possible to deploy only the LLaVA-OneVision model on this given iPad; however, it\u0026rsquo;s probably not worth it. As we\u0026rsquo;ve seen from this blog post\u0026rsquo;s introduction, the embedded LLaVA-OneVision model isn\u0026rsquo;t capable of holding a coherent conversation for very long. Meaning, Gemma\u0026rsquo;s memory consumption is a definitive roadblock in my app ambitions.\nAs a final check, I tried the same exercise with my iPhone 13 and got similar results. This is expected, given that both devices have the same memory capacity. Of course, all the Apple devices I could find are on the older side. Apple\u0026rsquo;s newer iPhones and iPads have a large enough RAM that I should be able to deploy Gemma 2B and LLaVA-OneVision Qwen2 0.5B together.\nDevice Available RAM iPhone X 3GB iPhone 13 4G iPad Pro V1 4G iPhone 16 8G iPad Air 6 8G Table 1: The RAM available on selected Apple products. The top 3 rows describe devices that I have immediate access to, while the last two describe Apple's latest iPhone and iPad offerings (source). The available RAM on Apple\u0026rsquo;s latest iPhone and iPad models is a testament to its commitment to cram AI tools and features into every part of the Apple user experience. Of course, it\u0026rsquo;s also wild to think that the latest iPhone is just as computationally strong — at least in RAM — as my laptop. This also means that if I could feed images into the iOS MLC Chat Engine, then I should be able to very easily deploy my multi-modal model setup on an iPhone — just not my phone.\nThat being said, I\u0026rsquo;m not in a rush to get the latest iPhone. I just have something to look forward to when its inevitably time to replace my current one.\nChanging Directions At this point, I\u0026rsquo;ve ruled out the possibility of deploying my multi-modal foundation model on any Apple device, especially those within my immediate reach.\nNow, I\u0026rsquo;m trying to contend with two different mysteries:\nWhy does MLC natively support some LLaVA models, but doesn\u0026rsquo;t support serving images? The main added value of the LLaVA model family is their exceptional ability to handle multi-modal inputs. Does the MLC Engine support serving a model images in any other platform-specific chat engine implementation? Realistically, I don\u0026rsquo;t think that I\u0026rsquo;m going to get an answer to my first question unless I manage to chase down some of the MLC project\u0026rsquo;s main contributors. Fortunately, the second question is easier to answer. If you recall from my first blog post, MLC implements their own version of OpenAI\u0026rsquo;s Python API.\nSince the original Python API supports serving images, there\u0026rsquo;s a good chance that the MLC implementation might offer the same functionality.\nThe Machine Learning Compiler bases their Python API off of OpenAI's well-established implementation. As we can see in the official OpenAI Python API documentation, the OpenAI implementation does support serving a model images (source). Sure enough, I look through when I look through the mlc-llm-cpu library\u0026rsquo;s source code, I find the same image_url variable in their conversation protocol definition.\nclass Conversation(BaseModel): ... def as_prompt(self, config=None) -\u0026gt; List[Any]: ... for item in content: assert isinstance(item, dict), \u0026quot;Content should be a string or a list of dicts\u0026quot; assert \u0026quot;type\u0026quot; in item, \u0026quot;Content item should have a type field\u0026quot; if item[\u0026quot;type\u0026quot;] == \u0026quot;text\u0026quot;: message = self.role_templates[role].replace( MessagePlaceholders[role.upper()].value, item[\u0026quot;text\u0026quot;] ) message_list.append(message) # MLC supports passing image URLs via its Python API elif item[\u0026quot;type\u0026quot;] == \u0026quot;image_url\u0026quot;: assert config is not None, \u0026quot;Model config is required\u0026quot; image_url = _get_url_from_item(item) message_list.append(data.ImageData.from_url(image_url, config)) message_list.append(\u0026quot;\\n\u0026quot;) else: raise ValueError(f\u0026quot;Unsupported content type: {item['type']}\u0026quot;) message_list.append(separator) ... return prompt Meaning, I still can implement my multi-modal chat pipeline via a Python script.\nAttempt 2: Deploying my multi-modal model as a Python CLI At this point, I just want to see if I could successfully deploy the embedded multi-modal foundation model on some device that I own. So, I wrote a very simple Python script that lets a user interact with my chat pipeline via the commandline. If the user provides an image filepath, the model will check if it\u0026rsquo;s there. If so, underneath the hood, LLaVA is annotating this image for Gemma 2B. If not, my chatbot will kindly ask the user to check if the image is really there. Otherwise, Gemma 2B will be doing all the talking.\nHere\u0026rsquo;s the end result:\nMy embedded multi-modal model recognizes the cute, cartoon husky in the supplied photo. It's also capable of generating an interesting story synopsis for what could possibly become a new international bestseller. As you can see, the LLaVA model was able to recognize the Husky in the cartoon image and pass this information along to Gemma. Afterwards, Gemma was able to quickly help me draft an initial synopsis for about a day in the life of Barnaby, a sea-faring Husky who travels the world in his tiny yellow submarine. If we had a bit more compute power and a stronger LLaVA model, perhaps we could also extend this system to support image generation. If so, do think you \u0026ldquo;Barnaby\u0026rsquo;s Deep Sea Adventures\u0026rdquo; could become an international bestseller?\nFor the exact source code used in this demo, please refer to this blog post\u0026rsquo;s corresponding GitHub repository.\nConclusion It\u0026rsquo;s truly incredible seeing how quickly embedded systems technology has progressed in this domain. The tech giants are definitely in a comfortable place when it comes to deploying sophisticated edge foundation models, but the open source tooling is still in its early phase. Through this project, I\u0026rsquo;ve defined stumbled over a few vague, low-level code errors of my own. Tools like the MLC framework (in their current iteration) well-suited for those of us who are:\nWell-versed in the latest deep learning research Comfortable debugging low code system errors Reasonably proficient in Swift (or similar programming languages) Own latest edge devices Of course, that\u0026rsquo;s a lot of conditions. Nonetheless, it\u0026rsquo;s incredible seeing how projects like the Machine Learning Compiler have democratized access to foundation models. Whether you\u0026rsquo;re moonlighting as an entrepreneurial or are a curious university student, you can get easily started deploying your own single-modal LLMs on edge today. Maybe in another 6 months, the tooling will have advanced to the point that it fully supports multi-modal chat — including image generation.\nIn the meantime, I plan to watch these industry development closely. While I\u0026rsquo;ve put my hopes of a private, multi-modal travel assistant on the shelf, I haven\u0026rsquo;t abandoned them entirely. Stay tuned to see what I\u0026rsquo;ll do next. 👋\n","date":1733097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733097600,"objectID":"31a4586973855a3bc89f74305ae3ef5a","permalink":"https://bellanich.github.io/post/edge-llm-app/","publishdate":"2024-12-02T00:00:00Z","relpermalink":"/post/edge-llm-app/","section":"post","summary":"After painstakingly embedding a mini multi-modal LLaVA model, I'm ready to properly deploy it as an iOS app and enjoy the fruits of my labor. Let's see if we can truly shrink the impossible.","tags":["LLM Optimization","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction Selected Architecture Why Not Only Use the LLaVA Model? The Overall Process Manual Porting LLaVA-OneVision to MLC MLC\u0026rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation Embedding Aggregation Output Features Explained GELU Approximation Embedding Normalization Summary Packaging Our Custom Model End Result Conclusion What\u0026rsquo;s next? Introduction I attended this year\u0026rsquo;s Google I/O Connect in Berlin, and seeing Google’s latest work in Edge AI was inspiring. Since then, I\u0026rsquo;ve been on a personal mission to deploy my own edge model. Given how rapidly the open source community is catching up with the tech giants in edge foundation models, I\u0026rsquo;ve decided test the limits of what I can realistically achieve as a solo developer. Can I embed a multi-modal foundation model onto my iPhone?\nIn my first blog post, I\u0026rsquo;ve introduced the Machine Learning Compiler Project as the open source solution that will make this possible. In my last blog post, I\u0026rsquo;ve given you the technical background knowledge needed to successfully deploy a multi-modal foundation model.\nNow, we\u0026rsquo;re going to put this theory into practice in a hands-on activity. I\u0026rsquo;m going to show you how to embed a custom foundation model onto an edge device — and all the things that can go wrong in the process.\nSelected Architecture Remember that my end vision is to have a multi-modal foundation model deployed on my iPhone. Ideally, I want it to be multi-lingual so I can get some help the next time that I\u0026rsquo;m lost in a foreign country.\nAt a high-level, my implementation will consist of two different models: (a) the smallest Large Language and Vision Assistant (LLaVA) model that I can find, and (b) an instruction-tuned version of Gemma 2B.\nI\u0026rsquo;ve chosen these models, since (a) the MLC Engine natively supports them, (b) they\u0026rsquo;re lightweight enough to be compiled on my laptop, and (c) their respective families have earned a reputation as high-performers.\nEach model will work on a specific task. Whenever the end user shares an image with our multi-modal chatbot, the mini LLaVA model will generate a text description for Gemma. Gemma will then use this information to respond to the user\u0026rsquo;s original prompt. If no image is shared, our user will only interact with Gemma 2B. Of course, this model specialization will be abstracted away. Our user won\u0026rsquo;t be aware that they\u0026rsquo;re actually conversing with two different foundation models rather than just one.\nMy multi-modal chat pipeline consists of two models: (1) the eloquent and multilingual Gemma2B model, and (2) a mini LLaVA-OneVision model. LLaVa will act as a translator for Gemma, generating a text description for any user inputted images. Why Not Only Use the LLaVA Model? It may sound a bit strange that we\u0026rsquo;re deploying two models instead of one — especially since all LLaVA understand text and images. However, we need to make a distinction between the smallest LLaVa-OneVision model, which is less than 1B parameters in size, and its much larger 7B+ counterparts.\nAccording to the 0.5B LLaVA-OneVision model card, the LLaVa model is just under 900M parameters in total. Given its 80,000+ model downloads, we can assume it does a decent job at its primary task: annotating images (source). Larger LLaVA models are perfectly capable of holding a coherent conversation and to understanding whatever images we show them. You can find a few interesting examples of large LLaVA models answering questions about images here.\nThe LLaVA-v1.6 series contains models ranging from 7B to 34B parameters. At these sizes, a LLaVA model can easily identify Leonardo da Vinci's world famous masterpiece from a screenshot and give us a quick art history lesson. Unfortunately, we can't expect the same from the smallest LLaVA models. (image credit). However, we need to adjust our expectations for a 900M parameter model. There\u0026rsquo;s only so much that a small model can do — and the LLaVA-OneVision Qwen2 O.5B model has been optimized for image annotation rather than instruction tasks. Meaning, we can probably converse directly with it, but may not be thrilled with the quality of its responses.\nIf we wanted to deploy a similar application in a cloud environment, then it would probably just make more sense to quantize a 7B LLaVA model and accept a slightly larger monthly bill. However, since we\u0026rsquo;re working on edge, we have tight resource constraints and need to make do with what we have.\nThe Overall Process For each model, we need to:\nQuantize its weights; and Apply hardware-specific optimizations to it. How simple this process really is comes down to the degree of built-in MLC Engine support. MLC has already pre-quantized an instruction-tuned version of Gemma 2B for us. Hence, deploying Gemma as a stand alone model in an iOS application is relatively straightforward task. Just follow MLC\u0026rsquo;s Quick Start Documentation for how to package Gemma 2B and their iOS Swift SDK instructions.\nOn the other hand, applying the same process to our LLaVA model is a bit trickier. If we go through the list of pre-quantized models offered by MLC,(as of November 2024) there is no pre-quantized LLaVA model available — much less our desired mini LLaVA model.\nSince this process for deployed a pre-quantized model from HuggingFace so well-documented, I\u0026rsquo;m not going to focus on it. Rather, I\u0026rsquo;ll show you how I ported a new model into the MLC framework.\nManual Porting LLaVA-OneVision to MLC At this point, you may be a bit confused. I\u0026rsquo;ve stated the MLC Engine supports LLaVA models, but I\u0026rsquo;m also talking about manually porting the 0.5B LLaVA-OneVision model into the MLC Framework.\nWhat\u0026rsquo;s going on? At an initial glance, it looks like MLC fully supports the LLaVa model family, but that\u0026rsquo;s only partially true. At the time of writing (November 2024), MLC only natively supports specific LLaVA implementations, more specifically those that use (a) use Llama or Mistral model as its text decoder, and (b) use a CLIP-trained vision encoder. Unfortunately, all LLaVA variants that meet these requirements are at 7B+ parameters. Meaning, they\u0026rsquo;re too large for my laptop — muchless my smartphone — to handle.\nAs a result, I need to manually port the much smaller llava-onevision-qwen2-0.5b-ov-hf model definition into the MLC framework. Practically speaking, this means defining this model in the style used in the mlc-llm-cpu Python library, which is only available through MLC AI\u0026rsquo;s own Python code repository. Afterwards, we need to then recompile this library locally so that it contains our new model definition.\nOnce that\u0026rsquo;s done, I can quantize and hardware-optimize my selected LLaVA model just like any other MLC-supported model. As its full name suggests, the 0.5B LLaVA-OneVision model uses the Qwen2 0.5B LLM as its text decoder. Fortunately for us, MLC already supports Qwen2 0.5B implementation. Meaning, this change is quite easy. We just copy and paste MLC\u0026rsquo;s definition of LLaVA, rename the files, and change a few key-value pairs:\nThe original MLC LLaVA model definition looks like this:\nfrom ..llama.llama_model import LlamaConfig, LlamaForCausalLM from ..mistral.mistral_model import MistralConfig, MistralForCasualLM CONFIG_MAP = { \u0026quot;LlamaForCausalLM\u0026quot;: LlamaConfig, \u0026quot;MistralForCausalLM\u0026quot;: MistralConfig, } ARCHITECTURE_MAP = { \u0026quot;LlamaForCausalLM\u0026quot;: LlamaForCausalLM, \u0026quot;MistralForCausalLM\u0026quot;: MistralForCasualLM, } Now, we just rewrite our LLaVA-OneVision model definition to support the LLaVA-OneVision Qwen2 0.5B model\u0026rsquo;s architecture definitions:\n# Defined by MLC from ..qwen2.qwen2_model import QWen2Config, QWen2LMHeadModel CONFIG_MAP = { \u0026quot;QWen2LMHeadModel\u0026quot;: QWen2Config, \u0026quot;Qwen2ForCausalLM\u0026quot;: QWen2Config } ARCHITECTURE_MAP = { \u0026quot;QWen2LMHeadModel\u0026quot;: QWen2LMHeadModel, \u0026quot;Qwen2ForCausalLM\u0026quot;: QWen2LMHeadModel, } So far, so good. Let\u0026rsquo;s take a closer look at the the 0.5B LLaVA-OneVision config.json file. We see that this LLaVA model\u0026rsquo;s vision encoder was trained using SigLIP — rather than MLC\u0026rsquo;s natively supported CLIP training framework.\nAs seen in the LLaVA-OnVision Qwen2 0.5B model's configuration file, the vision encoder was trained using SigLIP. As of now (November 2024), MLC's only natively supports CLIP-trained vision encoder (source). Meaning, there\u0026rsquo;s no MLC definition for us to just import. We need to write some custom Python model definition using MLC wrappers. Luckily, we already dived into the details of the SigLIP vision encoder in the previous blog post. So, we\u0026rsquo;re ready to get started.\nI\u0026rsquo;ve included the final SigLIP vision encoder definition on in this blogpost series\u0026rsquo;s corresponding GitHub repository. For the sake of brevity, I\u0026rsquo;m just going to focus on the technical differences between these two vision encoders and how these changes translate into code.\nMLC\u0026rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation Once again, we\u0026rsquo;re going to reference our LLaVA model\u0026rsquo;s trusty config.json file to get some clues about where to start. In particular, we see the key-value pair: \u0026quot;vision_feature_layer\u0026quot;: -1, whereas the original LLaVA config uses \u0026quot;vision_feature_layer\u0026quot;: -2. This is a hint about how a vision encoder aggregates its sequence of embeddings into a single vector.\nEmbedding Aggregation Both LLaVA models use a Vision Transformer, which outputs a sequence of embeddings. For the training of CLIP and SigLIP, we need a single vector. In this specific Huggingface implementation, CLIP and SigLIP do this aggregation in different ways.\nCLIP uses a class embedding, similar to common adaptations of the Vision Transformer for classification. Here, we add another token to each image sequence, which is fed through the Transformer along with the image tokens. In the end, we pick the aggregated image feature vector as the output of this classification token. By having the class token part of the self-attention layers, it gives the transformer the ability to aggregate information of the image in this token across its layers. In the implementation, we see this class embedding token being added to the image token sequence:\nclass CLIPVisionEmbeddings(Module): def __init__(self, config: CLIPVisionConfig): super().__init__() # Class Embedding Token, added to each image token sequence. self.class_embedding = nn.Parameter((self.embed_dim,)) ... def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: patch_embeds = self.patch_embedding(pixel_values) ... class_embeds = broadcast_to( self.class_embedding, shape=(batch_size, 1, self.embed_dim) ) # Add class embedding token to image token sequence. embeddings = concat([class_embeds, patch_embeds], dim=1) ... return embeddings In contrast, SigLIP pools its output features. Hence, the sequence of image tokens remains unchanged in the input and is fed through the layers. We add on top a pooling layer over the sequence dimension to aggregate all the feature information. This can either be done by a simple averaging, or, in case our specific case, with a multi-head attention pooling. This is similar to our self-attention layers, but just with a fixed query.\nNote. We\u0026rsquo;ve removed the code parts that are common to both models. This simplifies the code and allows us to highlight key differences.\nIn our SigLIP implementation, shown below, we see as a difference to CLIP that there is no class embedding.\nclass SiglipVisionEmbeddings(Module): def __init__(self, config: SiglipVisionConfig): super().__init__() ... def forward(self, pixel_values: Tensor, interpolate_pos_encoding: bool = False) -\u0026gt; Tensor: patch_embeds = self.patch_embedding(pixel_values) embeddings = patch_embeds ... return embeddings Output Features Explained In the previous blog post, we discuss how we need a whole sequence of image embeddings for LLaVA rather than a single image feature vector. This provides more detailed information to the decoder.\nWhile we do not make use of the output heads of CLIP and SigLIP respectively, it does affect which layer we select our features from. This is what the config argument vision_feature_layer ($-1$ for SigLIP and $-2$ for CLIP).\nIn other words, we choose the last layer in SigLIP, since the model was trained with image embeddings that are literally the weighted average of all image sequence tokens. Thus, the training process ensures that all these image embeddings have valuable information in them.\nAttention pooling represents a weighted average pooling, where the weights are determined by the normalized dot product between a static query and the keys per token. While this example shows the averaging for text tokens, it has the same idea with image patch tokens in our vision encoder (image credit). class SiglipVisionModel(Module): def __init__(self, config: SiglipVisionConfig): super().__init__() self.vision_model = SiglipVisionTransformer(config) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: # Siglip Qwen2 is using last layer, CLIP pre-last due to different # Transformer encoder. return self.vision_model(pixel_values)[-1] For CLIP, choosing the last layer is suboptimal, because of the usage of a class token. In the CLIP loss, only the output features of the class token are used. Thus, the output features of all other tokens, namely our image embeddings, were not used. In other words, these features did not receive any gradients during training, and we cannot be sure that the model has stored useful information in them. Most likely, the model has specialized the last layer specifically for the class embedding token, making the outputs of the other tokens (possibly) meaningless.\nHence, we need to go back one more layer (i.e, the pre-last layer), because these tokens did receive gradients during the training by their dependency on the class token in the last self-attention layer. This ensures that these embeddings have strong features and makes them usable in our LLaVA model implementation.\nclass CLIPVisionModel(Module): def __init__(self, config: CLIPVisionConfig): super().__init__() self.vision_model = CLIPVisionTransformer(config) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: return self.vision_model(pixel_values)[-2] GELU Approximation The Gaussian Error Linear Unit (GELU) is a very popular activation function for Transformers, and is used in both of our CLIP and SigLIP implementations. However, there are some specific details in about how we can implement the GELU activation.\nThe \u0026ldquo;true\u0026rdquo;, precise implementation of GELU involves the cumulative distribution function (CDF) of the Gaussian distribution $\\Phi(x)$:\n$\\text{gelu}(x)=x\\cdot\\Phi(x)$\nThis CDF is, however, expensive to implement and in particular for edge-devices, where every inference optimization counts, it\u0026rsquo;s sub-optimal. Instead, people commonly use GeLU approximations that are good enough. The standard approximation, often used during training and in frameworks like JAX and PyTorch, is the tanh-approximation:\n$\\text{gelu}(x)\\approx 0.5x\\left(1+\\tanh\\left[\\sqrt{\\frac{2}{\\pi}}(x+0.044715\\cdot x^3)\\right]\\right)$\nThis is also being used in the Huggingface implementation for SigLIP, and we port it over as shown below:\nclass QuickGELU(Module): # SigLIP implementation def forward(self, input_tensor: Tensor) -\u0026gt; Tensor: c = (2 / math.pi)**0.5 return 0.5 * input_tensor * ( 1 + tanh(c * (input_tensor + 0.044715 * input_tensor**3)) ) In the MLC implementation of CLIP, another approximation is used. This one involved the sigmoid function, and is simply:\n$\\text{gelu}(x)\\approx x\\cdot \\sigma(1.702x)$\nCLIP\nclass QuickGELU(Module): def forward(self, input_tensor: Tensor) -\u0026gt; Tensor: return input_tensor * sigmoid(input_tensor * 1.702) While the Sigmoid GeLU approximation is simpler and even cheaper to calculate, it is also less accurate. Thus, we have to make a tradeoff between efficiency and accuracy. Since we our selected SigLIP vision encoder was trained using the tanh-approximation, we\u0026rsquo;ll stick with it. Differences between the GeLU function implementation during training and inference time can cause a slight but noticeable drop in performance.\nA visualization of the different implementations of the GELU activation function. The original GeLU function is in red, the tanh approximation is in purple, and the sigmoid approximation is in green. As you can see, all of them are quite similar. For the sigmoid activation, there is a noticeable difference for the negative range -1.5 and -4. However, for the tanh approximation, we need to zoom in closely to see the difference, showcasing why the tanh approximation is often used as a close match. Embedding Normalization Another minor design choice is whether we normalize the embedding features before feeding them into the main Transformer model. Both models use the pre-activation Transformer implementation, which applies a LayerNorm before each Self-Attention and MLP layer. However, we can also apply a LayerNorm on the embeddings themselves, or leave the model to learn the scaling of the residual part.\nIn the CLIP implementation, we find a LayerNorm applied to the embeddings before feeding it through the layers.\nclass CLIPVisionTransformer(Module): def __init__(self, config: CLIPVisionConfig): super().__init__() embed_dim = config.hidden_size self.embeddings = CLIPVisionEmbeddings(config) self.pre_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) self.encoder = CLIPEncoder(config) self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: hidden_states = self.embeddings(pixel_values) hidden_states = self.pre_layernorm(hidden_states) encoder_outputs = self.encoder(inputs_embeds=hidden_states) return encoder_outputs In contrast, in the SigLIP implementation, this normalization is missing. However, it is not expected to cause a major performance difference.\nclass SiglipVisionTransformer(Module): def __init__(self, config: SiglipVisionConfig): super().__init__() embed_dim = config.hidden_size self.embeddings = SiglipVisionEmbeddings(config) self.encoder = SiglipEncoder(config) # Defined but not actually used. self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: hidden_states = self.embeddings(pixel_values) encoder_outputs = self.encoder(inputs_embeds=hidden_states) return encoder_outputs Summary Overall, our SigLIP implementation has some small, but crucial differences compared to MLC\u0026rsquo;S CLIP implementation. The table below summarizes the differences that we needed to account for.\nMLC LLaVA Model - CLIP 0.5B LLaVA-OneVision Qwen2 0.5B Model - SigLIP Output Feature Aggregation Class Token Attention Pooling Feature Layer Pre-Last Layer Last Layer Embedding Normalization Yes No GELU Implementation Sigmoid Approximation Tanh Approximation Now that we\u0026rsquo;ve implemented SigLIP in the MLC framework, it is straight forward to integrate the LLaVA-OneVision models into the MLC framework. We can now proceed with quantizing and optimizing our model for deployment on an edge device.\nPackaging Our Custom Model Once we\u0026rsquo;ve extended the mlc-llm-cpu library to include our custom model definition, then we can proceed as normally.\nFirst, we want to quantize it our newly ported LLaVA model. To do so, we run the following commands in our MLC LLM project\u0026rsquo;s repo directory:\n# Create directory mkdir -p dist/models \u0026amp;\u0026amp; cd dist/models # Clone Original LLaVA model's weights from HuggingFace git lfs install git clone https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf # Apply the `q4f16_1` quantization method to our model mlc_llm convert_weight ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \\ --quantization q4f16_1 \\ -o dist/llava-onevision-qwen2-0.5b-ov-hf --model-type llava_onevision Fortunately, the command ran successfully.\nAfter defining my target LLaVA-OneVision model as a new MLC `model-type`, I was able to easily quantize this model. Next, we need to apply some hardware-specific optimizations to our model.\n# Generate a MLC config file for our quantized model mkdir dist/libs mlc_llm gen_config ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \\ --quantization q4f16_1 \\ --conv-template redpajama_chat \\ --context-window-size 768 \\ -o dist/llava-onevision-qwen2-0.5b-ov-hf # Optimize LLaVA OneVision for an iOS app implementation mlc_llm compile ./dist/llava-onevision-qwen2-0.5b-ov-hf/mlc-chat-config.json \\ --device iphone \\ -o dist/libs/llava-onevision-qwen2-0.5b-ov-hf-iphone.tar Finally, we need to package the quantized and optimized model for my iOS App. To make my life easier, I\u0026rsquo;ve uploaded the pre-quantized LLaVA-OneVision Qwen2 0.5B model to my personal HuggingFace account.\nMy mlc-package-config.json file located in the ios/MLCChat/ subdirectory (following MLC LLM\u0026rsquo;s project structure) now looks like this:\n{ \u0026quot;device\u0026quot;: \u0026quot;iphone\u0026quot;, \u0026quot;model_list\u0026quot;: [ { \u0026quot;model\u0026quot;: \u0026quot;HF://bella-nich/llava-onevision-qwen2-0.5b-ov-q4f16_1-mlc\u0026quot;, \u0026quot;model_id\u0026quot;: \u0026quot;llava-onevision-qwen2-0.5b-ov-hf\u0026quot;, \u0026quot;estimated_vram_bytes\u0026quot;: 1000000000, \u0026quot;overrides\u0026quot;: { \u0026quot;prefill_chunk_size\u0026quot;: 128 } }, ] } So, I\u0026rsquo;m going to package my quantized and optimized LLaVA-OneVision model into a proper iOS app.\n# Words cd /path/to/MLCChat # e.g., \u0026quot;ios/MLCChat\u0026quot; export MLC_LLM_SOURCE_DIR=/path/to/mlc-llm # e.g., \u0026quot;../..\u0026quot; mlc_llm package I was able to successfully package my newly ported LLaVA-OneVision Qwen2 0.5B model without any errors. Meaning, there's a chance that everything was quantized and compiled correctly. End Result While the lack of compilation errors is promising, the only way to validate this entire process is talk to the embedded LLaVA model. So, I built and deployed my packaged iOS application.\nOnce I do, I\u0026rsquo;m greeted with a few strange but mostly understandable sentences.\nMy embedded LLaVA-OneVision Qwen2 0.5B model is able to form mostly coherent sentences. However, its sense of humor doesn't seem fully developed. In other words, LLaVA isn\u0026rsquo;t pure spouting gibberish. I take this as a sign that the quantization and model compilation processes have gone well. Of course, as the longer the conservation goes on, the less coherent LLaVA becomes. Pretty soon LLaVA is giving me random responses strung together.\nAfter my dissatisfaction with the embedded model's sense of humor, I decided to see if LLaVA can tell me a fun fact. To my surprise, I'm greeted with a sudden and odd request. The chat snippets of \u0026lt;im_start\u0026gt;, \u0026lt;im_end\u0026gt;, and assistant give us clues about how this LLaVA model was tuned for image annotation tasks. More specifically, this tells us about the structure of LLaVA-OneVision Qwen2 0.5 B\u0026rsquo;s chat template. A chat template restructures our current conversation, which is a list of string, into a single, tokenizable format that the model expects. Here, we can see that the chat template assistant role is prompting LLaVA to continue but in ways that are completely disconnected from my original text-only prompts.\nThe good news is that chat template assistant role should be more useful when we provide LLaVA image inputs. However, this LLaVA model\u0026rsquo;s current performance (in a task that it wasn\u0026rsquo;t fine-tuned for) highlights the differences between \u0026gt;1B and a 7B+ parameter models. Larger foundation models are simply more versatile than smaller ones.\nConclusion In this blogpost, I\u0026rsquo;ve shown you everything that it takes to embed an unsupported model onto an edge device using the Machine Learning Engine Compiler framework. As you can see, the devil is in the details. You need to be well-versed in the different permutations of a given neural architecture\u0026rsquo;s implementation and able to spot those differences in the wild.\nOf course, the only thing that\u0026rsquo;s more important than getting this process right is to choose the correct model to embed. The smaller we go in size, the more portable our foundation model becomes, but that portability comes at the cost of performance.\nWhat\u0026rsquo;s next? While embedding a custom model is an exciting milestone, we\u0026rsquo;re not done yet. As you can see, the embedded LLaVA model works but it doesn\u0026rsquo;t make for a scintillating conversation partner. Hence, we need to get Gemma 2B and the LLaVA-OneVision Qwen2 0.5B model to work together — which is exactly what I do in my next (and final) blogpost in this series. Stay tuned!\n","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733011200,"objectID":"08d81f1d9e09c89e604785aba3c5690c","permalink":"https://bellanich.github.io/post/edge-llm-embed-llava/","publishdate":"2024-12-01T00:00:00Z","relpermalink":"/post/edge-llm-embed-llava/","section":"post","summary":"Armed with some newfound vision transformer knowledge, we're ready to extend the Machine Learning Compiler framework to support a new, tiny but promising multi-modal model.","tags":["LLM Optimization","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC","type":"post"},{"authors":null,"categories":null,"content":"Generative AI tooling has become a staple for everyday tasks — from creating presentation visuals to finding the right code syntax. But I\u0026rsquo;ve always felt a little too uneasy trusting cloud-hosted LLMs with my private chats.\nAs an ML engineer, I assumed that the only alternative was spinning up an expensive private LLM in the cloud — until the Google I/O Connect event (June 2024). That’s where Google revealed their smallest LLM to date, Gemini Nano, running directly on the Pixel 8. Seeing an ultra-private AI assistant on edge was inspiring, but for solo developers like me, the open-source tools weren’t quite ready yet.\nFast-forward six months, and the landscape has changed. Thanks to projects like the Machine Learning Compiler (MLC) framework, solo developers can now optimize and deploy powerful LLMs on edge devices. Rather than sticking to a unimodal LLM — a well-worn path — I set my sights higher: deploying a multimodal LLM on the smallest edge devices possible.\nThe end result? I successfully embedded the multilingiual Gemma 2B and the multi-modal LLaVA-OneVision Qwen2 0.5B models on my laptop (among other devices). Take a look for yourself:\nAs shown, my embedded model can comfortably discuss the content of an image and give some interesting synopses. Pretty impressive, right? Here’s a look at how it all works:\nAt 800M parameters, the LLaVA-OneVision Qwen2 0.5B is ideal for edge deployment but it struggles with complex user instructions. Thus, I used Google’s ultra-efficient Gemma 2B model as a fallback. For the full story, including technical details, check out my corresponding \u0026ldquo;Shrinking the Impossible\u0026rdquo; blog series:\nPart 1: Optimizing Foundation Models for Edge Devices with MLC Part 2: Teaching Chatbots to See with LLaVA, CLIP, and SigLIP Part 3: Embedding a Custom-Defined LLaVA-OneVision Model with MLC Part 4: Deploying My Own Pocket-Sized Multi-Modal Large Language Model The source code is also available for your viewing pleasure on GitHub. Consider it your guide to shrinking impossibly large LLMs down to something that fits inside your pocket.\n","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733011200,"objectID":"6f11e7eba2fbfd8e9aa7e1869bca20ad","permalink":"https://bellanich.github.io/portfolio/edge-llm/","publishdate":"2024-12-01T00:00:00Z","relpermalink":"/portfolio/edge-llm/","section":"portfolio","summary":"A deep dive on embedding custom vision-text large language models across various iOS devices, from laptops to phones","tags":["Code"],"title":"Shrinking the Impossible: Deploying My Own Multi-Modal Edge Foundation Model","type":"portfolio"},{"authors":["Bella Nicholson"],"categories":null,"content":"Table of Contents Table of Contents Introduction LLaVA: Chatbots That Can \u0026ldquo;See\u0026rdquo; Visual Instruction Tuning of Text Decoders Vision Encoders Training the Vision Encoder CLIP SigLIP Inference Machine Learning Compiler Implementation The LLaVA Model Family on MLC LLaVA OneVision Conclusion What\u0026rsquo;s next? Introduction In my last blog post, I introduced you to the fascinating world of edge foundation models. I was dreaming big, imagining a foundation model that could \u0026ldquo;see\u0026rdquo; — well, at least understand the photos I share with it. Let\u0026rsquo;s be honest, I’ll probably need some help when I’m lost in a new city on my next vacation. A model that understands both text and images? Way more useful when I\u0026rsquo;m wandering around than a single-modal chatbot!\nRecently, things have been getting pretty exciting in the world of multi-modal models. Beyond just text and images, ChatGPT can now \u0026ldquo;see, hear, and speak\u0026rdquo; — processing text, images, and audio. There\u0026rsquo;s also Gemini, Google’s latest powerhouse, which can process everything from text to images to audio — and even long movies, thanks to its multi-million token context window. Sounds pretty impressive, right? But here\u0026rsquo;s the catch: these models are so large and computationally demanding that it’s nearly impossible to run them on edge devices (like phones and laptops).\nIn this blog post, we’ll explore some of the latest advancements in small, efficient multi-modal models that can actually be deployed on edge devices. We\u0026rsquo;ll introduce the LLaVA model family, which combines vision encoders and text decoders to provide a general-purpose multi-modal chatbot.\nThe LLaVA model family stands out as a high-performance, open-source collection of models. Upon its release, the LLaVA-1.5 series achieved state-of-the-art (SoTA) performance across 11 benchmarks (image credit). We’ll also take a closer look at the architecture behind LLaVA—specifically the vision encoders and text decoders that make it work. This includes the popular CLIP and SigLIP training frameworks.\nUnderstanding how these models work lays the groundwork for my next blog post.\nLLaVA: Chatbots That Can \u0026ldquo;See\u0026rdquo; The LLaVA model family is a collection of vision-language models that use pre-trained Vision Encoders to give pre-trained Large Language Models (LLMs) the ability to understand images. Why all the pre-training? Well, pre-trained models help keep training costs low while still leveraging the latest in foundation model technology.\nThis combination of pre-trained models has made LLaVA models increasingly popular for multi-modal tasks. These models — some named more creatively than others — are open-sourced in various sizes, ranging from 0.5B to 13B parameters.\nMost LLaVA models are named after their base transformer architectures, but this one has been named after a beloved Turkish delicacy (source). Don’t forget that the Machine Learning Compiler (MLC) Engine (introduced in my last blog post) supports quantizing LLaVA models. In other words, the smallest LLaVA models are promising candidates for my edge multi-modal ambitions.\nVisual Instruction Tuning of Text Decoders Before we jump into how LLaVA works its magic, let’s take a quick look at how plain text-based LLMs operate. A typical Large Language Model (LLM) begins by breaking your input text into discrete units called tokens using a tokenizer. These tokens are then transformed into high-dimensional embeddings — essentially numerical representations the model can \u0026ldquo;understand\u0026rdquo;. The embeddings pass through multiple layers of self-attention mechanisms and feed-forward neural networks, which process the information and predict the next token in the sequence. This process continues iteratively until the model generates the desired output text.\nBut here’s the challenge: when it comes to multi-modal tasks, like combining images and text, your traditional text-based LLM hits a wall — it simply can’t “see”. To fix this, we bring in vision encoders. Vision encoders translate images into embeddings, a deep learning model\u0026rsquo;s \u0026ldquo;native language\u0026rdquo;. Afterwards, a text decoder translates these embeddings into an output text based on both the image and the text input. We align the feature space of the vision encoder with the LLM by adding a trainable linear projection layer on top of the vision embeddings. By fine-tuning the text decoder on a multi-modal dataset, the model learns to generate text that is relevant to the input image.\nThe LLaVA model family combines a vision encoder with a text decoder to generate human-like text based on images. The vision encoder converts images into embeddings, which are then fed into the text decoder as a visual instruction to generate the output text (source). This approach is called Visual Instruction Tuning. If you’re familiar with Instruction Tuning, it’s the same idea with a multi-modal twist. In regular instruction tuning, you give the model a text instruction (“Summarize this paragraph”) and train it to produce the desired text output. Visual Instruction Tuning swaps that text instruction for an image. The goal? Train the model to generate text that describes or explains the image.\nFor example, LLaVA models are trained on datasets that pair images with captions or multi-modal Q\u0026amp;A examples. This forces them to become fluent in both visual and linguistic cues. In these finetuning setups, we commonly keep the vision encoder fixed and only fine-tune the text decoder and projection layer on the multi-modal dataset. his way, we can leverage the pre-trained vision encoder to understand images without the need for additional training data, while also benefiting from the power of the pre-trained LLM to generate human-like text.\nVision Encoders Essentially, Visual Instruction Tuning swaps out text instructions for images and teach the model to generate text based on what it “sees.” But there’s a big question here: how do we get an image - essentially a 2D grid of pixels — to become compatible with a token-consuming text-based model? While Convolutional Neural Networks (CNNs) have traditionally excelled at extracting features from images, Vision Transformers have shown promising results in processing images as sequences of tokens, similar to text.\nHere’s how it works: we divide up an image into smaller, fixed-size patches rather processing it all at once. Each patch is then flattened and mapped into a high-dimensional feature space — a numerical representation that captures the patch’s visual characteristics. To make sure the model doesn’t lose track of where these patches belong in the image, we add positional encodings. A Vision Transformer then consumes these location-annotated image patches, processing them through multiple layers of self-attention and feed-forward neural networks.\nThis process allows the model to understand the relationships between different parts of the image and distill its content into a sequence of semantically meaningful embeddings. This sequence of embeddings is particularly compatible with classical LLMs because it mirrors the token-like structure LLMs expect for text. Thus, the combination of Vision Transformers with Text Decoders gives us high-functioning multi-modal models.\nIf you want to learn about Vision Transformers in more detail, I recommend taking a closer look at the University of Amsterdam\u0026rsquo;s Deep Learning Tutorials. For now, let\u0026rsquo;s focus on the training frameworks for vision encoders, as they are crucial for the performance of the final LLaVA model.\nTraining the Vision Encoder The quality of the final multi-modal LLaVA model heavily depends on the quality of its vision encoder. This is why it is crucial to train the vision encoder on a diverse and large-scale dataset to ensure that it can understand a wide range of images. Two popular training frameworks for vision encoders are CLIP and SigLIP. We describe both of them in a bit more detail here, since we need this knowledge to properly imbed a LLaVA model onto an edge device.\nCLIP Contrastive Language-Image Pre-training (CLIP) is a pre-training framework that teaches the vision encoder to understand images by associating them with text descriptions. Here’s the gist: CLIP trains the vision encoder to predict what the image means using a text description, and vice versa, to predict the image from a text description. By doing this, the model essentially learns a shared “language” that lets it understand both images and text.\nCLIP has been shown to achieve state-of-the-art performance on a wide range of vision tasks, making it a popular choice for vision encoders in multi-modal models, in particular due to its alignment of the vision and text feature spaces.\nCLIP trains vision encoders to predict image representations that align with text representations of their respective caption. By doing so contrastively over many image-text pairs in a batch, the vision encoder learns strong, semantic image embeddings (source). A key aspect in CLIP is its use of contrastive learning, where the model is trained to maximize the similarity between positive pairs (image-text pairs that belong together) and minimize the similarity between negative pairs (image-text pairs that do not belong together).\nExample. The vision encoders learns to match positive pairs (like a picture of a dog with a caption saying \u0026ldquo;a dog\u0026rdquo;) and push apart negative pairs (like a picture of a dog with a caption saying \u0026ldquo;a cat\u0026rdquo;).\nThe similarity is measured by a softmax, which is applied over the dot products between the representation spaces. This allows the model to learn a discriminative representation space that captures the semantic content of images and text.\nWhile this approach is intuitive, it doesn\u0026rsquo;t scale well. For large representation spaces, we need large batch sizes to effectively capture a large variety of (negative) image-text pairs and improve the training process. This raises challenges in CLIP, in particular with the softmax. To calculate the CLIP loss, we need to apply the batch-level softmax twice to normalize the pairwise similarity scores across all images for training the text encoder, and across all texts for training the image encoder. These passes over the full batch size can be computationally expensive, especially when the batch size is sharded across multiple devices.\nIn a distribution training setup, we are often using data parallelism, where each device processes a part of the batch. While commonly, each device can do the forward and backward pass independently and only the final gradients are communicated, CLIP needs to already communicate the softmax statistics for the loss between all devices. This creates an efficiency bottleneck, especially when we scale to many devices. This bottleneck prevents CLIP from being scaled efficiently, and it\u0026rsquo;s the exact problem SigLIP solves.\nSigLIP The Sigmoid Loss for Language Image Pre-Training (SigLIP) replaces the softmax in CLIP with a sigmoid loss, which is applied element-wise to the dot products between the image and text representations. The loss objective is then closer to standard predictive learning with binary cross entropy, training the positive pairs to be $1$ while negative ones are pushed closer to $0$. This allows the model to train on a large batch size without the need for full-batch softmax computation, making it more efficient and scalable.\nSigLIP has been shown to achieve similar or even better performance to CLIP, in particular for smaller datasets of image-caption pairs. Furthermore, SigLIP is more computationally efficient at scale, making it a popular choice for training strong vision encoders in multi-modal models.\nInference Once the vision encoder is trained, we can use it to generate embeddings for images. However, both CLIP and SigLIP train single-vector representations, meaning that the image is represented by a single vector. This is not ideal for multi-modal models, as we want to generate a sequence of embeddings that can be fed into the text decoder. Furthermore, a single vector representation may not capture the full content of the image.\nTo address this, we can use the internal representations of the Vision Encoder, which is commonly a Vision Transformer, to extract a sequence of embeddings for the image. This allows us to capture a more detailed representation of the image, which can be used by the text decoder to generate more accurate and detailed text descriptions. This is a key aspect of the LLaVA model family, which combines the power of Vision Transformers with Large Language Models to generate human-like text based on images.\nIn the next blogpost, we will see that there are actually several variations on how a Vision Transformer can be implemented in the CLIP and SigLIP frameworks (e.g. using a separate class embedding, pooling, etc.). These variations can have a significant impact on the performance of the final LLaVA model and require adjusting the model architecture accordingly. Hence, it is important to carefully consider them when implementing your own LLaVA model.\nMachine Learning Compiler Implementation Now that we are familiar with the LLaVA model family and the vision encoders used in these models, let\u0026rsquo;s discuss how we can deploy these models on edge devices. As we have introduced in Part 1 of this blog post series, we use the Machine Learning Compiler (MLC) project for on-edge deployment. MLC aims to optimize and compile deep learning models for edge devices, enabling efficient and fast inference on resource-constrained hardware. MLC supports a wide range of deep learning models, including LLaVA models, making it a powerful tool for deploying multi-modal models on edge devices.\nThe LLaVA Model Family on MLC Out of the box, the MLC LLM supports the quantization of the LLaVA family of vision encoders and text decoders. For this, the LLaVA implementation of the Hugging Face Transformers library has been integrated into the MLC framework using the TVM stack. At the time of writing (November 2024), the default supported text decoders are Llama and Mistral, with a CLIP-trained vision encoder. However, the sizes of these models are often around 7B parameters and larger, making them unsuitable for deployment on small edge devices (and even my M2 MacBook Air). This is why we need to consider smaller models, which are more suitable for our travel recommendation chatbot.\nLLaVA OneVision One of the smallest LLaVA models is the LLaVA OneVision Qwen2 0.5B, which, as the name suggests, uses the Qwen2 language model with 0.5B parameters.\nLLaVA-OneVision models tackle even the trickiest secondary school math problems with ease. In this example, the model demonstrates its ability to interpret multiple images and coherently apply deductive reasoning (image credit). The LLaVA OneVision Qwen2 0.5B model is particularly suitable for deployment on edge devices, as it is small and lightweight. While we can\u0026rsquo;t expect the same performance that larger models offer, this small LLaVA OneVision model is a good starting point for our travel recommendation chatbot and allows a fast iteration cycle for model development.\nIn contrast to the original LLaVA models, the LLaVA OneVision model uses a SigLIP-pretrained vision encoder, as it has demonstrated higher multi-modal LLM performance among open vision encoders. While we mentioned that in theory, there are no differences between SigLIP and CLIP during inference, the encoders slighlty differ in their Huggingface Transformers implementation. This is why we need to first port the LLaVA OneVision model and its SigLIP vision encoder to the MLC framework. Once again, I\u0026rsquo;ll walk you through how to do in the next blog post.\nConclusion Multi-modal foundation models allow us to apply state-of-the-art models to new and more complex applications. We\u0026rsquo;ve introduced how we can use a pre-trained vision encoder and text decoder architecture to build a general-purpose vision-language chatbot. The Machine Learning Compiler Project currently supports embedding the LLaVA vision-to-text models on edge devices. However, due to our restricted local device, we will use the smallest LLaVA OneVision model with only 0.5B, which needs to be manually ported to the MLC framework. Thus, we\u0026rsquo;ve taken some time to understand the intricacies between LlaVA model implementations and what this means during model training and testing time.\nWhat\u0026rsquo;s next? This blog post provides you with the background knowledge needed to deploy a multi-modal vision-to-text model on edge devices. In the next blog post of this series, I\u0026rsquo;ll walk you through how to put this knowledge into practice.\n","date":1732924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732924800,"objectID":"72faca95df140ba7cf96c59c279dafbb","permalink":"https://bellanich.github.io/post/edge-llm-vision-encoders/","publishdate":"2024-11-30T00:00:00Z","relpermalink":"/post/edge-llm-vision-encoders/","section":"post","summary":"Vision transformers, with help from training frameworks like CLIP and SigLIP, make  multi-modal foundation models like LLaVA possible — bridging the gap between vision and text.","tags":["LLM Optimization","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction Project Inspiration Why Edge Foundation Models Matter Challenges of Running Foundation Models on Edge Devices My Gemma Debacle The Memory Struggle Is Real MLC LLM: A Quantum Leap in Deploying Edge Foundation Models How does MLC LLM work? Quantization What is Quantization? Quantization Methods 1. Post-Training Quantization (PTQ) 2. Quantization-Aware Training Quantizing Transformers Out of the Box MLC LLM Solutions Custom Solutions Hardware Optimizations Just-in-Time Model Compilation MLC LLM Implementation Conclusion What\u0026rsquo;s next? Introduction Ever since ChatGPT went mainstream, I’ve been captivated by the rapid advancements in large language models (LLMs). As a machine learning engineer, I’ve been eagerly awaiting my chance to experiment with these groundbreaking models. Yet, the reality of deploying and managing the required infrastructure — and its massive cost — always made me pause.\nProject Inspiration This June at the Google I/O Connect Event in Berlin, I realized my vision of working with server-free LLMs wasn’t as far-fetched as I’d thought. Google showcased Geminin nano, a powerful LLM integrated directly into their latest Android devices. While it wasn’t accessible to developers yet, it was a glimpse of what might soon be possible.\nGoogle originally launched Gemini Nano on the Pixel 8 Pro in December 2023. This edge LLM has been gradually rolled out to other popular Android phones, including Samsung's Galaxy S24 series (source, image credit). Inspired by this progress, I set out to test the limits of edge LLMs. Could I, as a solo enthusiast, deploy a LLM on an edge device like my iPhone? To make the challenge even more intriguing, I decided to aim for a multi-modal LLM. After all, who wouldn’t want a private chatbot that understands your phone’s photo gallery and keeps your secrets safe?\nIn this 4-part blog series, I document my experiment to prove that you don’t need a sprawling server farm or a high-end workstation to dive into the latest AI technology. With a bit of machine learning knowledge, solid documentation, and plenty of determination, you can get started with state-of-the-art models without a painful cloud bill (looking at you, AWS).\nWhy Edge Foundation Models Matter Foundation models have traditionally been too massive to run on edge devices like smartphones, IoT gadgets, or embedded systems. As a result, they’re typically hosted on centralized servers, which introduces several challenges:\nCost barriers. Deploying and serving large-scale models (think 10B+ parameters) in the cloud is prohibitively expensive, often costing millions in infrastructure and energy. This creates a significant barrier for students, hobbyists (like me), and smaller organizations looking to experiment with AI. By running models locally on edge devices, the need for expensive server infrastructure disappears, democratizing access to this cutting-edge technology.\nUnreliable performance. Cloud-based inference depends on a steady internet connection to send data to servers and retrieve results. This back-and-forth can cause frustrating delays, especially in areas with poor connectivity. Edge models, which run directly on local devices, bypass these issues. They deliver faster responses and work well even without an internet connection.\nSecurity concerns. Cloud-based systems inherently require sending data to remote servers, which comes with inherent risks. For users, their personal chat data could be exposed in a security breach or misused without consent. Businesses, meanwhile, must navigate strict regulations like GDPR or HIPAA when transferring sensitive data off-device. By processing data locally, edge models eliminate these risks, ensuring that your personal information stays private.\nIn short, edge foundation models break down cost barriers, improve reliability, and address privacy concerns. They make AI more accessible for curious minds and businesses alike while offering end users more peace of mind.\nChallenges of Running Foundation Models on Edge Devices By now, you might be thinking, Edge foundation models sound amazing! Why isn’t everyone using them? Well, as with most things in life (and AI), there’s a catch. Running foundation models on edge devices isn’t exactly a walk in the park. Let me walk you through some of the challenges, starting with my own cautionary tale.\nMy Gemma Debacle About six months ago, I got my hands on Google’s shiny new instruction-tuned Gemma 2B model. Gemma, for those unfamiliar, is the “baby” version of DeepMind’s Gemini family — a lightweight, open-weight LLM designed for resource-constrained environments.\nWhy Gemma2B? In a nutshell, it's very impressive for its size. In benchmarks tasks, Gemma 2B's performance is comparable those of 7B - 9B LLMs. It also consistently tops the HuggingFace leaderboard for smaller LLMs. Gemma 2B calls itself a polyglot, claiming fluency in a bunch of languages. If that checks out, it’s a pretty exciting pick for global business apps — just the kind of thing a solo developer (like me) could put to work. Basically, Gemma 2B was designed for laptops, desktops, and modest cloud setups. It sounded perfect for my trusty MacBook Air M2 (8GB of RAM).\nSpoiler alert: it wasn’t.\nI excitedly set up Gemma and attempted to serve my first request. My MacBook? It practically waved a white flag and crashed halfway through.\nLet’s do the math to see why this happened. The Gemma 2B model has (you guessed it) 2 billion parameters. Using the standard float32 data type, the model parameters alone would require $2B\\text{ parameters} * \\frac{4 \\text{ bytes}}{\\text{parameter}} = 8 \\text{ billion bytes} = 8 \\text{ GB of RAM}$.\nBut that’s not all my machine needs to handle:\nI need extra memory for activations (the intermediate calculations during inference). The operating system (in my case, macOS) also needs a hefty chunk of RAM to do its thing In short, my poor MacBook was way out of its depth. Even with more efficient data types like float16 or bfloat16 (which halve memory usage), the combined memory demands of the model, activations, and system processes were just too much. Now, imagine trying to squeeze this kind of workload onto a smartphone with even less RAM. You’d be lucky if your phone didn’t catch fire (kidding\u0026hellip;mostly).\nThe Memory Struggle Is Real Edge devices are, by design, resource-constrained. They’re great for portability, but they aren’t built to handle the sheer memory and compute demands of large language models. Even lightweight models like Gemma, which aim to close this gap, can still overwhelm devices with limited RAM or processing power.\nBut don’t despair! Engineers and researchers are tackling these challenges head-on. By using model compression techniques like quantization, pruning, and distillation, they’ve managed to shrink memory and compute requirements significantly. Add to this a new wave of hardware optimization techniques, and edge deployment is more feasible than ever.\nNow, innovative tools are building on these advancements to make edge LLMs not just possible, but practical for a wide range of devices. Curious about how these breakthroughs are unfolding in real-world applications? Let me introduce you to one powerful solution: the MLC LLM framework.\nMLC LLM: A Quantum Leap in Deploying Edge Foundation Models Fast forward to November 2024, I decided to try the same task as before but with the Machine Learn Compiler (MLC) LLM Engine. This time, I deployed a pre-quantized version of this Gemma 2B model onto an edge device — specifically, an iOS app. The results blew me away.\nFor starters, I encountered zero performance issues on my MacBook Air. The pre-quantized and hardware-optimized Gemma model ran smoothly and efficiently, without any of the lag or crashes I had faced with six months earlier.\nBut here\u0026rsquo;s where things really got exciting: the quality of the responses. They were practically indistinguishable from the likes of massive, cloud-based LLMs in an everyday conversation. Curious to see how well this mini-model handled other languages, I threw some Spanish and German at it. To my non-discerning eye, the results looked spot-on. (I’d love to hear what native speakers think, though.)\nI decided to practice my very rusty Spanish and seek vacation inspiration in one-go. Here are Gemma's recommendations for a visit to Valencia, Spain 🇪🇸 in Winter. Christmas time means Christmas Markets 🎄 — especially when you live so close to Germany 🇩🇪. So, I decided to see if Gemma could give me any fun suggestions in German. Now, you might be wondering: How did MLC manage to pull this off? Let’s take a step back and dive into the tech behind this feat.\nHow does MLC LLM work? At a high-level, the MLC LLM project makes it possible to embed smaller LLMs (under 10B parameters) on edge devices through a streamlined, three-step process:\nQuantize LLM weights as the model is downloaded. This prevented my machine from crashing due to insufficient memory. Embed the quantized model with hardware-specific optimizations applied during the model compilation stage. Provide a simple, pre-built user interface to interact with your newly embedded foundation model. The MLCEngine embeds LLMs across different software platforms through model quantization and hardware-specific optimization (image credit) MLC offers a user-friendly, open-source chat application for both Android and iOS. Alternatively, it implements its own version of OpenAI\u0026rsquo;s Python API, making it easy to integrate the optimized LLM into your own existing projects.\nQuantization MLC LLM caches pre-quantized model weights and compiled model library locally, which means you only need to download and quantize the model once. After that, the quantized model is ready to run on your device without requiring repeated downloads. This saves both time and bandwidth, making the process smoother and more efficient.\nWhat is Quantization? In simple terms, quantization is the process of reducing the precision of the numbers that represent a model’s parameters. The goal? Shrink the model’s memory footprint while keeping its performance as close to the original as possible. The real magic happens when you see the cost savings—quantization can cut your cloud compute bills by half, a quarter, or even a sixth, without any noticeable drop in performance. For massive models like LLMs, those savings can really add up.\nTake the example of yurts, a contractor for the U.S. government. They slashed its monthly cloud computing bill from USD 24,000 to USD 4,000 for a 70B parameter LLM by using a quantization method called Activation-aware Weight Quantization (AWQ). Pretty impressive, right?\nQuantization Methods When it comes to quantizing a model, there are a few common methods, but the two main approaches are:\n1. Post-Training Quantization (PTQ) After a model is trained, you can apply quantization to reduce the bit-width of its weights. The best part? It’s quick, easy, and requires minimal changes to the original model, while still offering significant memory savings.\nOne common PTQ technique is grouped quantization, where the model’s weights are grouped based on features like their layer or importance. Each group is quantized separately, making the process more tailored and efficient. This method has been around since the late 1990s and continues to evolve as a way to balance performance and memory efficiency.\nSome weight groups are more sensitive to quantization errors and need higher precision (more bits) to maintain accuracy. Others can handle lower precision without a noticeable hit to performance.\nWith the rise of foundation models, more specific implementations of grouped quantization have emerged. For an in-depth look, check out \u0026ldquo;The case for 4-bit precision: k-bit Inference Scaling Laws\u0026rdquo; and \u0026ldquo;The LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models\u0026rdquo;.\nMore recently, techniques like Activation-aware Weight Quantization (AWQ) have taken this dynamic quantization approach even further. AWQ uses activation statistics to pinpoint the most important individual weights and ensures they aren’t over-quantized, allowing for better compression without sacrificing performance.\n2. Quantization-Aware Training This method goes a step further by training the model with lower precision in mind from the start. By optimizing the model for reduced precision during training, you often get better results than you would with post-training quantization. Essentially, it allows the model to “learn” how to perform well with less precision, resulting in better overall performance. However, as we focus on deploying pre-trained models, we won’t explore this method further.\nQuantizing Transformers When quantizing Transformers, it’s not just the weights that need attention—activations play a big role too. Activations are the intermediate values generated during the model\u0026rsquo;s forward pass as it processes the input data. In a Transformer, these are the values produced at each layer as it handles each token. Just like with weights, activations can also be compressed during quantization, which further reduces memory usage.\nBut memory management doesn’t end with weights and activations. For Transformers, there’s also the key-value (KV) cache — this stores the context of the input sequence as the model processes longer inputs. As the model processes longer and longer inputs, it needs more memory to store the increasing number of keys and values. To keep things efficient, MLC LLM provides additional memory optimization techniques, like sliding windows, which help manage memory usage even when dealing with longer sequences.\nThe key-value (KV) cache in Transformers preserves the context of processed tokens, enabling the model to \"remember\" earlier parts of a conversation. Yet, as the conversation grows, the cache scales up rapidly, incurring risks of out-of-memory errors on edge devices if not handled properly. (image credit) Out of the Box MLC LLM Solutions As you can probably guess, the MLC Engine only implements post-training quantization (since we have no control over an open sourced LLM\u0026rsquo;s training process). In particular, MLC LLM implements the grouping quantization methods shown below.\nMethod Name Weight Quantization Activation Quantization Version No. Stable? q0f16 None 16 bits - Yes q0f32 None 32 bits - Yes q3f16_1 3 bits 16 bits 1 Yes q4f16_1 4 bits 16 bits 1 Yes q4f32_1 4 bits 32 bits 1 Yes q4f16_awq 4 bits 16 bits 1 No Table 1: An Overview of MLC's implemented quantization methods (source). MLC Enginer also offers an AWQ implementation (called q4f16_awq), but it\u0026rsquo;s currently unstable so use it at your own risk.\nOf course, the folks behind MLC have already gone and quantized most of the very popular open-source LLMs. You can download these pre-quantized model weights from their official MLC AI\u0026rsquo;s HuggingFace account.\nIf you want to quantize a new model, then there\u0026rsquo;s a little more work involved. MLC right now supports quantization of these model types: baichuan, bert, chatglm3, cohere, eagle, gemma, gemma2, gpt2, gpt_bigcode, gpt_neox, internlm, internlm2, llama, llava, medusa, minicpm, mistral, mixtral, orion, phi, phi3, phi3v, qwen, qwen2, qwen2_moe, rwkv5, rwkv6, stable_lm, and starcoder2.\nSo, if you want to quantize of these model types yourself, then all you have to do is run a few simple commands.\nCustom Solutions If you want to quantize an unsupported model type, you\u0026rsquo;ll need to extend MLC LLM\u0026rsquo;s source code. This involves inferring your target model\u0026rsquo;s architecture from its source config.json file on HuggingFace and wrapping its original Python definition (e.g., from the transformers Python library) with MLC LLM\u0026rsquo;s wrappers. I ended up having to do this to support multi-modal functional in the 3rd blog post in this series.\nHardware Optimizations Quantization is just one part of MLC’s bag of tricks. The other? Squeezing every last drop of performance out of your hardware through smart optimizations. See, your LLM model might start as high-level Python code, but it doesn’t interact directly with your device’s hardware. There’s a crucial middle step where MLC translates that model into something your CPU or GPU can actually understand—and it does this in the most efficient way possible.\nJust-in-Time Model Compilation Just-in-Time (JIT) model compilation is the secret sauce behind MLC’s stellar efficiency. Instead of pre-compiling everything in advance or running the model eagerly line-by-line, JIT optimizes your model right before it executes, ensuring it’s perfectly suited to your specific hardware.\nJIT strikes a balance between two compiler approaches:\nInterpreted execution processes code step-by-step as it runs. This makes the code super flexible and easy to debug, but leaves no room for optimizations. In other words, it\u0026rsquo;s painfully slow. Ahead-of-Time (AOT) compilation pre-compiles everything into a fixed version before execution. This is much faster, but comes with a catch: we assume a one-size-fits-all solution. If the model encounters unexpected conditions or hardware variations, AOT’s rigid approach can leave performance on the table because it can’t take full advantage of the specific device running the code. JIT avoids these pitfalls by waiting until runtime to optimize. It tailors the model’s code to your hardware and execution context just before runtime, ensuring maximum efficiency. Here\u0026rsquo;s how this process works:\nTracing or scripting. First, the engine analyzes your model’s high-level code and maps out its computation graph and operations. Think of it as creating a blueprint for what the model will do Optimization. Next, the engine gets to work refining that blueprint. It fuses operations, removes redundancies, and inlines functions, streamlining execution wherever possible. (It’s like an architect revising a blueprint for a more efficient construction process.) Low-level code generation. Once the optimizations are done, the graph is compiled into low-level machine code tailored to your specific hardware—whether that’s a CPU, GPU, or something fancier. Execution. Finally, the optimized code is executed, running faster and using less memory thanks to all the pre-launch optimizations. MLC uses JIT model compilation to get the most out of your edge device\u0026rsquo;s limited resources. And the best part? This process is abstracted away into a few simple CLI commands.\nMLC LLM Implementation Deep neural networks are computationally demanding. Hence, most deep learning frameworks include built-in JIT compilation extensions. For example, Accelerated Linear Algebra (XLA), the backbone of JAX, offers cross-framework JIT support. Looking specifically at PyTorch, torch.compile provides a general-purpose solution that supports both training and inference.\nHowever, MLC takes it a step further by leveraging Apache\u0026rsquo;s Tensor Virtual Machine (TVM) for even deeper hardware-level optimizations.\nWe can think of TVM as an improvement on PyTorch's optimizations, offering advanced optimizations and hardware-specific tuning that PyTorch's JIT lacks. Additionally, TVM is easy to use due to the separation of its compiler and runtime components. This makes it possible for me to compile a ML model on one machine (e.g., a MacBook) and deploy it on another (e.g., a Raspberry Pi). (image credit) TVM works by providing a Python API for tensor operations like matrix multiplications, sums, and type conversions. It also makes it a breeze to port models from PyTorch. Once we have the model in TVM, we can translate it into C++ as we optimize it for execution.\nHere’s how exactly TVM supercharges model optimization:\nOperation Fusion. TVM combines smaller operations (like element-wise additions or multiplications) into larger, more efficient ones.\nExample. Instead of calculating ReLU(x) followed by Add(x, y), TVM can combine them into a single, efficient kernel, saving memory and time.\nMemory Layout Optimization:. TVM fine-tunes memory access patterns to align with the hardware’s strengths. For example, GPUs perform better when accessing data in large, coalesced blocks, while CPUs benefit from loop optimizations that prevent cache misses.\nKernel Selection and Tuning. A \u0026ldquo;kernel\u0026rdquo; is a specialized function designed to perform specific operations, like matrix multiplication. TVM either selects the best pre-tuned kernels or auto-tunes them for maximum performance on the target hardware.\nThese optimizations make it possible to (hypothetically) fit a 7B+ parameter model onto an iPhone. But of course, there’s a trade-off: the more optimizations we apply, the less flexible the model becomes. Debugging also gets trickier — any issues that arise are often low-level errors, especially when input sizes change.\nDespite these challenges, the benefits far outweigh the costs. Without TVM, deploying models on edge devices would be much more difficult.\nConclusion In the past six months, the AI research community has made groundbreaking strides in optimizing foundation models for edge devices. Back in June 2024, my personal machine crashed when I tried to run the Gemma 2B model locally — without quantization or hardware optimizations. But thanks to the rapid progress in this field, even solo enthusiasts like myself can now, as of November 2024, easily deploy the same model (or even larger ones) locally—without needing to become compiler engineers.\nIn this blog post, I’ve introduced the Machine Learning Compiler (MLC) as a powerful new tool to make this possible. I’ve also walked you through its inner workings and provided essential background knowledge to help you get started.\nWhat\u0026rsquo;s next? In my next blog post, we’ll dive into how we can extend the MLC Engine to support embedding LLMs that aren’t natively supported. After all, our goal is to deploy a multi-modal LLM on an iPhone.\n","date":1732838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732838400,"objectID":"954b0af80ab45bdef1021e282d31758d","permalink":"https://bellanich.github.io/post/edge-llm-mlc/","publishdate":"2024-11-29T00:00:00Z","relpermalink":"/post/edge-llm-mlc/","section":"post","summary":"The open-source Machine Learning Compiler Engine project is transforming foundation models into efficient and portable powerhouses.","tags":["LLM Optimization","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":"","date":1713350700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713350700,"objectID":"0dc754f4308cd3b8751e26c7425683e9","permalink":"https://bellanich.github.io/talk/the-mlops-blueprint-productionalizing-machine-learning/","publishdate":"2024-04-17T10:45:00Z","relpermalink":"/talk/the-mlops-blueprint-productionalizing-machine-learning/","section":"event","summary":"I joined the Women Who Code Community to give a crash course in Machine Learning Operations (MLOps). This talk brings together different MLOps perspectives to create a clear and actionable framework, giving you the blueprint to navigate real-world ML systems.","tags":["Talks"],"title":"The MLOps Blueprint: Productionalizing Machine Learning","type":"event"},{"authors":null,"categories":null,"content":"In the past month, I\u0026rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it\u0026rsquo;s also about crafting solutions that rise above them altogether.\nIn this case, I found myself asking: \u0026ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project\u0026rdquo;? Well, from putting together this Python ML Template repository, my answer is three. (See the demo above for a quick example.)\nIf you\u0026rsquo;re curious about my implementation, checkout my project\u0026rsquo;s README file and documentation. Happy ML feature development!\n","date":1711324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711324800,"objectID":"042ea30eb72b64cfa2bc5adc2ac11469","permalink":"https://bellanich.github.io/post/python-ml-template/","publishdate":"2024-03-25T00:00:00Z","relpermalink":"/post/python-ml-template/","section":"post","summary":"After churning out too many projects from scratch in one month, I built this ML template to make life easier—for both of us. Start ML development with just 3 commands.","tags":["News"],"title":"From Framework to Functionality: 📢  My Custom Python ML Template Launch","type":"post"},{"authors":null,"categories":null,"content":"In the past month, I\u0026rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it\u0026rsquo;s also about crafting solutions that rise above them altogether.\nIn this case, I found myself asking: \u0026ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project\u0026rdquo;? Well, from putting together this Python ML Template repository, my answer is three. (See the demo above for a quick example.)\nIf you\u0026rsquo;re curious about my implementation, checkout my project\u0026rsquo;s README file and documentation. Happy ML feature development!\n","date":1711324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711324800,"objectID":"60758eda9188d20b21305bfe0f1b0cee","permalink":"https://bellanich.github.io/portfolio/python-ml-template/","publishdate":"2024-03-25T00:00:00Z","relpermalink":"/portfolio/python-ml-template/","section":"portfolio","summary":"A starter kit for Python-based Machine Learning projects. Jumpstart your ML development with just 3 bash commands 🚀","tags":["Code"],"title":"Python ML Template","type":"portfolio"},{"authors":null,"categories":null,"content":"As a Machine Learning engineer, I use Git daily, but even useful commands can feel cryptic at times. I struggled to find a cheatsheet that bridged the gap between theory and real-world application. That\u0026rsquo;s why I created my own cheatsheet – a resource that demystifies core concepts alongside practical commands.\nDownload the PDF below, or grab the source code to customize it for your needs. Let\u0026rsquo;s up our Git game together!\n","date":1698710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698710400,"objectID":"064956c829afdd29e4fe6832c36c9752","permalink":"https://bellanich.github.io/portfolio/git-cheatsheet/","publishdate":"2023-10-31T00:00:00Z","relpermalink":"/portfolio/git-cheatsheet/","section":"portfolio","summary":"A brief but comprehensive guide to Git -- both in practice and in theory","tags":["Notes"],"title":"Git It Right: A Complete Cheatsheet","type":"portfolio"},{"authors":null,"categories":null,"content":"As a Machine Learning engineer, I use Git daily, but even useful commands can feel cryptic at times. I struggled to find a cheatsheet that bridged the gap between theory and real-world application. That\u0026rsquo;s why I created my own cheatsheet – a resource that demystifies core concepts alongside practical commands.\nDownload the PDF below, or grab the source code to customize it for your needs. Let\u0026rsquo;s up our Git game together!\n","date":1698710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698710400,"objectID":"49fecb387c4b9fa8927f69c12b745898","permalink":"https://bellanich.github.io/post/git-cheatsheet/","publishdate":"2023-10-31T00:00:00Z","relpermalink":"/post/git-cheatsheet/","section":"post","summary":"As I transition to my new role, I used my downtime to go deep into Git — a tool I rely on daily — and condensed everything I learned into a concise, 3-page cheatsheet.","tags":["News"],"title":"Notes Alert: 📓 A Handy 3-Page Cheatsheet for Version Control Pros","type":"post"},{"authors":["Bella Nicholson","Aoibhinn Reddington"],"categories":null,"content":"","date":1698317100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698317100,"objectID":"60c5fe00eab021fe4256e5008702500b","permalink":"https://bellanich.github.io/talk/a-case-study-on-building-scalable-machine-learning-systems/","publishdate":"2023-10-26T10:45:00Z","relpermalink":"/talk/a-case-study-on-building-scalable-machine-learning-systems/","section":"event","summary":"I delivered a guest lecture for TU Delft's Machine Learning course. I introduced fundamental ML engineering topics, as illustrated through my real-life experiences at a Dutch e-commerce tech company.","tags":["Talks"],"title":"A Case Study on Building Scalable Machine Learning Systems","type":"event"},{"authors":["Bella Nicholson","Bob Borsboom","Tim van Loenhout","Jochem Hölscher"],"categories":null,"content":" Introduction When it comes to policy based methods, gradient behavior can be very telling. In essence, gradient stability determines: (a) how long it takes a given model to learn and (b) whether it can even learn anything at all. To illustrate what we mean, we will spend this blog post investigating the stability of policy gradients in a few different contexts (i.e., different algorithm-environment combinations). Particularly, we use the REINFORCE, G(PO)MDP and G(PO)MDP+ whitening algorithms, since we can view the latter two algorithms as a progression of the former such that each step of this progression leads to more stable gradients. Meaning, these algorithms are similar enough to each other such that we can make fair comparisons, but also are theoretically guaranteed to exhibit different degrees of gradient stability. We apply these algorithms to two classical RL problems: a self-designed version of GridWorld and OpenAI’s CartPole problem. We do so since both environments vary in their rewards distributions and state-action space complexity.\nDon’t worry if you’re not completely up to date on your reinforcement learning knowledge. We’ll first briefly introduce all the need-to-know concepts — including a proper explanation of what policy gradient-based methods are, and what our chosen environments look like. So, if you’re more confident in your RL knowledge, feel free to skip ahead to our experiments.\nAre you ready?\nGridWorld GridWorld is one of the most simple RL problems, since it’s so easy to visualize and hence properly understand. As such, the intuition that this problem provides is not only invaluable, but it is also needed for our dive into policy based methods. Therefore it will be the first thing we discuss.\nThe premise of GridWorld is as the name suggests: we model our “world” as a finite grid. Generally speaking, our agent knows nothing about this world at first, but it needs to learn the best way to move through it. Thus, our agent relies on trial and failure. Consider the GridWorld problem of teaching a robot to pick an apple from the apple tree. We will model the field our robot is walking through as a 5 x 5 grid, since our robot cannot vary its step size. For simplicity’s sake, we’ll assume that if the robot reaches the apple tree, then it is always able to grab an apple. When it lands in a square that contains an apple tree, we give it a reward of +2. If the robot is unlucky enough to find a rock to trip over, it gets a penalty of -1 as we want it to walk without hurting itself. To keep things interesting, we don’t give the robot any feedback unless it falls or reaches its goal. Whenever the robot gets an apple or runs out of time, we wipe our rewards count clean, pick it up, and place it at the same starting point to try again.\nPolicy based methods In Reinforcement Learning, there are different ways to learn a good policy. For example, you can first explore GridWorld by random walk until you either discover an apple tree or a pesky rock. Afterwards, different algorithms have different ways of extracting a policy from the “experience” we’ve gained. Suppose, we want to learn this policy directly from our past experiences rather than rely on indirect methods. In essence, this is what policy based methods do. We convert policy learning into a calculus problem, where we model the cumulative rewards our robot receives as a function with the intent to optimize it.\nREINFORCE With REINFORCE, we play a game (e.g. our apple-picking robot in GridWorld) until we reach some terminal state (e.g., arriving at an apple tree). Afterwards, we start all over again to gather even more information about how to formulate the best strategy. During the course of each run through this game, we sum all the derivatives over the log probabilities of an action given its current state. Once we hit our terminal state, our trajectory gets cut off and we multiply the sum of gradients with that trajectory’s reward. We formalize this process as:\nWe run the algorithm N times and then take the average over these runs to get a more stable and thus less-noisy update. Thus, the expression above becomes\nwhere we not only loop over every i-th trajectory but also every state-action pair t that we encounter. Once our gradients are calculated, we use the following update rule to calculate our new parameter values. This rule will improve the policy.\nwhere alpha represents our learning rate.\nIf we refer to Figure 2, alpha literally determines the step size we take along our expected rewards surface once we learn something new. For these reasons, it is also interchangeably referred to as the step size. While more complex methods anneal the step size with respects to time, we’ve kept things simple and treated alpha as a constant hyperparameter.\nAn Intuitive Look at REINFORCE As we’ve previously established, we multiply a trajectory’s reward by the sum of its gradients. Suppose, we obtain a cumulative negative reward, because our robot walked over a few obstacles before it reached an apple tree. Now, the gradients are multiplied with a negative number. Meaning, our new policy lowers the likelihood of performing the same actions given each respective state. On the other hand, suppose that we get a positive reward, since our robot successfully walked toward an apple tree. In this case, our policy will make the selection of these actions for these given states more probable in future trajectories.\nThe Problems with REINFORCE Thus far, we’ve painted a rather rosy picture of the REINFORCE algorithm. Now, it’s time to take a more critical look at REINFORCE and see what room there is for future improvements. Let’s look at two paths that our robot can take in a slightly different version of GridWorld.\nOnce again, our robot’s objective is to walk towards the positive reward without hurting itself. In the lefthand side of the figure, the agent’s performance is perfect until its last step. Now, REINFORCE will make ALL of these actions less probable. Whereas, we only want to “punish” the last action.\nA similar mistake happens again on the right hand side of figure 3. Here, the total reward for that trajectory is zero, so the model won’t change the action-state probabilities. In other words, we don’t learn from the mistakes we made by choosing the wrong first action. Over enough trajectories, REINFORCE may be able to find the optimal policy, but this high degree of variance means that this process takes longer.\nG(PO)MDP G(PO)MDP is an improvement as it fixes the previously mentioned mistakes of REINFORCE. Instead of multiplying the reward of the trajectory by all the actions performed in that trajectory, it handles this situation in a more clever way. G(PO)MDP only multiplies the reward from a trajectory by the steps which lead to this action. We formalize this introduced temporal-reward dependency as,\nWe simply sum over all the steps in a trajectory, but here comes the interesting part: the right summation sums all the log derivatives of actions taken until we get a reward at time step t’. Afterwards, we multiply this reward by the steps which lead to this reward. Meaning, only the actions that lead to a reward will be “influenced” by that reward. While all the actions were previously influenced by the rewards which came before that set of actions, G(PO)MDP provides a fairer policy update. Simply put, our action probabilities now increase or decrease solely based on the reward a given action leads to.\nThis tweak in our objective function makes G(PO)MDP more fair in its assignment of action-state likelihoods and lowers the variance of our policy gradients. However, we’re still interested in lowering it even further, since this can decrease the time it takes us to figure out what the optimal strategy is.\nBaselines We’ll take a look at baselines now as a means to further lower policy gradient variance. Baselines are a trick researchers found to stabilize policy learning through the reduction of gradient variance. Fundamentally, we subtract a constant estimate of our expected rewards from our current return G_t Meaning, our parameter update becomes\nwhere we refer to this expected rewards estimate as a baseline. Many baselines variants have been proposed and tested, and each comes with its own set of advantages and disadvantages. To keep things simple, we’ve settled for a baseline technique called whitening, where we scale our cumulative returns by their mean and standard deviation values. i.e.,\nWhile more sophisticated methods do exist, whitening gives us a basic understanding of a baseline alters policy gradient behavior.\nDifferent environments With the premises of our chosen algorithms established, we also need to discuss the problems we’ll be using these algorithms to solve. Since algorithm behavior largely depends on the given environment, our GridWorld problem isn’t enough to gain a comprehensive picture of gradient stability. As a result, we also need to consider an environment that is significantly different from GridWorld — both in its state-space structure and in its reward distribution. That is, we want a continuous state-space environment that offers more immediate feedback.\nThus, we settled for OpenAI’s CartPole environment, which is described as follows:\nCartPole. A pole is attached by an unactuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force left or right to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center. A link to the environment can be found here.\nIf you want to learn more about the OpenAI Gymnasium, take a look here.\nThrough this choice of environments, we can see two different “forces” that contribute to a problem’s difficulty: (1) the size of the state-action space we have to search through, and (2) the level of feedback our agent receives for every good or bad action that it takes. Intuitively speaking, the more feedback you receive as you try to learn some new skill or task, the easier the learning process becomes.\nFrom this perspective, neither problem is indisputably more difficult than the other. The CartPole environment requires our agent to navigate through a more complex state-space; however, GridWorld does not always offer feedback. Our agent can move in such a way where it neither encounters an apple tree or a rock, and thus receives no reward. This level of non-immediate feedback adds complexity to what seemed like a rather simple problem.\nGradient Stability Up to this point we’ve established the importance of gradient stability but have not yet detailed a means to quantify it. Given that the concept of gradient stability is simultaneously intuitive and nebulous, we need to reframe the way we view it in order to make it measurable. More specifically, we define gradient stability by how similar policy gradients are to one another within each time step. To do so, we use the statistical analysis tools of variance, kurtosis and gradient sign change.\nVariance Also known as the second central moment, variance describes the spread of the gradients, and thus clues us in to our model’s update precision. i.e., How consistent or steady are our model updates from to run? High levels of variance indicate that the policy gradients go in many different directions, while low levels of variance mean our gradients are more or less uniform in the direction they point in.\nKurtosis However, distributions with identical means and variances can still have have very different shapes. In such cases, we also need to look at kurtosis (the fourth standardized central moment), which describes how heavily tailed a distribution is. In other words, kurtosis quantifies the presence of gradient outliers. The higher kurtosis is, the more extreme these gradient values become. This in turn makes makes our policy gradients become more inconsistent — i.e., less stable.\nFigure 5 - Different kurtosis values for identical variance (Source) Sign changes Finally, the number of sign changes that occur between time steps also provides clues about gradient stability. If the gradient signs keep changing, this means that we are literally walking back and forth along our loss surface. This implicates one of two possibilities, we either: (1) are consistently overshooting in our updates, or (2) are moving randomly as we do not know where to go. Either way, this indicates less confidence in our parameter updates and hints at model instability.\nExperimental Setup We can breakdown our gradient stability analysis into two parts: (a) differences in stability between algorithms in the same environment, and (b) differences in stability between the same algorithm in different environments. Of course, we’ve already covered part (a) in our background. Over all, G(PO)MDP is not influenced by the noise of rewards from prior actions, and whitening only further reduces the variance of whatever policy based method we apply it to. As such, we expect REINFORCE to have the least stable gradients, followed by G(PO)MDP, and by then G(PO)MDP + whitening.\nMeanwhile, the complexity of CartPole’s state space could highlight the shortcomings of REINFORCE. However, this overlooks the main distinction between REINFORCE and G(PO)MDP as one handles rewards considerably better than the other. Therefore, the sparsity of GridWorld’s rewards distribution may outweigh the problems introduced with high state-action search spaces. As a result, we expect a larger difference in policy behavior between the algorithms in the GridWorld environment. We used the following experimental setup to verify our hypothesis.\nPolicy Architecture Generally, the problem we are trying to solve dictates our policy network architecture. As we’ve established previously, our state-action space is small and discrete in GridWorld. More specifically, we only have 25 potential states and 4 potential actions to choose from. Given that we define a policy by the action distribution for each state, our policy is nothing more than a look-up table. In practice, this translate to the use of a single linear layer when we apply (deep) policy gradient methods. In contrast, once we delve into continuous state-space problems, we need more complex (i.e., multi-layered) models. Thus, for the CartPole environment, we default to a simplified version of the standard Q-Network architecture, as shown in Figure 6. The original DQN paper can be found here. Note that we use a final softmax layer to get a probability distribution over the possible actions we can take.\nIf you want to learn more about the basics of neural networks, this is a good starting point.\nParameter Configurations Initially, we conducted hyperparameter tuning with respects to each algorithm-environment combination, but quickly discovered that the environment more or less determined the optimal learning rate. Here, we define the optimal learning rate as the hyperparameter value which yields the highest cumulative rewards on average. Of course, we cannot apply the same exact process to define our discount factor, since the higher our discount factor is, the higher our cumulative rewards become by definition. However, a good discount factor is still important. So, we \u0026ldquo;tuned\u0026rdquo; discount factor by whichever value lead to convergence — which was once again model specific. Table 1 summarizes all of the values that we used.\nObtaining policy gradients We apply each of our three algorithms to the two environments we’ve previously described, and train our model for N=800 episodes. Every 20 iterations, we pause policy training to test policy performance. In this validation step, we sample a 100 different episodes to gather the gradients of each weight within our network as well as the cumulative rewards observed. The latter is particularly important, since it allows us to contextualize how well our policy is really doing. Meanwhile, the former allows us to understand how gradient behavior varies with respects to time. Due to the sensitivity policy networks exhibit towards their initialized weights, we repeat each model-environment combination using 20 different seeds and report the average statistical measurements that we’ve observed.\nObtaining variance, kurtosis and sign changes Since the collected policy gradients values are high dimensional tensors, we cannot directly visualize the policy gradient distribution. Instead, we compute the variance and kurtosis for each individual network weight over the 100 episodes sampled per validation step. Then, we take the average over these statistics to obtain a set of aggregated variance and kurtosis measurements. Together, these values describe the average gradient distribution for a single algorithm-environment combination at a specific time step. We repeat these calculations for all sampled policy gradients and plot our results. Note that the obtained gradients are squeezed between -1 and 1 across the episode dimension. This rescaling does not impact the variance/kurtosis ratio between two algorithms for a single weight gradient, but it does make sure that all weights contribute equally to our aggregate variance and kurtosis values.\nResults Starting with a look at gradient variance, Figure 7 proves that a choice in environments makes a stark difference in policy gradient behavior. As expected, we observe clear differences in gradient stability when in GridWorld, but when not when we’re in CartPole. This confirms our hypothesis that the rewards distribution has the capacity to incite more instability than the complexity of our search (state-action) space.\nInterestingly enough, Figure 7b suggests that REINFORCE somehow exhibits the least amount of variance. Outside of any other context, one could mistakenly presume that this means REINFORCE is the best performing algorithm. However, Figure 8 reveals a very different reality: REINFORCE performs so poorly that our model never learns anything. In the context of our apple-picking robot, our robot literally runs in circles and continuously trips over the same rock. Often times, it only stops when it runs out of time (i.e., the number of steps it can take per episode). With this knowledge, we can re-contextualize the abnormality of REINFORCE’s variance as a sign that REINFORCE is consistently random.\nOf course, this begs the question: How would REINFORCE perform as an objective function if we were able to get a sensible (i.e., non-random) policy? To answer this question, we trained our policy networking using our most stable algorithm, G(PO)MDP + whitening, and only use each algorithm’s respective loss function for policy gradients sampling (Figure 9). While the results observed for Figures 7a and 9a are more or less interchangeable, the same cannot be said for Figures 7b and 9b. In Figure 9b, REINFORCE’s variance starts low as the policy is close to random, but it soon enough converges to the variance of G(PO)MDP. Note that G(PO)MDP + whitening has the lowest variance of all, but this is only logical since the model has been trained to minimize this specific loss function.\nNonetheless, we have to acknowledge that having the same variances does not implicate different policy gradient distributions to be the same. Meaning, the results we obtain from Figures 7a and 9a are, in a sense, inconclusive. For these reasons, we also consider the kurtosis of our policy gradients (Figure 10). REINFORCE exhibits the highest levels of kurtosis followed by G(PO)MDP and then by G(PO)MDP + whitening. Once again the higher kurtosis is, the more gradient outliers a given algorithm has and the noisier its updates become. Meaning, the stability of each algorithm in CartPole is as we expected it to be.\nWith the stability established with respects to each algorithm and each problem, we also want to look more closely at how gradient stability varies over time. To keep our figures legible, we only visualize the percentage of positive gradient sign changes. As seen in Figure 11, our policy gradients generally start off as relatively balanced as we begin to make our away along our loss surface. However, during the progression of policy learning, this balance tips in favor of negative gradient sign changes. When this decline in percentage of positive gradient changes is monotonic, we deem overall model learning to be stable. Given that our model relies on these policies to learn, we can view the stability of policy gradients and model learning as one and the same.\nSurprisingly, there is a very slight decline in the percentage of positive gradient sign changes observed in REINFORCE’s gradients (Figure 11b). However, we don’t believe that this model slightly improves on a consistent basis, since GridWorld is too simple to provide opportunities for nuanced policy learning. Rather, we attribute this slight decline to the sensitivity policy networks have towards their initialized weights. Likely, there might be one or two policies out of the 20 seeds that we averaged over where our agent was actually able to learn something using reinforce — even if this something was to avoid states where it would trip. Similarly, we can also use this to explain the erratic behavior of REINFORCE with respects to its policy gradient kurtosis in CartPole.\nConclusion As seen throughout our results, context is critical when describing policy learning stability. Whenever we rely on a single figure, we risk critically misinterpreting our results and landing on a faulty assumption — such as mistaking our worst performing algorithm for our best performing one. This need for context extends well beyond the consideration of many measurements and goes into explicitly stating the underlying assumptions we make when formulating a hypothesis. After all, in all of our predictions, we assumed the models would eventually learn something about how to play each game, even this something was far from perfect. By never formalizing this assumption, we never considered the alternative possibilities: models are never guaranteed to learn. Especially, models as simple as the ones we implemented here.\nGitHub If you want to take a closer look to our implementation, here you can find a link to our github. Have fun!\nNote. This blog post was originally posted on Medium in October 2020. This assignment was completed as a part of the University of Amsterdam\u0026rsquo;s reinforcement learning course.\n","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"33a178cf80848e88e0171d10f194f54e","permalink":"https://bellanich.github.io/post/reinforcement-learning/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/post/reinforcement-learning/","section":"post","summary":"How does the gradient stability differ between REINFORCE, G(PO)MDP, G(PO)MDP+ whitening during policy learning?","tags":["Academic","Reinforcement Learning","Article"],"title":"Reinforcement Learning: Investigating Gradient Stability in Policy Based Methods","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1f4cb24553577f6808a73065326a8b9d","permalink":"https://bellanich.github.io/blog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://bellanich.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://bellanich.github.io/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://bellanich.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]