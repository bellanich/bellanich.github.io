[{"authors":null,"categories":null,"content":"I’m a ML engineer at Brenntag, where I deploy and maintain ML solutions across 70+ countries. My work focuses on applying software engineering principles to increase the stability and performance of ML systems.\nSimply put, I deliver business value by improving ML system efficiency. For example, I stabilized the “For You” homepage recommendation system for an established Dutch e-commerce platform. With less downtime in production, this recommendation system drove more customer transactions and boosted company revenue. Beyond my daily work, I’m increasingly interested in adapting foundation models for edge applications.\nI completed my master’s degree in Artificial Intelligence at the University of Amsterdam. During my bachelor’s, I participated in an edge computer vision internship in Spain.\n","date":1733097600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1733097600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m a ML engineer at Brenntag, where I deploy and maintain ML solutions across 70+ countries. My work focuses on applying software engineering principles to increase the stability and performance of ML systems.","tags":null,"title":"Bella Nicholson","type":"authors"},{"authors":null,"categories":null,"content":"Generative AI tooling has become a staple for everyday tasks — from creating presentation visuals to finding the right code syntax. But I\u0026rsquo;ve always felt a little too uneasy trusting cloud-hosted LLMs with my private chats.\nAs an ML engineer, I assumed that the only alternative was spinning up an expensive private LLM in the cloud — until the Google I/O Connect event (June 2024). That’s where Google revealed their smallest LLM to date, Gemini Nano, running directly on the Pixel 8. Seeing an ultra-private AI assistant on edge was inspiring, but for solo developers like me, the open-source tools weren’t quite ready yet.\nFast-forward six months, and the landscape has changed. Thanks to projects like the Machine Learning Compiler (MLC) framework, solo developers can now optimize and deploy powerful LLMs on edge devices. Rather than sticking to a unimodal LLM — a well-worn path — I set my sights higher: deploying a multimodal LLM on the smallest edge devices possible.\nThe end result? I successfully embedded the multilingiual Gemma 2B and the multi-modal LLaVA-OneVision Qwen2 0.5B models on my laptop (among other devices). Take a look for yourself:\nAs shown, my embedded model can comfortably discuss the content of an image and give some interesting synopses. Pretty impressive, right? Here’s a look at how it all works:\nAt 800M parameters, the LLaVA-OneVision Qwen2 0.5B is ideal for edge deployment but it struggles with complex user instructions. Thus, I used Google’s ultra-efficient Gemma 2B model as a fallback. For the full story, including technical details, check out my corresponding blog series. The source code is also available for your viewing pleasure on GitHub. Consider it your guide to shrinking impossibly large LLMs down to something edge device-sized.\n","date":1735603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735603200,"objectID":"6f11e7eba2fbfd8e9aa7e1869bca20ad","permalink":"https://bellanich.github.io/portfolio/edge-llm/","publishdate":"2024-12-31T00:00:00Z","relpermalink":"/portfolio/edge-llm/","section":"portfolio","summary":"A deep dive on embedding custom vision-text large language models across various iOS devices, from laptops to phones","tags":["Code"],"title":"Shrinking the Impossible: Deploying My Own Multi-Modal Edge Foundation Model","type":"portfolio"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction My Game Plan Deployment Attempt 1: Deploying my multi-modal model as an iOS app What if the MLC Chat Engine supported uploading images? Changing Directions Attempt 2: Deploying my multi-modal model as a Python CLI Conclusion Introduction If you\u0026rsquo;ve been following along in this blog post series, you know that I got a little too inspired at this year\u0026rsquo;s Google I/O Connect event. After hearing about Google\u0026rsquo;s incredible feat of embedding Gemini Nano into their Pixel 8 phones, I\u0026rsquo;ve been not-so-patiently waiting for the open source community to release tools that make similar feats possible for solo developers like me. Fortunately, the Machine Learning Compiler (MLC) project has matured enough that my dreams might just be possible.\nHence, I\u0026rsquo;ve been a personal quest to assess the limits of this new and exciting technology. To keep things interesting, I\u0026rsquo;ve decided to embed a multi-modal LLM. I want to test a less established setup, but I\u0026rsquo;m also secretly hoping to have a nice souvenir after everything is said and done. (Personally, I could really use a multi-lingual and multi-modal chatbot during my next holidays — especially once that doesn\u0026rsquo;t eat up my monthly data plan.)\nAt this point, I\u0026rsquo;m very close to this end goal. In my first blog post in this series, I introduced you to the MLC framework, which has been doing most of the heavy-lifting. I also gave you a quick crash course in what it takes to embed a large machine learning model on a resource-constrained device. In the 2nd blogpost, I gave you a primer on vision transformers — specifically, what we need to know to embed this tiny and multi-modal LLaVA-OneVision Qwen2 0.5B model. In my last blog post, I used this LLaVA model as an example to show you how to extend the MLC framework to support a custom model definition.\nAfter going through this entire process, I have a fully functional - but not very logical - LLaVA model running on my laptop. As the name suggests, this LLaVA model belongs to the high-performing, open source, and multi-modal Large Language and Vision Assistant (LLaVA) model family. Fortunately, for us, this model has less than 0.5B parameters. Meaning, it should be small enough to squeeze onto an iPhone. Here\u0026rsquo;s what its chat conversations look like:\nAt the start of a very simple discussion, my embedded LLaVA-OneVision model looks promising. Unfortunately, it doesn't take too long to see the cracks in LLaVA's logical reasoning — and to watch them crumble. Given this LLaVA model\u0026rsquo;s lack of coherence, we don\u0026rsquo;t want it talking directly to our end user. Rather, we\u0026rsquo;ll let it stick to what it does best (image annotation) and call in a slightly large LLM for backup.\nMy Game Plan So, what\u0026rsquo;s next? If you recall the diagram that I shared with you in the last time, I need to deploy a two-model ensemble. My plan is to use LLaVa-OneVision\u0026rsquo;s image annotation capabilities to expand Gemma 2B\u0026rsquo;s ridiculously good conversational capabilities. Essentially, LLaVA will describe the user provided image to Gemma in plain text. Gemma will receive the original user prompt and LLaVA\u0026rsquo;s image description. Between these two, Gemma should have enough information to return a lucid and logical response.\nMy multi-modal chat pipeline consists of two models: (1) the eloquent and multilingual Gemma2B model, and (2) a mini LLaVA-OneVision model. LLaVa will act as a translator for Gemma, generating a text descriptions for any provided images. Deployment My original vision was to have an edge multi-modal embedded onto my iPhone. That way, I could chat with my personal travel assistant anytime and anywhere — no internet connection required.\nAttempt 1: Deploying my multi-modal model as an iOS app Before I can package Gemma 2B and LLaVA together as a single model, I first need to figure out how to input images into MLC\u0026rsquo;s simple, pre-built chat iOS application, and this is where I run into my first roadblock.\nIf you\u0026rsquo;re observant, you might have noticed the grayed out text \u0026ldquo;Upload picture to chat\u0026rdquo; in my conversation screenshots with LLaVA. One could logically conclude that this means that MLC has some sort of user interface for uploading images. However, after pouring over the Swift code, I couldn\u0026rsquo;t find any such feature. Before I start cobbling something together in Swift, I decide to do a quick sanity check: Can I pass an image URL to the MLC Chat Engine?\nWell\u0026hellip;not really. I\u0026rsquo;d have to go in deep to the MLC\u0026rsquo;s low code and Swift implementations to make that possible. That\u0026rsquo;s a rabbit hole that I don\u0026rsquo;t want to fall into, so it\u0026rsquo;s time for me to pivot directions.\nWhat if the MLC Chat Engine supported uploading images? Let\u0026rsquo;s suppose that I was able to the the MLC Chat Engine iOS implementation so that it:\nIt now supports serving images to models. There\u0026rsquo;s a cool and sleek user interface for uploading said images. Could I deploy any of the Apple products in my household? Well, I decided to do an initial test on my housemate\u0026rsquo;s iPad Pro 11 inch (V1). I copied over and downloaded my iOS app. While I was able to open the app\u0026rsquo;s homepage and start my conversation with Gemma 2B, the app would crash before I could even ask Gemma a followup question.\nPerplexed, I checked my iOS app\u0026rsquo;s memory consumption in Xcode after I opened a new chat with Gemma. Well, it turns out that Gemma consumes a bit over 2GB of RAM. Given that this borrowed iPad only has 4GB of RAM in total, it\u0026rsquo;s not a surprise that the iOS application was crashing. Even when I closed all other applications, other background processes were consuming this iPad\u0026rsquo;s limited and valuable memory.\n(a) Gemma 2B (b) LLaVA-OneVision The memory footprints of my embedded Gemma 2B and LLaVA-OneVision models on my 8GB M2 MacBook Air. In contrast, the LLaVA-OneVision model has a much smaller memory footprint at 755MB. So, yes, it\u0026rsquo;s technically possible to deploy only the LLaVA-OneVision model on this given iPad; however, it\u0026rsquo;s probably not worth it. As we\u0026rsquo;ve seen from this blog post\u0026rsquo;s introduction, the embedded LLaVA-OneVision model isn\u0026rsquo;t capable of holding a coherent conversation for very long. Meaning, Gemma\u0026rsquo;s memory consumption is a definitive roadblock in my app ambitions.\nAs a final check, I tried the same exercise with my iPhone 13 and got similar results. This is expected, given that both devices have the same memory capacity. Of course, all the Apple devices I could find are on the older side. Apple\u0026rsquo;s newer iPhones and iPads have a large enough RAM that I should be able to deploy Gemma 2B and LLaVA-OneVision Qwen2 0.5B together.\nDevice Available RAM iPhone X 3GB iPhone 13 4G iPad Pro V1 4G iPhone 16 8G iPad Air 6 8G Table 1: The RAM available on selected Apple products. The top 3 rows describe devices that I have immediate access to, while the last two describe Apple's latest iPhone and iPad offerings (source). The available RAM on Apple\u0026rsquo;s latest iPhone and iPad models is a testament to its commitment to cram AI tools and features into every part of the Apple user experience. Of course, it\u0026rsquo;s also wild to think that the latest iPhone is just as computationally strong — at least in RAM — as my laptop. This also means that if I could feed images into the iOS MLC Chat Engine, then I should be able to very easily deploy my multi-modal model setup on an iPhone — just not my phone.\nThat being said, I\u0026rsquo;m not in a rush to get the latest iPhone. I just have something to look forward to when its inevitably time to replace my current one.\nChanging Directions At this point, I\u0026rsquo;ve ruled out the possibility of deploying my multi-modal foundation model on any Apple device, especially those within my immediate reach.\nNow, I\u0026rsquo;m trying to contend with two different mysteries:\nWhy does MLC natively support some LLaVA models, but doesn\u0026rsquo;t support serving images? The main added value of the LLaVA model family is their exceptional ability to handle multi-modal inputs. Does the MLC Engine support serving a model images in any other platform-specific chat engine implementation? Realistically, I don\u0026rsquo;t think that I\u0026rsquo;m going to get an answer to my first question unless I manage to chase down some of the MLC project\u0026rsquo;s main contributors. Fortunately, the second question is easier to answer. If you recall from my first blog post, MLC implements their own version of OpenAI\u0026rsquo;s Python API.\nSince the original Python API supports serving images, there\u0026rsquo;s a good chance that the MLC implementation might offer the same functionality.\nThe Machine Learning Compiler bases their Python API off of OpenAI's well-established implementation. As we can see in the official OpenAI Python API documentation, the OpenAI implementation does support serving a model images (source). Sure enough, I look through when I look through the mlc-llm-cpu library\u0026rsquo;s source code, I find the same image_url variable in their conversation protocol definition.\nclass Conversation(BaseModel): ... def as_prompt(self, config=None) -\u0026gt; List[Any]: ... for item in content: assert isinstance(item, dict), \u0026quot;Content should be a string or a list of dicts\u0026quot; assert \u0026quot;type\u0026quot; in item, \u0026quot;Content item should have a type field\u0026quot; if item[\u0026quot;type\u0026quot;] == \u0026quot;text\u0026quot;: message = self.role_templates[role].replace( MessagePlaceholders[role.upper()].value, item[\u0026quot;text\u0026quot;] ) message_list.append(message) # MLC supports passing image URLs via its Python API elif item[\u0026quot;type\u0026quot;] == \u0026quot;image_url\u0026quot;: assert config is not None, \u0026quot;Model config is required\u0026quot; image_url = _get_url_from_item(item) message_list.append(data.ImageData.from_url(image_url, config)) message_list.append(\u0026quot;\\n\u0026quot;) else: raise ValueError(f\u0026quot;Unsupported content type: {item['type']}\u0026quot;) message_list.append(separator) ... return prompt Meaning, I still can implement my multi-modal chat pipeline via a Python script.\nAttempt 2: Deploying my multi-modal model as a Python CLI At this point, I just want to see if I could successfully deploy the embedded multi-modal foundation model on some device that I own. So, I wrote a very simple Python script that lets a user interact with my chat pipeline via the commandline. If the user provides an image filepath, the model will check if it\u0026rsquo;s there. If so, underneath the hood, LLaVA is annotating this image for Gemma 2B. If not, my chatbot will kindly ask the user to check if the image is really there. Otherwise, Gemma 2B will be doing all the talking.\nHere\u0026rsquo;s the end result:\nMy embedded multi-modal model recognizes the cute, cartoon husky in the supplied photo. It's also capable of generating an interesting story synopsis for what could possibly become a new international bestseller. As you can see, the LLaVA model was able to recognize the Husky in the cartoon image and pass this information along to Gemma. Afterwards, Gemma was able to quickly help me draft an initial synopsis for about a day in the life of Barnaby, a sea-faring Husky who travels the world in his tiny yellow submarine. If we had a bit more compute power and a stronger LLaVA model, perhaps we could also extend this system to support image generation. If so, do think you \u0026ldquo;Barnaby\u0026rsquo;s Deep Sea Adventures\u0026rdquo; could become an international bestseller?\nFor the exact source code used in this demo, please refer to this blog post\u0026rsquo;s corresponding GitHub repository.\nConclusion It\u0026rsquo;s truly incredible seeing how quickly embedded systems technology has progressed in this domain. The tech giants are definitely in a comfortable place when it comes to deploying sophisticated edge foundation models, but the open source tooling is still in its early phase. Through this project, I\u0026rsquo;ve defined stumbled over a few vague, low-level code errors of my own. Tools like the MLC framework (in their current iteration) well-suited for those of us who are:\nWell-versed in the latest deep learning research Comfortable debugging low code system errors Reasonably proficient in Swift (or similar programming languages) Own latest edge devices Of course, that\u0026rsquo;s a lot of conditions. Nonetheless, it\u0026rsquo;s incredible seeing how projects like the Machine Learning Compiler have democratized access to foundation models. Whether you\u0026rsquo;re moonlighting as an entrepreneurial or are a curious university student, you can get easily started deploying your own single-modal LLMs on edge today. Maybe in another 6 months, the tooling will have advanced to the point that it fully supports multi-modal chat — including image generation.\nIn the meantime, I plan to watch these industry development closely. While I\u0026rsquo;ve put my hopes of a private, multi-modal travel assistant on the shelf, I haven\u0026rsquo;t abandoned them entirely. Stay tuned to see what I\u0026rsquo;ll do next. 👋\n","date":1733097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733097600,"objectID":"31a4586973855a3bc89f74305ae3ef5a","permalink":"https://bellanich.github.io/post/edge-llm-app/","publishdate":"2024-12-02T00:00:00Z","relpermalink":"/post/edge-llm-app/","section":"post","summary":"After painstakingly embedding a mini multi-modal LLaVA model, I'm ready to properly deploy it as an iOS app and enjoy the fruits of my labor. Let's see if we can truly shrink the impossible.","tags":["Generative AI","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction Selected Architecture Why Not Only Use the LLaVA Model? The Overall Process Manual Porting LLaVA-OneVision to MLC MLC\u0026rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation Embedding Aggregation Output Features Explained GELU Approximation Embedding Normalization Summary Packaging Our Custom Model End Result Conclusion What\u0026rsquo;s next? Introduction I attended this year\u0026rsquo;s Google I/O Connect in Berlin, and seeing Google’s latest work in Edge AI was inspiring. Since then, I\u0026rsquo;ve been on a personal mission to deploy my own edge model. Given how rapidly the open source community is catching up with the tech giants in edge foundation models, I\u0026rsquo;ve decided test the limits of what I can realistically achieve as a solo developer. Can I embed a multi-modal foundation model onto my iPhone?\nIn my first blog post, I\u0026rsquo;ve introduced the Machine Learning Compiler Project as the open source solution that will make this possible. In my last blog post, I\u0026rsquo;ve given you the technical background knowledge needed to successfully deploy a multi-modal foundation model.\nNow, we\u0026rsquo;re going to put this theory into practice in a hands-on activity. I\u0026rsquo;m going to show you how to embed a custom foundation model onto an edge device — and all the things that can go wrong in the process.\nSelected Architecture Remember that my end vision is to have a multi-modal foundation model deployed on my iPhone. Ideally, I want it to be multi-lingual so I can get some help the next time that I\u0026rsquo;m lost in a foreign country.\nAt a high-level, my implementation will consist of two different models: (a) the smallest Large Language and Vision Assistant (LLaVA) model that I can find, and (b) an instruction-tuned version of Gemma 2B.\nI\u0026rsquo;ve chosen these models, since (a) the MLC Engine natively supports them, (b) they\u0026rsquo;re lightweight enough to be compiled on my laptop, and (c) their respective families have earned a reputation as high-performers.\nEach model will work on a specific task. Whenever the end user shares an image with our multi-modal chatbot, the mini LLaVA model will generate a text description for Gemma. Gemma will then use this information to respond to the user\u0026rsquo;s original prompt. If no image is shared, our user will only interact with Gemma 2B. Of course, this model specialization will be abstracted away. Our user won\u0026rsquo;t be aware that they\u0026rsquo;re actually conversing with two different foundation models rather than just one.\nMy multi-modal chat pipeline consists of two models: (1) the eloquent and multilingual Gemma2B model, and (2) a mini LLaVA-OneVision model. LLaVa will act as a translator for Gemma, generating a text description for any user inputted images. Why Not Only Use the LLaVA Model? It may sound a bit strange that we\u0026rsquo;re deploying two models instead of one — especially since all LLaVA understand text and images. However, we need to make a distinction between the smallest LLaVa-OneVision model, which is less than 1B parameters in size, and its much larger 7B+ counterparts.\nAccording to the 0.5B LLaVA-OneVision model card, the LLaVa model is just under 900M parameters in total. Given its 80,000+ model downloads, we can assume it does a decent job at its primary task: annotating images (source). Larger LLaVA models are perfectly capable of holding a coherent conversation and to understanding whatever images we show them. You can find a few interesting examples of large LLaVA models answering questions about images here.\nThe LLaVA-v1.6 series contains models ranging from 7B to 34B parameters. At these sizes, a LLaVA model can easily identify Leonardo da Vinci's world famous masterpiece from a screenshot and give us a quick art history lesson. Unfortunately, we can't expect the same from the smallest LLaVA models. (image credit). However, we need to adjust our expectations for a 900M parameter model. There\u0026rsquo;s only so much that a small model can do — and the LLaVA-OneVision Qwen2 O.5B model has been optimized for image annotation rather than instruction tasks. Meaning, we can probably converse directly with it, but may not be thrilled with the quality of its responses.\nIf we wanted to deploy a similar application in a cloud environment, then it would probably just make more sense to quantize a 7B LLaVA model and accept a slightly larger monthly bill. However, since we\u0026rsquo;re working on edge, we have tight resource constraints and need to make do with what we have.\nThe Overall Process For each model, we need to:\nQuantize its weights; and Apply hardware-specific optimizations to it. How simple this process really is comes down to the degree of built-in MLC Engine support. MLC has already pre-quantized an instruction-tuned version of Gemma 2B for us. Hence, deploying Gemma as a stand alone model in an iOS application is relatively straightforward task. Just follow MLC\u0026rsquo;s Quick Start Documentation for how to package Gemma 2B and their iOS Swift SDK instructions.\nOn the other hand, applying the same process to our LLaVA model is a bit trickier. If we go through the list of pre-quantized models offered by MLC,(as of November 2024) there is no pre-quantized LLaVA model available — much less our desired mini LLaVA model.\nSince this process for deployed a pre-quantized model from HuggingFace so well-documented, I\u0026rsquo;m not going to focus on it. Rather, I\u0026rsquo;ll show you how I ported a new model into the MLC framework.\nManual Porting LLaVA-OneVision to MLC At this point, you may be a bit confused. I\u0026rsquo;ve stated the MLC Engine supports LLaVA models, but I\u0026rsquo;m also talking about manually porting the 0.5B LLaVA-OneVision model into the MLC Framework.\nWhat\u0026rsquo;s going on? At an initial glance, it looks like MLC fully supports the LLaVa model family, but that\u0026rsquo;s only partially true. At the time of writing (November 2024), MLC only natively supports specific LLaVA implementations, more specifically those that use (a) use Llama or Mistral model as its text decoder, and (b) use a CLIP-trained vision encoder. Unfortunately, all LLaVA variants that meet these requirements are at 7B+ parameters. Meaning, they\u0026rsquo;re too large for my laptop — muchless my smartphone — to handle.\nAs a result, I need to manually port the much smaller llava-onevision-qwen2-0.5b-ov-hf model definition into the MLC framework. Practically speaking, this means defining this model in the style used in the mlc-llm-cpu Python library, which is only available through MLC AI\u0026rsquo;s own Python code repository. Afterwards, we need to then recompile this library locally so that it contains our new model definition.\nOnce that\u0026rsquo;s done, I can quantize and hardware-optimize my selected LLaVA model just like any other MLC-supported model. As its full name suggests, the 0.5B LLaVA-OneVision model uses the Qwen2 0.5B LLM as its text decoder. Fortunately for us, MLC already supports Qwen2 0.5B implementation. Meaning, this change is quite easy. We just copy and paste MLC\u0026rsquo;s definition of LLaVA, rename the files, and change a few key-value pairs:\nThe original MLC LLaVA model definition looks like this:\nfrom ..llama.llama_model import LlamaConfig, LlamaForCausalLM from ..mistral.mistral_model import MistralConfig, MistralForCasualLM CONFIG_MAP = { \u0026quot;LlamaForCausalLM\u0026quot;: LlamaConfig, \u0026quot;MistralForCausalLM\u0026quot;: MistralConfig, } ARCHITECTURE_MAP = { \u0026quot;LlamaForCausalLM\u0026quot;: LlamaForCausalLM, \u0026quot;MistralForCausalLM\u0026quot;: MistralForCasualLM, } Now, we just rewrite our LLaVA-OneVision model definition to support the LLaVA-OneVision Qwen2 0.5B model\u0026rsquo;s architecture definitions:\n# Defined by MLC from ..qwen2.qwen2_model import QWen2Config, QWen2LMHeadModel CONFIG_MAP = { \u0026quot;QWen2LMHeadModel\u0026quot;: QWen2Config, \u0026quot;Qwen2ForCausalLM\u0026quot;: QWen2Config } ARCHITECTURE_MAP = { \u0026quot;QWen2LMHeadModel\u0026quot;: QWen2LMHeadModel, \u0026quot;Qwen2ForCausalLM\u0026quot;: QWen2LMHeadModel, } So far, so good. Let\u0026rsquo;s take a closer look at the the 0.5B LLaVA-OneVision config.json file. We see that this LLaVA model\u0026rsquo;s vision encoder was trained using SigLIP — rather than MLC\u0026rsquo;s natively supported CLIP training framework.\nAs seen in the LLaVA-OnVision Qwen2 0.5B model's configuration file, the vision encoder was trained using SigLIP. As of now (November 2024), MLC's only natively supports CLIP-trained vision encoder (source). Meaning, there\u0026rsquo;s no MLC definition for us to just import. We need to write some custom Python model definition using MLC wrappers. Luckily, we already dived into the details of the SigLIP vision encoder in the previous blog post. So, we\u0026rsquo;re ready to get started.\nI\u0026rsquo;ve included the final SigLIP vision encoder definition on in this blogpost series\u0026rsquo;s corresponding GitHub repository. For the sake of brevity, I\u0026rsquo;m just going to focus on the technical differences between these two vision encoders and how these changes translate into code.\nMLC\u0026rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation Once again, we\u0026rsquo;re going to reference our LLaVA model\u0026rsquo;s trusty config.json file to get some clues about where to start. In particular, we see the key-value pair: \u0026quot;vision_feature_layer\u0026quot;: -1, whereas the original LLaVA config uses \u0026quot;vision_feature_layer\u0026quot;: -2. This is a hint about how a vision encoder aggregates its sequence of embeddings into a single vector.\nEmbedding Aggregation Both LLaVA models use a Vision Transformer, which outputs a sequence of embeddings. For the training of CLIP and SigLIP, we need a single vector. In this specific Huggingface implementation, CLIP and SigLIP do this aggregation in different ways.\nCLIP uses a class embedding, similar to common adaptations of the Vision Transformer for classification. Here, we add another token to each image sequence, which is fed through the Transformer along with the image tokens. In the end, we pick the aggregated image feature vector as the output of this classification token. By having the class token part of the self-attention layers, it gives the transformer the ability to aggregate information of the image in this token across its layers. In the implementation, we see this class embedding token being added to the image token sequence:\nclass CLIPVisionEmbeddings(Module): def __init__(self, config: CLIPVisionConfig): super().__init__() # Class Embedding Token, added to each image token sequence. self.class_embedding = nn.Parameter((self.embed_dim,)) ... def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: patch_embeds = self.patch_embedding(pixel_values) ... class_embeds = broadcast_to( self.class_embedding, shape=(batch_size, 1, self.embed_dim) ) # Add class embedding token to image token sequence. embeddings = concat([class_embeds, patch_embeds], dim=1) ... return embeddings In contrast, SigLIP pools its output features. Hence, the sequence of image tokens remains unchanged in the input and is fed through the layers. We add on top a pooling layer over the sequence dimension to aggregate all the feature information. This can either be done by a simple averaging, or, in case our specific case, with a multi-head attention pooling. This is similar to our self-attention layers, but just with a fixed query.\nNote. We\u0026rsquo;ve removed the code parts that are common to both models. This simplifies the code and allows us to highlight key differences.\nIn our SigLIP implementation, shown below, we see as a difference to CLIP that there is no class embedding.\nclass SiglipVisionEmbeddings(Module): def __init__(self, config: SiglipVisionConfig): super().__init__() ... def forward(self, pixel_values: Tensor, interpolate_pos_encoding: bool = False) -\u0026gt; Tensor: patch_embeds = self.patch_embedding(pixel_values) embeddings = patch_embeds ... return embeddings Output Features Explained In the previous blog post, we discuss how we need a whole sequence of image embeddings for LLaVA rather than a single image feature vector. This provides more detailed information to the decoder.\nWhile we do not make use of the output heads of CLIP and SigLIP respectively, it does affect which layer we select our features from. This is what the config argument vision_feature_layer ($-1$ for SigLIP and $-2$ for CLIP).\nIn other words, we choose the last layer in SigLIP, since the model was trained with image embeddings that are literally the weighted average of all image sequence tokens. Thus, the training process ensures that all these image embeddings have valuable information in them.\nAttention pooling represents a weighted average pooling, where the weights are determined by the normalized dot product between a static query and the keys per token. While this example shows the averaging for text tokens, it has the same idea with image patch tokens in our vision encoder (image credit). class SiglipVisionModel(Module): def __init__(self, config: SiglipVisionConfig): super().__init__() self.vision_model = SiglipVisionTransformer(config) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: # Siglip Qwen2 is using last layer, CLIP pre-last due to different # Transformer encoder. return self.vision_model(pixel_values)[-1] For CLIP, choosing the last layer is suboptimal, because of the usage of a class token. In the CLIP loss, only the output features of the class token are used. Thus, the output features of all other tokens, namely our image embeddings, were not used. In other words, these features did not receive any gradients during training, and we cannot be sure that the model has stored useful information in them. Most likely, the model has specialized the last layer specifically for the class embedding token, making the outputs of the other tokens (possibly) meaningless.\nHence, we need to go back one more layer (i.e, the pre-last layer), because these tokens did receive gradients during the training by their dependency on the class token in the last self-attention layer. This ensures that these embeddings have strong features and makes them usable in our LLaVA model implementation.\nclass CLIPVisionModel(Module): def __init__(self, config: CLIPVisionConfig): super().__init__() self.vision_model = CLIPVisionTransformer(config) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: return self.vision_model(pixel_values)[-2] GELU Approximation The Gaussian Error Linear Unit (GELU) is a very popular activation function for Transformers, and is used in both of our CLIP and SigLIP implementations. However, there are some specific details in about how we can implement the GELU activation.\nThe \u0026ldquo;true\u0026rdquo;, precise implementation of GELU involves the cumulative distribution function (CDF) of the Gaussian distribution $\\Phi(x)$:\n$\\text{gelu}(x)=x\\cdot\\Phi(x)$\nThis CDF is, however, expensive to implement and in particular for edge-devices, where every inference optimization counts, it\u0026rsquo;s sub-optimal. Instead, people commonly use GeLU approximations that are good enough. The standard approximation, often used during training and in frameworks like JAX and PyTorch, is the tanh-approximation:\n$\\text{gelu}(x)\\approx 0.5x\\left(1+\\tanh\\left[\\sqrt{\\frac{2}{\\pi}}(x+0.044715\\cdot x^3)\\right]\\right)$\nThis is also being used in the Huggingface implementation for SigLIP, and we port it over as shown below:\nclass QuickGELU(Module): # SigLIP implementation def forward(self, input_tensor: Tensor) -\u0026gt; Tensor: c = (2 / math.pi)**0.5 return 0.5 * input_tensor * ( 1 + tanh(c * (input_tensor + 0.044715 * input_tensor**3)) ) In the MLC implementation of CLIP, another approximation is used. This one involved the sigmoid function, and is simply:\n$\\text{gelu}(x)\\approx x\\cdot \\sigma(1.702x)$\nCLIP\nclass QuickGELU(Module): def forward(self, input_tensor: Tensor) -\u0026gt; Tensor: return input_tensor * sigmoid(input_tensor * 1.702) While the Sigmoid GeLU approximation is simpler and even cheaper to calculate, it is also less accurate. Thus, we have to make a tradeoff between efficiency and accuracy. Since we our selected SigLIP vision encoder was trained using the tanh-approximation, we\u0026rsquo;ll stick with it. Differences between the GeLU function implementation during training and inference time can cause a slight but noticeable drop in performance.\nA visualization of the different implementations of the GELU activation function. The original GeLU function is in red, the tanh approximation is in purple, and the sigmoid approximation is in green. As you can see, all of them are quite similar. For the sigmoid activation, there is a noticeable difference for the negative range -1.5 and -4. However, for the tanh approximation, we need to zoom in closely to see the difference, showcasing why the tanh approximation is often used as a close match. Embedding Normalization Another minor design choice is whether we normalize the embedding features before feeding them into the main Transformer model. Both models use the pre-activation Transformer implementation, which applies a LayerNorm before each Self-Attention and MLP layer. However, we can also apply a LayerNorm on the embeddings themselves, or leave the model to learn the scaling of the residual part.\nIn the CLIP implementation, we find a LayerNorm applied to the embeddings before feeding it through the layers.\nclass CLIPVisionTransformer(Module): def __init__(self, config: CLIPVisionConfig): super().__init__() embed_dim = config.hidden_size self.embeddings = CLIPVisionEmbeddings(config) self.pre_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) self.encoder = CLIPEncoder(config) self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: hidden_states = self.embeddings(pixel_values) hidden_states = self.pre_layernorm(hidden_states) encoder_outputs = self.encoder(inputs_embeds=hidden_states) return encoder_outputs In contrast, in the SigLIP implementation, this normalization is missing. However, it is not expected to cause a major performance difference.\nclass SiglipVisionTransformer(Module): def __init__(self, config: SiglipVisionConfig): super().__init__() embed_dim = config.hidden_size self.embeddings = SiglipVisionEmbeddings(config) self.encoder = SiglipEncoder(config) # Defined but not actually used. self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps) def forward(self, pixel_values: Tensor) -\u0026gt; Tensor: hidden_states = self.embeddings(pixel_values) encoder_outputs = self.encoder(inputs_embeds=hidden_states) return encoder_outputs Summary Overall, our SigLIP implementation has some small, but crucial differences compared to MLC\u0026rsquo;S CLIP implementation. The table below summarizes the differences that we needed to account for.\nMLC LLaVA Model - CLIP 0.5B LLaVA-OneVision Qwen2 0.5B Model - SigLIP Output Feature Aggregation Class Token Attention Pooling Feature Layer Pre-Last Layer Last Layer Embedding Normalization Yes No GELU Implementation Sigmoid Approximation Tanh Approximation Now that we\u0026rsquo;ve implemented SigLIP in the MLC framework, it is straight forward to integrate the LLaVA-OneVision models into the MLC framework. We can now proceed with quantizing and optimizing our model for deployment on an edge device.\nPackaging Our Custom Model Once we\u0026rsquo;ve extended the mlc-llm-cpu library to include our custom model definition, then we can proceed as normally.\nFirst, we want to quantize it our newly ported LLaVA model. To do so, we run the following commands in our MLC LLM project\u0026rsquo;s repo directory:\n# Create directory mkdir -p dist/models \u0026amp;\u0026amp; cd dist/models # Clone Original LLaVA model's weights from HuggingFace git lfs install git clone https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf # Apply the `q4f16_1` quantization method to our model mlc_llm convert_weight ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \\ --quantization q4f16_1 \\ -o dist/llava-onevision-qwen2-0.5b-ov-hf --model-type llava_onevision Fortunately, the command ran successfully.\nAfter defining my target LLaVA-OneVision model as a new MLC `model-type`, I was able to easily quantize this model. Next, we need to apply some hardware-specific optimizations to our model.\n# Generate a MLC config file for our quantized model mkdir dist/libs mlc_llm gen_config ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \\ --quantization q4f16_1 \\ --conv-template redpajama_chat \\ --context-window-size 768 \\ -o dist/llava-onevision-qwen2-0.5b-ov-hf # Optimize LLaVA OneVision for an iOS app implementation mlc_llm compile ./dist/llava-onevision-qwen2-0.5b-ov-hf/mlc-chat-config.json \\ --device iphone \\ -o dist/libs/llava-onevision-qwen2-0.5b-ov-hf-iphone.tar Finally, we need to package the quantized and optimized model for my iOS App. To make my life easier, I\u0026rsquo;ve uploaded the pre-quantized LLaVA-OneVision Qwen2 0.5B model to my personal HuggingFace account.\nMy mlc-package-config.json file located in the ios/MLCChat/ subdirectory (following MLC LLM\u0026rsquo;s project structure) now looks like this:\n{ \u0026quot;device\u0026quot;: \u0026quot;iphone\u0026quot;, \u0026quot;model_list\u0026quot;: [ { \u0026quot;model\u0026quot;: \u0026quot;HF://bella-nich/llava-onevision-qwen2-0.5b-ov-q4f16_1-mlc\u0026quot;, \u0026quot;model_id\u0026quot;: \u0026quot;llava-onevision-qwen2-0.5b-ov-hf\u0026quot;, \u0026quot;estimated_vram_bytes\u0026quot;: 1000000000, \u0026quot;overrides\u0026quot;: { \u0026quot;prefill_chunk_size\u0026quot;: 128 } }, ] } So, I\u0026rsquo;m going to package my quantized and optimized LLaVA-OneVision model into a proper iOS app.\n# Words cd /path/to/MLCChat # e.g., \u0026quot;ios/MLCChat\u0026quot; export MLC_LLM_SOURCE_DIR=/path/to/mlc-llm # e.g., \u0026quot;../..\u0026quot; mlc_llm package I was able to successfully package my newly ported LLaVA-OneVision Qwen2 0.5B model without any errors. Meaning, there's a chance that everything was quantized and compiled correctly. End Result While the lack of compilation errors is promising, the only way to validate this entire process is talk to the embedded LLaVA model. So, I built and deployed my packaged iOS application.\nOnce I do, I\u0026rsquo;m greeted with a few strange but mostly understandable sentences.\nMy embedded LLaVA-OneVision Qwen2 0.5B model is able to form mostly coherent sentences. However, its sense of humor doesn't seem fully developed. In other words, LLaVA isn\u0026rsquo;t pure spouting gibberish. I take this as a sign that the quantization and model compilation processes have gone well. Of course, as the longer the conservation goes on, the less coherent LLaVA becomes. Pretty soon LLaVA is giving me random responses strung together.\nAfter my dissatisfaction with the embedded model's sense of humor, I decided to see if LLaVA can tell me a fun fact. To my surprise, I'm greeted with a sudden and odd request. The chat snippets of \u0026lt;im_start\u0026gt;, \u0026lt;im_end\u0026gt;, and assistant give us clues about how this LLaVA model was tuned for image annotation tasks. More specifically, this tells us about the structure of LLaVA-OneVision Qwen2 0.5 B\u0026rsquo;s chat template. A chat template restructures our current conversation, which is a list of string, into a single, tokenizable format that the model expects. Here, we can see that the chat template assistant role is prompting LLaVA to continue but in ways that are completely disconnected from my original text-only prompts.\nThe good news is that chat template assistant role should be more useful when we provide LLaVA image inputs. However, this LLaVA model\u0026rsquo;s current performance (in a task that it wasn\u0026rsquo;t fine-tuned for) highlights the differences between \u0026gt;1B and a 7B+ parameter models. Larger foundation models are simply more versatile than smaller ones.\nConclusion In this blogpost, I\u0026rsquo;ve shown you everything that it takes to embed an unsupported model onto an edge device using the Machine Learning Engine Compiler framework. As you can see, the devil is in the details. You need to be well-versed in the different permutations of a given neural architecture\u0026rsquo;s implementation and able to spot those differences in the wild.\nOf course, the only thing that\u0026rsquo;s more important than getting this process right is to choose the correct model to embed. The smaller we go in size, the more portable our foundation model becomes, but that portability comes at the cost of performance.\nWhat\u0026rsquo;s next? While embedding a custom model is an exciting milestone, we\u0026rsquo;re not done yet. As you can see, the embedded LLaVA model works but it doesn\u0026rsquo;t make for a scintillating conversation partner. Hence, we need to get Gemma 2B and the LLaVA-OneVision Qwen2 0.5B model to work together — which is exactly what I do in my next (and final) blogpost in this series. Stay tuned!\n","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733011200,"objectID":"08d81f1d9e09c89e604785aba3c5690c","permalink":"https://bellanich.github.io/post/edge-llm-embed-llava/","publishdate":"2024-12-01T00:00:00Z","relpermalink":"/post/edge-llm-embed-llava/","section":"post","summary":"Armed with some newfound vision transformer knowledge, we're ready to extend the Machine Learning Compiler framework to support a new, tiny but promising multi-modal model.","tags":["Generative AI","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":"Table of Contents Table of Contents Introduction LLaVA: Chatbots That Can \u0026ldquo;See\u0026rdquo; Visual Instruction Tuning of Text Decoders Vision Encoders Training the Vision Encoder CLIP SigLIP Inference Machine Learning Compiler Implementation The LLaVA Model Family on MLC LLaVA OneVision Conclusion What\u0026rsquo;s next? Introduction In my last blog post, I introduced you to the fascinating world of edge foundation models. I was dreaming big, imagining a foundation model that could \u0026ldquo;see\u0026rdquo; — well, at least understand the photos I share with it. Let\u0026rsquo;s be honest, I’ll probably need some help when I’m lost in a new city on my next vacation. A model that understands both text and images? Way more useful when I\u0026rsquo;m wandering around than a single-modal chatbot!\nRecently, things have been getting pretty exciting in the world of multi-modal models. Beyond just text and images, ChatGPT can now \u0026ldquo;see, hear, and speak\u0026rdquo; — processing text, images, and audio. There\u0026rsquo;s also Gemini, Google’s latest powerhouse, which can process everything from text to images to audio — and even long movies, thanks to its multi-million token context window. Sounds pretty impressive, right? But here\u0026rsquo;s the catch: these models are so large and computationally demanding that it’s nearly impossible to run them on edge devices (like phones and laptops).\nIn this blog post, we’ll explore some of the latest advancements in small, efficient multi-modal models that can actually be deployed on edge devices. We\u0026rsquo;ll introduce the LLaVA model family, which combines vision encoders and text decoders to provide a general-purpose multi-modal chatbot.\nThe LLaVA model family stands out as a high-performance, open-source collection of models. Upon its release, the LLaVA-1.5 series achieved state-of-the-art (SoTA) performance across 11 benchmarks (image credit). We’ll also take a closer look at the architecture behind LLaVA—specifically the vision encoders and text decoders that make it work. This includes the popular CLIP and SigLIP training frameworks.\nUnderstanding how these models work lays the groundwork for my next blog post.\nLLaVA: Chatbots That Can \u0026ldquo;See\u0026rdquo; The LLaVA model family is a collection of vision-language models that use pre-trained Vision Encoders to give pre-trained Large Language Models (LLMs) the ability to understand images. Why all the pre-training? Well, pre-trained models help keep training costs low while still leveraging the latest in foundation model technology.\nThis combination of pre-trained models has made LLaVA models increasingly popular for multi-modal tasks. These models — some named more creatively than others — are open-sourced in various sizes, ranging from 0.5B to 13B parameters.\nMost LLaVA models are named after their base transformer architectures, but this one has been named after a beloved Turkish delicacy (source). Don’t forget that the Machine Learning Compiler (MLC) Engine (introduced in my last blog post) supports quantizing LLaVA models. In other words, the smallest LLaVA models are promising candidates for my edge multi-modal ambitions.\nVisual Instruction Tuning of Text Decoders Before we jump into how LLaVA works its magic, let’s take a quick look at how plain text-based LLMs operate. A typical Large Language Model (LLM) begins by breaking your input text into discrete units called tokens using a tokenizer. These tokens are then transformed into high-dimensional embeddings — essentially numerical representations the model can \u0026ldquo;understand\u0026rdquo;. The embeddings pass through multiple layers of self-attention mechanisms and feed-forward neural networks, which process the information and predict the next token in the sequence. This process continues iteratively until the model generates the desired output text.\nBut here’s the challenge: when it comes to multi-modal tasks, like combining images and text, your traditional text-based LLM hits a wall — it simply can’t “see”. To fix this, we bring in vision encoders. Vision encoders translate images into embeddings, a deep learning model\u0026rsquo;s \u0026ldquo;native language\u0026rdquo;. Afterwards, a text decoder translates these embeddings into an output text based on both the image and the text input. We align the feature space of the vision encoder with the LLM by adding a trainable linear projection layer on top of the vision embeddings. By fine-tuning the text decoder on a multi-modal dataset, the model learns to generate text that is relevant to the input image.\nThe LLaVA model family combines a vision encoder with a text decoder to generate human-like text based on images. The vision encoder converts images into embeddings, which are then fed into the text decoder as a visual instruction to generate the output text (source). This approach is called Visual Instruction Tuning. If you’re familiar with Instruction Tuning, it’s the same idea with a multi-modal twist. In regular instruction tuning, you give the model a text instruction (“Summarize this paragraph”) and train it to produce the desired text output. Visual Instruction Tuning swaps that text instruction for an image. The goal? Train the model to generate text that describes or explains the image.\nFor example, LLaVA models are trained on datasets that pair images with captions or multi-modal Q\u0026amp;A examples. This forces them to become fluent in both visual and linguistic cues. In these finetuning setups, we commonly keep the vision encoder fixed and only fine-tune the text decoder and projection layer on the multi-modal dataset. his way, we can leverage the pre-trained vision encoder to understand images without the need for additional training data, while also benefiting from the power of the pre-trained LLM to generate human-like text.\nVision Encoders Essentially, Visual Instruction Tuning swaps out text instructions for images and teach the model to generate text based on what it “sees.” But there’s a big question here: how do we get an image - essentially a 2D grid of pixels — to become compatible with a token-consuming text-based model? While Convolutional Neural Networks (CNNs) have traditionally excelled at extracting features from images, Vision Transformers have shown promising results in processing images as sequences of tokens, similar to text.\nHere’s how it works: we divide up an image into smaller, fixed-size patches rather processing it all at once. Each patch is then flattened and mapped into a high-dimensional feature space — a numerical representation that captures the patch’s visual characteristics. To make sure the model doesn’t lose track of where these patches belong in the image, we add positional encodings. A Vision Transformer then consumes these location-annotated image patches, processing them through multiple layers of self-attention and feed-forward neural networks.\nThis process allows the model to understand the relationships between different parts of the image and distill its content into a sequence of semantically meaningful embeddings. This sequence of embeddings is particularly compatible with classical LLMs because it mirrors the token-like structure LLMs expect for text. Thus, the combination of Vision Transformers with Text Decoders gives us high-functioning multi-modal models.\nIf you want to learn about Vision Transformers in more detail, I recommend taking a closer look at the University of Amsterdam\u0026rsquo;s Deep Learning Tutorials. For now, let\u0026rsquo;s focus on the training frameworks for vision encoders, as they are crucial for the performance of the final LLaVA model.\nTraining the Vision Encoder The quality of the final multi-modal LLaVA model heavily depends on the quality of its vision encoder. This is why it is crucial to train the vision encoder on a diverse and large-scale dataset to ensure that it can understand a wide range of images. Two popular training frameworks for vision encoders are CLIP and SigLIP. We describe both of them in a bit more detail here, since we need this knowledge to properly imbed a LLaVA model onto an edge device.\nCLIP Contrastive Language-Image Pre-training (CLIP) is a pre-training framework that teaches the vision encoder to understand images by associating them with text descriptions. Here’s the gist: CLIP trains the vision encoder to predict what the image means using a text description, and vice versa, to predict the image from a text description. By doing this, the model essentially learns a shared “language” that lets it understand both images and text.\nCLIP has been shown to achieve state-of-the-art performance on a wide range of vision tasks, making it a popular choice for vision encoders in multi-modal models, in particular due to its alignment of the vision and text feature spaces.\nCLIP trains vision encoders to predict image representations that align with text representations of their respective caption. By doing so contrastively over many image-text pairs in a batch, the vision encoder learns strong, semantic image embeddings (source). A key aspect in CLIP is its use of contrastive learning, where the model is trained to maximize the similarity between positive pairs (image-text pairs that belong together) and minimize the similarity between negative pairs (image-text pairs that do not belong together).\nExample. The vision encoders learns to match positive pairs (like a picture of a dog with a caption saying \u0026ldquo;a dog\u0026rdquo;) and push apart negative pairs (like a picture of a dog with a caption saying \u0026ldquo;a cat\u0026rdquo;).\nThe similarity is measured by a softmax, which is applied over the dot products between the representation spaces. This allows the model to learn a discriminative representation space that captures the semantic content of images and text.\nWhile this approach is intuitive, it doesn\u0026rsquo;t scale well. For large representation spaces, we need large batch sizes to effectively capture a large variety of (negative) image-text pairs and improve the training process. This raises challenges in CLIP, in particular with the softmax. To calculate the CLIP loss, we need to apply the batch-level softmax twice to normalize the pairwise similarity scores across all images for training the text encoder, and across all texts for training the image encoder. These passes over the full batch size can be computationally expensive, especially when the batch size is sharded across multiple devices.\nIn a distribution training setup, we are often using data parallelism, where each device processes a part of the batch. While commonly, each device can do the forward and backward pass independently and only the final gradients are communicated, CLIP needs to already communicate the softmax statistics for the loss between all devices. This creates an efficiency bottleneck, especially when we scale to many devices. This bottleneck prevents CLIP from being scaled efficiently, and it\u0026rsquo;s the exact problem SigLIP solves.\nSigLIP The Sigmoid Loss for Language Image Pre-Training (SigLIP) replaces the softmax in CLIP with a sigmoid loss, which is applied element-wise to the dot products between the image and text representations. The loss objective is then closer to standard predictive learning with binary cross entropy, training the positive pairs to be $1$ while negative ones are pushed closer to $0$. This allows the model to train on a large batch size without the need for full-batch softmax computation, making it more efficient and scalable.\nSigLIP has been shown to achieve similar or even better performance to CLIP, in particular for smaller datasets of image-caption pairs. Furthermore, SigLIP is more computationally efficient at scale, making it a popular choice for training strong vision encoders in multi-modal models.\nInference Once the vision encoder is trained, we can use it to generate embeddings for images. However, both CLIP and SigLIP train single-vector representations, meaning that the image is represented by a single vector. This is not ideal for multi-modal models, as we want to generate a sequence of embeddings that can be fed into the text decoder. Furthermore, a single vector representation may not capture the full content of the image.\nTo address this, we can use the internal representations of the Vision Encoder, which is commonly a Vision Transformer, to extract a sequence of embeddings for the image. This allows us to capture a more detailed representation of the image, which can be used by the text decoder to generate more accurate and detailed text descriptions. This is a key aspect of the LLaVA model family, which combines the power of Vision Transformers with Large Language Models to generate human-like text based on images.\nIn the next blogpost, we will see that there are actually several variations on how a Vision Transformer can be implemented in the CLIP and SigLIP frameworks (e.g. using a separate class embedding, pooling, etc.). These variations can have a significant impact on the performance of the final LLaVA model and require adjusting the model architecture accordingly. Hence, it is important to carefully consider them when implementing your own LLaVA model.\nMachine Learning Compiler Implementation Now that we are familiar with the LLaVA model family and the vision encoders used in these models, let\u0026rsquo;s discuss how we can deploy these models on edge devices. As we have introduced in Part 1 of this blog post series, we use the Machine Learning Compiler (MLC) project for on-edge deployment. MLC aims to optimize and compile deep learning models for edge devices, enabling efficient and fast inference on resource-constrained hardware. MLC supports a wide range of deep learning models, including LLaVA models, making it a powerful tool for deploying multi-modal models on edge devices.\nThe LLaVA Model Family on MLC Out of the box, the MLC LLM supports the quantization of the LLaVA family of vision encoders and text decoders. For this, the LLaVA implementation of the Hugging Face Transformers library has been integrated into the MLC framework using the TVM stack. At the time of writing (November 2024), the default supported text decoders are Llama and Mistral, with a CLIP-trained vision encoder. However, the sizes of these models are often around 7B parameters and larger, making them unsuitable for deployment on small edge devices (and even my M2 MacBook Air). This is why we need to consider smaller models, which are more suitable for our travel recommendation chatbot.\nLLaVA OneVision One of the smallest LLaVA models is the LLaVA OneVision Qwen2 0.5B, which, as the name suggests, uses the Qwen2 language model with 0.5B parameters.\nLLaVA-OneVision models tackle even the trickiest secondary school math problems with ease. In this example, the model demonstrates its ability to interpret multiple images and coherently apply deductive reasoning (image credit). The LLaVA OneVision Qwen2 0.5B model is particularly suitable for deployment on edge devices, as it is small and lightweight. While we can\u0026rsquo;t expect the same performance that larger models offer, this small LLaVA OneVision model is a good starting point for our travel recommendation chatbot and allows a fast iteration cycle for model development.\nIn contrast to the original LLaVA models, the LLaVA OneVision model uses a SigLIP-pretrained vision encoder, as it has demonstrated higher multi-modal LLM performance among open vision encoders. While we mentioned that in theory, there are no differences between SigLIP and CLIP during inference, the encoders slighlty differ in their Huggingface Transformers implementation. This is why we need to first port the LLaVA OneVision model and its SigLIP vision encoder to the MLC framework. Once again, I\u0026rsquo;ll walk you through how to do in the next blog post.\nConclusion Multi-modal foundation models allow us to apply state-of-the-art models to new and more complex applications. We\u0026rsquo;ve introduced how we can use a pre-trained vision encoder and text decoder architecture to build a general-purpose vision-language chatbot. The Machine Learning Compiler Project currently supports embedding the LLaVA vision-to-text models on edge devices. However, due to our restricted local device, we will use the smallest LLaVA OneVision model with only 0.5B, which needs to be manually ported to the MLC framework. Thus, we\u0026rsquo;ve taken some time to understand the intricacies between LlaVA model implementations and what this means during model training and testing time.\nWhat\u0026rsquo;s next? This blog post provides you with the background knowledge needed to deploy a multi-modal vision-to-text model on edge devices. In the next blog post of this series, I\u0026rsquo;ll walk you through how to put this knowledge into practice.\n","date":1732924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732924800,"objectID":"72faca95df140ba7cf96c59c279dafbb","permalink":"https://bellanich.github.io/post/edge-llm-vision-encoders/","publishdate":"2024-11-30T00:00:00Z","relpermalink":"/post/edge-llm-vision-encoders/","section":"post","summary":"Vision transformers, with help from training frameworks like CLIP and SigLIP, make  multi-modal foundation models like LLaVA possible — bridging the gap between vision and text.","tags":["Generative AI","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":" Table of Contents Table of Contents Introduction Project Inspiration Why Edge Foundation Models Matter Challenges of Running Foundation Models on Edge Devices My Gemma Debacle The Memory Struggle Is Real MLC LLM: A Quantum Leap in Deploying Edge Foundation Models How does MLC LLM work? Quantization What is Quantization? Quantization Methods 1. Post-Training Quantization (PTQ) 2. Quantization-Aware Training Quantizing Transformers Out of the Box MLC LLM Solutions Custom Solutions Hardware Optimizations Just-in-Time Model Compilation MLC LLM Implementation Conclusion What\u0026rsquo;s next? Introduction Ever since ChatGPT went mainstream, I’ve been captivated by the rapid advancements in large language models (LLMs). As a machine learning engineer, I’ve been eagerly awaiting my chance to experiment with these groundbreaking models. Yet, the reality of deploying and managing the required infrastructure — and its massive cost — always made me pause.\nProject Inspiration This June at the Google I/O Connect Event in Berlin, I realized my vision of working with server-free LLMs wasn’t as far-fetched as I’d thought. Google showcased Geminin nano, a powerful LLM integrated directly into their latest Android devices. While it wasn’t accessible to developers yet, it was a glimpse of what might soon be possible.\nGoogle originally launched Gemini Nano on the Pixel 8 Pro in December 2023. This edge LLM has been gradually rolled out to other popular Android phones, including Samsung's Galaxy S24 series (source, image credit). Inspired by this progress, I set out to test the limits of edge LLMs. Could I, as a solo enthusiast, deploy a LLM on an edge device like my iPhone? To make the challenge even more intriguing, I decided to aim for a multi-modal LLM. After all, who wouldn’t want a private chatbot that understands your phone’s photo gallery and keeps your secrets safe?\nIn this 4-part blog series, I document my experiment to prove that you don’t need a sprawling server farm or a high-end workstation to dive into the latest AI technology. With a bit of machine learning knowledge, solid documentation, and plenty of determination, you can get started with state-of-the-art models without a painful cloud bill (looking at you, AWS).\nWhy Edge Foundation Models Matter Foundation models have traditionally been too massive to run on edge devices like smartphones, IoT gadgets, or embedded systems. As a result, they’re typically hosted on centralized servers, which introduces several challenges:\nCost barriers. Deploying and serving large-scale models (think 10B+ parameters) in the cloud is prohibitively expensive, often costing millions in infrastructure and energy. This creates a significant barrier for students, hobbyists (like me), and smaller organizations looking to experiment with AI. By running models locally on edge devices, the need for expensive server infrastructure disappears, democratizing access to this cutting-edge technology.\nUnreliable performance. Cloud-based inference depends on a steady internet connection to send data to servers and retrieve results. This back-and-forth can cause frustrating delays, especially in areas with poor connectivity. Edge models, which run directly on local devices, bypass these issues. They deliver faster responses and work well even without an internet connection.\nSecurity concerns. Cloud-based systems inherently require sending data to remote servers, which comes with inherent risks. For users, their personal chat data could be exposed in a security breach or misused without consent. Businesses, meanwhile, must navigate strict regulations like GDPR or HIPAA when transferring sensitive data off-device. By processing data locally, edge models eliminate these risks, ensuring that your personal information stays private.\nIn short, edge foundation models break down cost barriers, improve reliability, and address privacy concerns. They make AI more accessible for curious minds and businesses alike while offering end users more peace of mind.\nChallenges of Running Foundation Models on Edge Devices By now, you might be thinking, Edge foundation models sound amazing! Why isn’t everyone using them? Well, as with most things in life (and AI), there’s a catch. Running foundation models on edge devices isn’t exactly a walk in the park. Let me walk you through some of the challenges, starting with my own cautionary tale.\nMy Gemma Debacle About six months ago, I got my hands on Google’s shiny new instruction-tuned Gemma 2B model. Gemma, for those unfamiliar, is the “baby” version of DeepMind’s Gemini family — a lightweight, open-weight LLM designed for resource-constrained environments.\nWhy Gemma2B? In a nutshell, it's very impressive for its size. In benchmarks tasks, Gemma 2B's performance is comparable those of 7B - 9B LLMs. It also consistently tops the HuggingFace leaderboard for smaller LLMs. Gemma 2B calls itself a polyglot, claiming fluency in a bunch of languages. If that checks out, it’s a pretty exciting pick for global business apps — just the kind of thing a solo developer (like me) could put to work. Basically, Gemma 2B was designed for laptops, desktops, and modest cloud setups. It sounded perfect for my trusty MacBook Air M2 (8GB of RAM).\nSpoiler alert: it wasn’t.\nI excitedly set up Gemma and attempted to serve my first request. My MacBook? It practically waved a white flag and crashed halfway through.\nLet’s do the math to see why this happened. The Gemma 2B model has (you guessed it) 2 billion parameters. Using the standard float32 data type, the model parameters alone would require $2B\\text{ parameters} * \\frac{4 \\text{ bytes}}{\\text{parameter}} = 8 \\text{ billion bytes} = 8 \\text{ GB of RAM}$.\nBut that’s not all my machine needs to handle:\nI need extra memory for activations (the intermediate calculations during inference). The operating system (in my case, macOS) also needs a hefty chunk of RAM to do its thing In short, my poor MacBook was way out of its depth. Even with more efficient data types like float16 or bfloat16 (which halve memory usage), the combined memory demands of the model, activations, and system processes were just too much. Now, imagine trying to squeeze this kind of workload onto a smartphone with even less RAM. You’d be lucky if your phone didn’t catch fire (kidding\u0026hellip;mostly).\nThe Memory Struggle Is Real Edge devices are, by design, resource-constrained. They’re great for portability, but they aren’t built to handle the sheer memory and compute demands of large language models. Even lightweight models like Gemma, which aim to close this gap, can still overwhelm devices with limited RAM or processing power.\nBut don’t despair! Engineers and researchers are tackling these challenges head-on. By using model compression techniques like quantization, pruning, and distillation, they’ve managed to shrink memory and compute requirements significantly. Add to this a new wave of hardware optimization techniques, and edge deployment is more feasible than ever.\nNow, innovative tools are building on these advancements to make edge LLMs not just possible, but practical for a wide range of devices. Curious about how these breakthroughs are unfolding in real-world applications? Let me introduce you to one powerful solution: the MLC LLM framework.\nMLC LLM: A Quantum Leap in Deploying Edge Foundation Models Fast forward to November 2024, I decided to try the same task as before but with the Machine Learn Compiler (MLC) LLM Engine. This time, I deployed a pre-quantized version of this Gemma 2B model onto an edge device — specifically, an iOS app. The results blew me away.\nFor starters, I encountered zero performance issues on my MacBook Air. The pre-quantized and hardware-optimized Gemma model ran smoothly and efficiently, without any of the lag or crashes I had faced with six months earlier.\nBut here\u0026rsquo;s where things really got exciting: the quality of the responses. They were practically indistinguishable from the likes of massive, cloud-based LLMs in an everyday conversation. Curious to see how well this mini-model handled other languages, I threw some Spanish and German at it. To my non-discerning eye, the results looked spot-on. (I’d love to hear what native speakers think, though.)\nI decided to practice my very rusty Spanish and seek vacation inspiration in one-go. Here are Gemma's recommendations for a visit to Valencia, Spain 🇪🇸 in Winter. Christmas time means Christmas Markets 🎄 — especially when you live so close to Germany 🇩🇪. So, I decided to see if Gemma could give me any fun suggestions in German. Now, you might be wondering: How did MLC manage to pull this off? Let’s take a step back and dive into the tech behind this feat.\nHow does MLC LLM work? At a high-level, the MLC LLM project makes it possible to embed smaller LLMs (under 10B parameters) on edge devices through a streamlined, three-step process:\nQuantize LLM weights as the model is downloaded. This prevented my machine from crashing due to insufficient memory. Embed the quantized model with hardware-specific optimizations applied during the model compilation stage. Provide a simple, pre-built user interface to interact with your newly embedded foundation model. The MLCEngine embeds LLMs across different software platforms through model quantization and hardware-specific optimization (image credit) MLC offers a user-friendly, open-source chat application for both Android and iOS. Alternatively, it implements its own version of OpenAI\u0026rsquo;s Python API, making it easy to integrate the optimized LLM into your own existing projects.\nQuantization MLC LLM caches pre-quantized model weights and compiled model library locally, which means you only need to download and quantize the model once. After that, the quantized model is ready to run on your device without requiring repeated downloads. This saves both time and bandwidth, making the process smoother and more efficient.\nWhat is Quantization? In simple terms, quantization is the process of reducing the precision of the numbers that represent a model’s parameters. The goal? Shrink the model’s memory footprint while keeping its performance as close to the original as possible. The real magic happens when you see the cost savings—quantization can cut your cloud compute bills by half, a quarter, or even a sixth, without any noticeable drop in performance. For massive models like LLMs, those savings can really add up.\nTake the example of yurts, a contractor for the U.S. government. They slashed its monthly cloud computing bill from USD 24,000 to USD 4,000 for a 70B parameter LLM by using a quantization method called Activation-aware Weight Quantization (AWQ). Pretty impressive, right?\nQuantization Methods When it comes to quantizing a model, there are a few common methods, but the two main approaches are:\n1. Post-Training Quantization (PTQ) After a model is trained, you can apply quantization to reduce the bit-width of its weights. The best part? It’s quick, easy, and requires minimal changes to the original model, while still offering significant memory savings.\nOne common PTQ technique is grouped quantization, where the model’s weights are grouped based on features like their layer or importance. Each group is quantized separately, making the process more tailored and efficient. This method has been around since the late 1990s and continues to evolve as a way to balance performance and memory efficiency.\nSome weight groups are more sensitive to quantization errors and need higher precision (more bits) to maintain accuracy. Others can handle lower precision without a noticeable hit to performance.\nWith the rise of foundation models, more specific implementations of grouped quantization have emerged. For an in-depth look, check out \u0026ldquo;The case for 4-bit precision: k-bit Inference Scaling Laws\u0026rdquo; and \u0026ldquo;The LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models\u0026rdquo;.\nMore recently, techniques like Activation-aware Weight Quantization (AWQ) have taken this dynamic quantization approach even further. AWQ uses activation statistics to pinpoint the most important individual weights and ensures they aren’t over-quantized, allowing for better compression without sacrificing performance.\n2. Quantization-Aware Training This method goes a step further by training the model with lower precision in mind from the start. By optimizing the model for reduced precision during training, you often get better results than you would with post-training quantization. Essentially, it allows the model to “learn” how to perform well with less precision, resulting in better overall performance. However, as we focus on deploying pre-trained models, we won’t explore this method further.\nQuantizing Transformers When quantizing Transformers, it’s not just the weights that need attention—activations play a big role too. Activations are the intermediate values generated during the model\u0026rsquo;s forward pass as it processes the input data. In a Transformer, these are the values produced at each layer as it handles each token. Just like with weights, activations can also be compressed during quantization, which further reduces memory usage.\nBut memory management doesn’t end with weights and activations. For Transformers, there’s also the key-value (KV) cache — this stores the context of the input sequence as the model processes longer inputs. As the model processes longer and longer inputs, it needs more memory to store the increasing number of keys and values. To keep things efficient, MLC LLM provides additional memory optimization techniques, like sliding windows, which help manage memory usage even when dealing with longer sequences.\nThe key-value (KV) cache in Transformers preserves the context of processed tokens, enabling the model to \"remember\" earlier parts of a conversation. Yet, as the conversation grows, the cache scales up rapidly, incurring risks of out-of-memory errors on edge devices if not handled properly. (image credit) Out of the Box MLC LLM Solutions As you can probably guess, the MLC Engine only implements post-training quantization (since we have no control over an open sourced LLM\u0026rsquo;s training process). In particular, MLC LLM implements the grouping quantization methods shown below.\nMethod Name Weight Quantization Activation Quantization Version No. Stable? q0f16 None 16 bits - Yes q0f32 None 32 bits - Yes q3f16_1 3 bits 16 bits 1 Yes q4f16_1 4 bits 16 bits 1 Yes q4f32_1 4 bits 32 bits 1 Yes q4f16_awq 4 bits 16 bits 1 No Table 1: An Overview of MLC's implemented quantization methods (source). MLC Enginer also offers an AWQ implementation (called q4f16_awq), but it\u0026rsquo;s currently unstable so use it at your own risk.\nOf course, the folks behind MLC have already gone and quantized most of the very popular open-source LLMs. You can download these pre-quantized model weights from their official MLC AI\u0026rsquo;s HuggingFace account.\nIf you want to quantize a new model, then there\u0026rsquo;s a little more work involved. MLC right now supports quantization of these model types: baichuan, bert, chatglm3, cohere, eagle, gemma, gemma2, gpt2, gpt_bigcode, gpt_neox, internlm, internlm2, llama, llava, medusa, minicpm, mistral, mixtral, orion, phi, phi3, phi3v, qwen, qwen2, qwen2_moe, rwkv5, rwkv6, stable_lm, and starcoder2.\nSo, if you want to quantize of these model types yourself, then all you have to do is run a few simple commands.\nCustom Solutions If you want to quantize an unsupported model type, you\u0026rsquo;ll need to extend MLC LLM\u0026rsquo;s source code. This involves inferring your target model\u0026rsquo;s architecture from its source config.json file on HuggingFace and wrapping its original Python definition (e.g., from the transformers Python library) with MLC LLM\u0026rsquo;s wrappers. I ended up having to do this to support multi-modal functional in the 3rd blog post in this series.\nHardware Optimizations Quantization is just one part of MLC’s bag of tricks. The other? Squeezing every last drop of performance out of your hardware through smart optimizations. See, your LLM model might start as high-level Python code, but it doesn’t interact directly with your device’s hardware. There’s a crucial middle step where MLC translates that model into something your CPU or GPU can actually understand—and it does this in the most efficient way possible.\nJust-in-Time Model Compilation Just-in-Time (JIT) model compilation is the secret sauce behind MLC’s stellar efficiency. Instead of pre-compiling everything in advance or running the model eagerly line-by-line, JIT optimizes your model right before it executes, ensuring it’s perfectly suited to your specific hardware.\nJIT strikes a balance between two compiler approaches:\nInterpreted execution processes code step-by-step as it runs. This makes the code super flexible and easy to debug, but leaves no room for optimizations. In other words, it\u0026rsquo;s painfully slow. Ahead-of-Time (AOT) compilation pre-compiles everything into a fixed version before execution. This is much faster, but comes with a catch: we assume a one-size-fits-all solution. If the model encounters unexpected conditions or hardware variations, AOT’s rigid approach can leave performance on the table because it can’t take full advantage of the specific device running the code. JIT avoids these pitfalls by waiting until runtime to optimize. It tailors the model’s code to your hardware and execution context just before runtime, ensuring maximum efficiency. Here\u0026rsquo;s how this process works:\nTracing or scripting. First, the engine analyzes your model’s high-level code and maps out its computation graph and operations. Think of it as creating a blueprint for what the model will do Optimization. Next, the engine gets to work refining that blueprint. It fuses operations, removes redundancies, and inlines functions, streamlining execution wherever possible. (It’s like an architect revising a blueprint for a more efficient construction process.) Low-level code generation. Once the optimizations are done, the graph is compiled into low-level machine code tailored to your specific hardware—whether that’s a CPU, GPU, or something fancier. Execution. Finally, the optimized code is executed, running faster and using less memory thanks to all the pre-launch optimizations. MLC uses JIT model compilation to get the most out of your edge device\u0026rsquo;s limited resources. And the best part? This process is abstracted away into a few simple CLI commands.\nMLC LLM Implementation Deep neural networks are computationally demanding. Hence, most deep learning frameworks include built-in JIT compilation extensions. For example, Accelerated Linear Algebra (XLA), the backbone of JAX, offers cross-framework JIT support. Looking specifically at PyTorch, torch.compile provides a general-purpose solution that supports both training and inference.\nHowever, MLC takes it a step further by leveraging Apache\u0026rsquo;s Tensor Virtual Machine (TVM) for even deeper hardware-level optimizations.\nWe can think of TVM as an improvement on PyTorch's optimizations, offering advanced optimizations and hardware-specific tuning that PyTorch's JIT lacks. Additionally, TVM is easy to use due to the separation of its compiler and runtime components. This makes it possible for me to compile a ML model on one machine (e.g., a MacBook) and deploy it on another (e.g., a Raspberry Pi). (image credit) TVM works by providing a Python API for tensor operations like matrix multiplications, sums, and type conversions. It also makes it a breeze to port models from PyTorch. Once we have the model in TVM, we can translate it into C++ as we optimize it for execution.\nHere’s how exactly TVM supercharges model optimization:\nOperation Fusion. TVM combines smaller operations (like element-wise additions or multiplications) into larger, more efficient ones.\nExample. Instead of calculating ReLU(x) followed by Add(x, y), TVM can combine them into a single, efficient kernel, saving memory and time.\nMemory Layout Optimization:. TVM fine-tunes memory access patterns to align with the hardware’s strengths. For example, GPUs perform better when accessing data in large, coalesced blocks, while CPUs benefit from loop optimizations that prevent cache misses.\nKernel Selection and Tuning. A \u0026ldquo;kernel\u0026rdquo; is a specialized function designed to perform specific operations, like matrix multiplication. TVM either selects the best pre-tuned kernels or auto-tunes them for maximum performance on the target hardware.\nThese optimizations make it possible to (hypothetically) fit a 7B+ parameter model onto an iPhone. But of course, there’s a trade-off: the more optimizations we apply, the less flexible the model becomes. Debugging also gets trickier — any issues that arise are often low-level errors, especially when input sizes change.\nDespite these challenges, the benefits far outweigh the costs. Without TVM, deploying models on edge devices would be much more difficult.\nConclusion In the past six months, the AI research community has made groundbreaking strides in optimizing foundation models for edge devices. Back in June 2024, my personal machine crashed when I tried to run the Gemma 2B model locally — without quantization or hardware optimizations. But thanks to the rapid progress in this field, even solo enthusiasts like myself can now, as of November 2024, easily deploy the same model (or even larger ones) locally—without needing to become compiler engineers.\nIn this blog post, I’ve introduced the Machine Learning Compiler (MLC) as a powerful new tool to make this possible. I’ve also walked you through its inner workings and provided essential background knowledge to help you get started.\nWhat\u0026rsquo;s next? In my next blog post, we’ll dive into how we can extend the MLC Engine to support embedding LLMs that aren’t natively supported. After all, our goal is to deploy a multi-modal LLM on an iPhone.\n","date":1732838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732838400,"objectID":"954b0af80ab45bdef1021e282d31758d","permalink":"https://bellanich.github.io/post/edge-llm-mlc/","publishdate":"2024-11-29T00:00:00Z","relpermalink":"/post/edge-llm-mlc/","section":"post","summary":"The open-source Machine Learning Compiler Engine project is transforming foundation models into efficient and portable powerhouses.","tags":["Generative AI","Edge ML","Embedded Systems","Article"],"title":"Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC","type":"post"},{"authors":["Bella Nicholson"],"categories":null,"content":"","date":1713350700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713350700,"objectID":"0dc754f4308cd3b8751e26c7425683e9","permalink":"https://bellanich.github.io/talk/the-mlops-blueprint-productionalizing-machine-learning/","publishdate":"2024-04-17T10:45:00Z","relpermalink":"/talk/the-mlops-blueprint-productionalizing-machine-learning/","section":"event","summary":"I joined the Women Who Code Community to give a crash course in Machine Learning Operations (MLOps). This talk brings together different MLOps perspectives to create a clear and actionable framework, giving you the blueprint to navigate real-world ML systems.","tags":["Talks"],"title":"The MLOps Blueprint: Productionalizing Machine Learning","type":"event"},{"authors":null,"categories":null,"content":"In the past month, I\u0026rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it\u0026rsquo;s also about crafting solutions that rise above them altogether.\nIn this case, I found myself asking: \u0026ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project\u0026rdquo;? Well, from putting together this Python ML Template repository, my answer is three. (See the demo above for a quick example.)\nIf you\u0026rsquo;re curious about my implementation, checkout my project\u0026rsquo;s README file and documentation. Happy ML feature development!\n","date":1711324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711324800,"objectID":"042ea30eb72b64cfa2bc5adc2ac11469","permalink":"https://bellanich.github.io/post/python-ml-template/","publishdate":"2024-03-25T00:00:00Z","relpermalink":"/post/python-ml-template/","section":"post","summary":"After churning out too many projects from scratch in one month, I built this ML template to make life easier—for both of us. Start ML development with just 3 commands.","tags":["News"],"title":"From Framework to Functionality: 📢  My Custom Python ML Template Launch","type":"post"},{"authors":null,"categories":null,"content":"In the past month, I\u0026rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it\u0026rsquo;s also about crafting solutions that rise above them altogether.\nIn this case, I found myself asking: \u0026ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project\u0026rdquo;? Well, from putting together this Python ML Template repository, my answer is three. (See the demo above for a quick example.)\nIf you\u0026rsquo;re curious about my implementation, checkout my project\u0026rsquo;s README file and documentation. Happy ML feature development!\n","date":1711324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711324800,"objectID":"60758eda9188d20b21305bfe0f1b0cee","permalink":"https://bellanich.github.io/portfolio/python-ml-template/","publishdate":"2024-03-25T00:00:00Z","relpermalink":"/portfolio/python-ml-template/","section":"portfolio","summary":"A starter kit for Python-based Machine Learning projects. Jumpstart your ML development with just 3 bash commands 🚀","tags":["Code"],"title":"Python ML Template","type":"portfolio"},{"authors":null,"categories":null,"content":"As a Machine Learning engineer, I use Git daily, but even useful commands can feel cryptic at times. I struggled to find a cheatsheet that bridged the gap between theory and real-world application. That\u0026rsquo;s why I created my own cheatsheet – a resource that demystifies core concepts alongside practical commands.\nDownload the PDF below, or grab the source code to customize it for your needs. Let\u0026rsquo;s up our Git game together!\n","date":1698710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698710400,"objectID":"064956c829afdd29e4fe6832c36c9752","permalink":"https://bellanich.github.io/portfolio/git-cheatsheet/","publishdate":"2023-10-31T00:00:00Z","relpermalink":"/portfolio/git-cheatsheet/","section":"portfolio","summary":"A brief but comprehensive guide to Git -- both in practice and in theory","tags":["Notes"],"title":"Git It Right: A Complete Cheatsheet","type":"portfolio"},{"authors":null,"categories":null,"content":"As a Machine Learning engineer, I use Git daily, but even useful commands can feel cryptic at times. I struggled to find a cheatsheet that bridged the gap between theory and real-world application. That\u0026rsquo;s why I created my own cheatsheet – a resource that demystifies core concepts alongside practical commands.\nDownload the PDF below, or grab the source code to customize it for your needs. Let\u0026rsquo;s up our Git game together!\n","date":1698710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698710400,"objectID":"49fecb387c4b9fa8927f69c12b745898","permalink":"https://bellanich.github.io/post/git-cheatsheet/","publishdate":"2023-10-31T00:00:00Z","relpermalink":"/post/git-cheatsheet/","section":"post","summary":"As I transition to my new role, I used my downtime to go deep into Git — a tool I rely on daily — and condensed everything I learned into a concise, 3-page cheatsheet.","tags":["News"],"title":"Notes Alert: 📓 A Handy 3-Page Cheatsheet for Version Control Pros","type":"post"},{"authors":["Bella Nicholson","Aoibhinn Reddington"],"categories":null,"content":"","date":1698317100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698317100,"objectID":"60c5fe00eab021fe4256e5008702500b","permalink":"https://bellanich.github.io/talk/a-case-study-on-building-scalable-machine-learning-systems/","publishdate":"2023-10-26T10:45:00Z","relpermalink":"/talk/a-case-study-on-building-scalable-machine-learning-systems/","section":"event","summary":"I delivered a guest lecture for TU Delft's Machine Learning course. I introduced fundamental ML engineering topics, as illustrated through my real-life experiences at a Dutch e-commerce tech company.","tags":["Talks"],"title":"A Case Study on Building Scalable Machine Learning Systems","type":"event"},{"authors":["Bella Nicholson","Bob Borsboom","Tim van Loenhout","Jochem Hölscher"],"categories":null,"content":" Introduction When it comes to policy based methods, gradient behavior can be very telling. In essence, gradient stability determines: (a) how long it takes a given model to learn and (b) whether it can even learn anything at all. To illustrate what we mean, we will spend this blog post investigating the stability of policy gradients in a few different contexts (i.e., different algorithm-environment combinations). Particularly, we use the REINFORCE, G(PO)MDP and G(PO)MDP+ whitening algorithms, since we can view the latter two algorithms as a progression of the former such that each step of this progression leads to more stable gradients. Meaning, these algorithms are similar enough to each other such that we can make fair comparisons, but also are theoretically guaranteed to exhibit different degrees of gradient stability. We apply these algorithms to two classical RL problems: a self-designed version of GridWorld and OpenAI’s CartPole problem. We do so since both environments vary in their rewards distributions and state-action space complexity.\nDon’t worry if you’re not completely up to date on your reinforcement learning knowledge. We’ll first briefly introduce all the need-to-know concepts — including a proper explanation of what policy gradient-based methods are, and what our chosen environments look like. So, if you’re more confident in your RL knowledge, feel free to skip ahead to our experiments.\nAre you ready?\nGridWorld GridWorld is one of the most simple RL problems, since it’s so easy to visualize and hence properly understand. As such, the intuition that this problem provides is not only invaluable, but it is also needed for our dive into policy based methods. Therefore it will be the first thing we discuss.\nThe premise of GridWorld is as the name suggests: we model our “world” as a finite grid. Generally speaking, our agent knows nothing about this world at first, but it needs to learn the best way to move through it. Thus, our agent relies on trial and failure. Consider the GridWorld problem of teaching a robot to pick an apple from the apple tree. We will model the field our robot is walking through as a 5 x 5 grid, since our robot cannot vary its step size. For simplicity’s sake, we’ll assume that if the robot reaches the apple tree, then it is always able to grab an apple. When it lands in a square that contains an apple tree, we give it a reward of +2. If the robot is unlucky enough to find a rock to trip over, it gets a penalty of -1 as we want it to walk without hurting itself. To keep things interesting, we don’t give the robot any feedback unless it falls or reaches its goal. Whenever the robot gets an apple or runs out of time, we wipe our rewards count clean, pick it up, and place it at the same starting point to try again.\nPolicy based methods In Reinforcement Learning, there are different ways to learn a good policy. For example, you can first explore GridWorld by random walk until you either discover an apple tree or a pesky rock. Afterwards, different algorithms have different ways of extracting a policy from the “experience” we’ve gained. Suppose, we want to learn this policy directly from our past experiences rather than rely on indirect methods. In essence, this is what policy based methods do. We convert policy learning into a calculus problem, where we model the cumulative rewards our robot receives as a function with the intent to optimize it.\nREINFORCE With REINFORCE, we play a game (e.g. our apple-picking robot in GridWorld) until we reach some terminal state (e.g., arriving at an apple tree). Afterwards, we start all over again to gather even more information about how to formulate the best strategy. During the course of each run through this game, we sum all the derivatives over the log probabilities of an action given its current state. Once we hit our terminal state, our trajectory gets cut off and we multiply the sum of gradients with that trajectory’s reward. We formalize this process as:\nWe run the algorithm N times and then take the average over these runs to get a more stable and thus less-noisy update. Thus, the expression above becomes\nwhere we not only loop over every i-th trajectory but also every state-action pair t that we encounter. Once our gradients are calculated, we use the following update rule to calculate our new parameter values. This rule will improve the policy.\nwhere alpha represents our learning rate.\nIf we refer to Figure 2, alpha literally determines the step size we take along our expected rewards surface once we learn something new. For these reasons, it is also interchangeably referred to as the step size. While more complex methods anneal the step size with respects to time, we’ve kept things simple and treated alpha as a constant hyperparameter.\nAn Intuitive Look at REINFORCE As we’ve previously established, we multiply a trajectory’s reward by the sum of its gradients. Suppose, we obtain a cumulative negative reward, because our robot walked over a few obstacles before it reached an apple tree. Now, the gradients are multiplied with a negative number. Meaning, our new policy lowers the likelihood of performing the same actions given each respective state. On the other hand, suppose that we get a positive reward, since our robot successfully walked toward an apple tree. In this case, our policy will make the selection of these actions for these given states more probable in future trajectories.\nThe Problems with REINFORCE Thus far, we’ve painted a rather rosy picture of the REINFORCE algorithm. Now, it’s time to take a more critical look at REINFORCE and see what room there is for future improvements. Let’s look at two paths that our robot can take in a slightly different version of GridWorld.\nOnce again, our robot’s objective is to walk towards the positive reward without hurting itself. In the lefthand side of the figure, the agent’s performance is perfect until its last step. Now, REINFORCE will make ALL of these actions less probable. Whereas, we only want to “punish” the last action.\nA similar mistake happens again on the right hand side of figure 3. Here, the total reward for that trajectory is zero, so the model won’t change the action-state probabilities. In other words, we don’t learn from the mistakes we made by choosing the wrong first action. Over enough trajectories, REINFORCE may be able to find the optimal policy, but this high degree of variance means that this process takes longer.\nG(PO)MDP G(PO)MDP is an improvement as it fixes the previously mentioned mistakes of REINFORCE. Instead of multiplying the reward of the trajectory by all the actions performed in that trajectory, it handles this situation in a more clever way. G(PO)MDP only multiplies the reward from a trajectory by the steps which lead to this action. We formalize this introduced temporal-reward dependency as,\nWe simply sum over all the steps in a trajectory, but here comes the interesting part: the right summation sums all the log derivatives of actions taken until we get a reward at time step t’. Afterwards, we multiply this reward by the steps which lead to this reward. Meaning, only the actions that lead to a reward will be “influenced” by that reward. While all the actions were previously influenced by the rewards which came before that set of actions, G(PO)MDP provides a fairer policy update. Simply put, our action probabilities now increase or decrease solely based on the reward a given action leads to.\nThis tweak in our objective function makes G(PO)MDP more fair in its assignment of action-state likelihoods and lowers the variance of our policy gradients. However, we’re still interested in lowering it even further, since this can decrease the time it takes us to figure out what the optimal strategy is.\nBaselines We’ll take a look at baselines now as a means to further lower policy gradient variance. Baselines are a trick researchers found to stabilize policy learning through the reduction of gradient variance. Fundamentally, we subtract a constant estimate of our expected rewards from our current return G_t Meaning, our parameter update becomes\nwhere we refer to this expected rewards estimate as a baseline. Many baselines variants have been proposed and tested, and each comes with its own set of advantages and disadvantages. To keep things simple, we’ve settled for a baseline technique called whitening, where we scale our cumulative returns by their mean and standard deviation values. i.e.,\nWhile more sophisticated methods do exist, whitening gives us a basic understanding of a baseline alters policy gradient behavior.\nDifferent environments With the premises of our chosen algorithms established, we also need to discuss the problems we’ll be using these algorithms to solve. Since algorithm behavior largely depends on the given environment, our GridWorld problem isn’t enough to gain a comprehensive picture of gradient stability. As a result, we also need to consider an environment that is significantly different from GridWorld — both in its state-space structure and in its reward distribution. That is, we want a continuous state-space environment that offers more immediate feedback.\nThus, we settled for OpenAI’s CartPole environment, which is described as follows:\nCartPole. A pole is attached by an unactuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force left or right to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center. A link to the environment can be found here.\nIf you want to learn more about the OpenAI Gymnasium, take a look here.\nThrough this choice of environments, we can see two different “forces” that contribute to a problem’s difficulty: (1) the size of the state-action space we have to search through, and (2) the level of feedback our agent receives for every good or bad action that it takes. Intuitively speaking, the more feedback you receive as you try to learn some new skill or task, the easier the learning process becomes.\nFrom this perspective, neither problem is indisputably more difficult than the other. The CartPole environment requires our agent to navigate through a more complex state-space; however, GridWorld does not always offer feedback. Our agent can move in such a way where it neither encounters an apple tree or a rock, and thus receives no reward. This level of non-immediate feedback adds complexity to what seemed like a rather simple problem.\nGradient Stability Up to this point we’ve established the importance of gradient stability but have not yet detailed a means to quantify it. Given that the concept of gradient stability is simultaneously intuitive and nebulous, we need to reframe the way we view it in order to make it measurable. More specifically, we define gradient stability by how similar policy gradients are to one another within each time step. To do so, we use the statistical analysis tools of variance, kurtosis and gradient sign change.\nVariance Also known as the second central moment, variance describes the spread of the gradients, and thus clues us in to our model’s update precision. i.e., How consistent or steady are our model updates from to run? High levels of variance indicate that the policy gradients go in many different directions, while low levels of variance mean our gradients are more or less uniform in the direction they point in.\nKurtosis However, distributions with identical means and variances can still have have very different shapes. In such cases, we also need to look at kurtosis (the fourth standardized central moment), which describes how heavily tailed a distribution is. In other words, kurtosis quantifies the presence of gradient outliers. The higher kurtosis is, the more extreme these gradient values become. This in turn makes makes our policy gradients become more inconsistent — i.e., less stable.\nFigure 5 - Different kurtosis values for identical variance (Source) Sign changes Finally, the number of sign changes that occur between time steps also provides clues about gradient stability. If the gradient signs keep changing, this means that we are literally walking back and forth along our loss surface. This implicates one of two possibilities, we either: (1) are consistently overshooting in our updates, or (2) are moving randomly as we do not know where to go. Either way, this indicates less confidence in our parameter updates and hints at model instability.\nExperimental Setup We can breakdown our gradient stability analysis into two parts: (a) differences in stability between algorithms in the same environment, and (b) differences in stability between the same algorithm in different environments. Of course, we’ve already covered part (a) in our background. Over all, G(PO)MDP is not influenced by the noise of rewards from prior actions, and whitening only further reduces the variance of whatever policy based method we apply it to. As such, we expect REINFORCE to have the least stable gradients, followed by G(PO)MDP, and by then G(PO)MDP + whitening.\nMeanwhile, the complexity of CartPole’s state space could highlight the shortcomings of REINFORCE. However, this overlooks the main distinction between REINFORCE and G(PO)MDP as one handles rewards considerably better than the other. Therefore, the sparsity of GridWorld’s rewards distribution may outweigh the problems introduced with high state-action search spaces. As a result, we expect a larger difference in policy behavior between the algorithms in the GridWorld environment. We used the following experimental setup to verify our hypothesis.\nPolicy Architecture Generally, the problem we are trying to solve dictates our policy network architecture. As we’ve established previously, our state-action space is small and discrete in GridWorld. More specifically, we only have 25 potential states and 4 potential actions to choose from. Given that we define a policy by the action distribution for each state, our policy is nothing more than a look-up table. In practice, this translate to the use of a single linear layer when we apply (deep) policy gradient methods. In contrast, once we delve into continuous state-space problems, we need more complex (i.e., multi-layered) models. Thus, for the CartPole environment, we default to a simplified version of the standard Q-Network architecture, as shown in Figure 6. The original DQN paper can be found here. Note that we use a final softmax layer to get a probability distribution over the possible actions we can take.\nIf you want to learn more about the basics of neural networks, this is a good starting point.\nParameter Configurations Initially, we conducted hyperparameter tuning with respects to each algorithm-environment combination, but quickly discovered that the environment more or less determined the optimal learning rate. Here, we define the optimal learning rate as the hyperparameter value which yields the highest cumulative rewards on average. Of course, we cannot apply the same exact process to define our discount factor, since the higher our discount factor is, the higher our cumulative rewards become by definition. However, a good discount factor is still important. So, we \u0026ldquo;tuned\u0026rdquo; discount factor by whichever value lead to convergence — which was once again model specific. Table 1 summarizes all of the values that we used.\nObtaining policy gradients We apply each of our three algorithms to the two environments we’ve previously described, and train our model for N=800 episodes. Every 20 iterations, we pause policy training to test policy performance. In this validation step, we sample a 100 different episodes to gather the gradients of each weight within our network as well as the cumulative rewards observed. The latter is particularly important, since it allows us to contextualize how well our policy is really doing. Meanwhile, the former allows us to understand how gradient behavior varies with respects to time. Due to the sensitivity policy networks exhibit towards their initialized weights, we repeat each model-environment combination using 20 different seeds and report the average statistical measurements that we’ve observed.\nObtaining variance, kurtosis and sign changes Since the collected policy gradients values are high dimensional tensors, we cannot directly visualize the policy gradient distribution. Instead, we compute the variance and kurtosis for each individual network weight over the 100 episodes sampled per validation step. Then, we take the average over these statistics to obtain a set of aggregated variance and kurtosis measurements. Together, these values describe the average gradient distribution for a single algorithm-environment combination at a specific time step. We repeat these calculations for all sampled policy gradients and plot our results. Note that the obtained gradients are squeezed between -1 and 1 across the episode dimension. This rescaling does not impact the variance/kurtosis ratio between two algorithms for a single weight gradient, but it does make sure that all weights contribute equally to our aggregate variance and kurtosis values.\nResults Starting with a look at gradient variance, Figure 7 proves that a choice in environments makes a stark difference in policy gradient behavior. As expected, we observe clear differences in gradient stability when in GridWorld, but when not when we’re in CartPole. This confirms our hypothesis that the rewards distribution has the capacity to incite more instability than the complexity of our search (state-action) space.\nInterestingly enough, Figure 7b suggests that REINFORCE somehow exhibits the least amount of variance. Outside of any other context, one could mistakenly presume that this means REINFORCE is the best performing algorithm. However, Figure 8 reveals a very different reality: REINFORCE performs so poorly that our model never learns anything. In the context of our apple-picking robot, our robot literally runs in circles and continuously trips over the same rock. Often times, it only stops when it runs out of time (i.e., the number of steps it can take per episode). With this knowledge, we can re-contextualize the abnormality of REINFORCE’s variance as a sign that REINFORCE is consistently random.\nOf course, this begs the question: How would REINFORCE perform as an objective function if we were able to get a sensible (i.e., non-random) policy? To answer this question, we trained our policy networking using our most stable algorithm, G(PO)MDP + whitening, and only use each algorithm’s respective loss function for policy gradients sampling (Figure 9). While the results observed for Figures 7a and 9a are more or less interchangeable, the same cannot be said for Figures 7b and 9b. In Figure 9b, REINFORCE’s variance starts low as the policy is close to random, but it soon enough converges to the variance of G(PO)MDP. Note that G(PO)MDP + whitening has the lowest variance of all, but this is only logical since the model has been trained to minimize this specific loss function.\nNonetheless, we have to acknowledge that having the same variances does not implicate different policy gradient distributions to be the same. Meaning, the results we obtain from Figures 7a and 9a are, in a sense, inconclusive. For these reasons, we also consider the kurtosis of our policy gradients (Figure 10). REINFORCE exhibits the highest levels of kurtosis followed by G(PO)MDP and then by G(PO)MDP + whitening. Once again the higher kurtosis is, the more gradient outliers a given algorithm has and the noisier its updates become. Meaning, the stability of each algorithm in CartPole is as we expected it to be.\nWith the stability established with respects to each algorithm and each problem, we also want to look more closely at how gradient stability varies over time. To keep our figures legible, we only visualize the percentage of positive gradient sign changes. As seen in Figure 11, our policy gradients generally start off as relatively balanced as we begin to make our away along our loss surface. However, during the progression of policy learning, this balance tips in favor of negative gradient sign changes. When this decline in percentage of positive gradient changes is monotonic, we deem overall model learning to be stable. Given that our model relies on these policies to learn, we can view the stability of policy gradients and model learning as one and the same.\nSurprisingly, there is a very slight decline in the percentage of positive gradient sign changes observed in REINFORCE’s gradients (Figure 11b). However, we don’t believe that this model slightly improves on a consistent basis, since GridWorld is too simple to provide opportunities for nuanced policy learning. Rather, we attribute this slight decline to the sensitivity policy networks have towards their initialized weights. Likely, there might be one or two policies out of the 20 seeds that we averaged over where our agent was actually able to learn something using reinforce — even if this something was to avoid states where it would trip. Similarly, we can also use this to explain the erratic behavior of REINFORCE with respects to its policy gradient kurtosis in CartPole.\nConclusion As seen throughout our results, context is critical when describing policy learning stability. Whenever we rely on a single figure, we risk critically misinterpreting our results and landing on a faulty assumption — such as mistaking our worst performing algorithm for our best performing one. This need for context extends well beyond the consideration of many measurements and goes into explicitly stating the underlying assumptions we make when formulating a hypothesis. After all, in all of our predictions, we assumed the models would eventually learn something about how to play each game, even this something was far from perfect. By never formalizing this assumption, we never considered the alternative possibilities: models are never guaranteed to learn. Especially, models as simple as the ones we implemented here.\nGitHub If you want to take a closer look to our implementation, here you can find a link to our github. Have fun!\nNote. This blog post was originally posted on Medium in October 2020. This assignment was completed as a part of the University of Amsterdam\u0026rsquo;s reinforcement learning course.\n","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"33a178cf80848e88e0171d10f194f54e","permalink":"https://bellanich.github.io/post/reinforcement-learning/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/post/reinforcement-learning/","section":"post","summary":"How does the gradient stability differ between REINFORCE, G(PO)MDP, G(PO)MDP+ whitening during policy learning?","tags":["Academic","Reinforcement Learning","Article"],"title":"Reinforcement Learning: Investigating Gradient Stability in Policy Based Methods","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1f4cb24553577f6808a73065326a8b9d","permalink":"https://bellanich.github.io/blog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://bellanich.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://bellanich.github.io/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://bellanich.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]