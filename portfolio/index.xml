<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Bella Nicholson</title><link>https://bellanich.github.io/portfolio/</link><atom:link href="https://bellanich.github.io/portfolio/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 27 Jan 2025 00:00:00 +0000</lastBuildDate><image><url>https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://bellanich.github.io/portfolio/</link></image><item><title>Transformers Decoded: A Quick Guide to ML Research</title><link>https://bellanich.github.io/portfolio/transformers-handbook/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/transformers-handbook/</guid><description>&lt;p>I&amp;rsquo;ve had a whirlwind introduction to world of LLMs. Between &lt;a href="../edge-llm/">my adventure in embedding a multi-modal foundation model on various iOS devices&lt;/a> and jetting of to &lt;a href="https://neurips.cc/Conferences/2024" target="_blank" rel="noopener">NeurIPS 2024 in Vancouver&lt;/a> ðŸ‡¨ðŸ‡¦, Iâ€™ve been immersed in all things related to LLMs. I&amp;rsquo;ve spent countless hours talking to the researchers building state-of-the-art LLMs and even more time pouring over online resources to understand the fundamentals.&lt;/p>
&lt;p>To save you the trouble, I&amp;rsquo;ve organized everything I&amp;rsquo;ve learned into &lt;a href="https://bellanich.github.io/media/transformers_handbook.pdf"> a handy study guide: &amp;ldquo;Transformers Decoded: A Quick Guide to ML Research&amp;rdquo; &lt;/a>. This guide explains key concepts, industry trends, and the critical optimization techniques that make LLMs so performant. We pay close attention to crucial inference optimization techniques, including:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Speculative decoding&lt;/strong>: Inference is accelerated by generating multiple candidate outputs with a smaller LLM and verifying them with a larger, more accurate LLM.&lt;/li>
&lt;li>&lt;strong>Flash attention&lt;/strong>: By restructuring the attention operations to minimize memory reads and writes â€” the bottleneck in classical attention â€” we achieve significant speedups.&lt;/li>
&lt;li>&lt;strong>Continuous batching&lt;/strong>: Incoming requests are dynamically batched to maximize hardware utilization and throughput, especially in online serving.&lt;/li>
&lt;/ul>
&lt;p>Further details on these (and other) optimization techniques can be found the PDF below. Let&amp;rsquo;s decode the latest in AI research, together!&lt;/p>
&lt;!-- Load local file -->
&lt;embed src="https://bellanich.github.io/media/transformers_handbook.pdf" width="700" height="550" type="application/pdf"></description></item><item><title>Shrinking the Impossible: Deploying My Own Multi-Modal Edge Foundation Model</title><link>https://bellanich.github.io/portfolio/edge-llm/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/edge-llm/</guid><description>&lt;p>Generative AI tooling has become a staple for everyday tasks â€” from creating presentation visuals to finding the right code syntax. But I&amp;rsquo;ve always felt a little too uneasy trusting cloud-hosted LLMs with my private chats.&lt;/p>
&lt;p>As an ML engineer, I assumed that the only alternative was spinning up an expensive private LLM in the cloud â€” until &lt;a href="https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/" target="_blank" rel="noopener">the Google I/O Connect event (June 2024)&lt;/a>. Thatâ€™s where Google revealed their smallest LLM to date, &lt;a href="https://deepmind.google/technologies/gemini/nano/" target="_blank" rel="noopener">Gemini Nano&lt;/a>, running directly &lt;a href="https://www.androidauthority.com/how-to-get-gemini-nano-on-pixel-8-8a-3450466/" target="_blank" rel="noopener">on the Pixel 8&lt;/a>. Seeing an ultra-private AI assistant on edge was inspiring, but for solo developers like me, the open-source tools werenâ€™t quite ready yet.&lt;/p>
&lt;p>Fast-forward six months, and the landscape has changed. Thanks to projects like &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">the Machine Learning Compiler (MLC) framework&lt;/a>, solo developers can now optimize and deploy powerful LLMs on edge devices. Rather than sticking to a unimodal LLM â€” a well-worn path â€” I set my sights higher: deploying a multimodal LLM on the smallest edge devices possible.&lt;/p>
&lt;p>The end result? I successfully embedded &lt;a href="https://huggingface.co/google/gemma-2-2b-it" target="_blank" rel="noopener">the multilingiual Gemma 2B&lt;/a> and &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">the multi-modal LLaVA-OneVision Qwen2 0.5B&lt;/a> models on my laptop (among other devices). Take a look for yourself:&lt;/p>
&lt;img src="images/demo.gif">
&lt;p>As shown, my embedded model can comfortably discuss the content of an image and give some interesting synopses. Pretty impressive, right? Hereâ€™s a look at how it all works:&lt;/p>
&lt;figure>
&lt;img src="images/model_flowchart.png">
&lt;figcaption>
At 800M parameters, the &lt;a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision Qwen2 0.5B&lt;/a> is ideal for edge deployment but it struggles with complex user instructions. Thus, I used Googleâ€™s ultra-efficient &lt;a href="https://huggingface.co/google/gemma-2b">Gemma 2B model&lt;/a> as a fallback.
&lt;/figcaption>
&lt;/figure>
&lt;p>For the full story, including technical details, check out my corresponding &lt;strong>&amp;ldquo;Shrinking the Impossible&amp;rdquo; blog series&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../../post/edge-llm-mlc/">Part 1: Optimizing Foundation Models for Edge Devices with MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-vision-encoders/">Part 2: Teaching Chatbots to See with LLaVA, CLIP, and SigLIP&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-embed-llava/">Part 3: Embedding a Custom-Defined LLaVA-OneVision Model with MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-app/">Part 4: Deploying My Own Pocket-Sized Multi-Modal Large Language Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The source code is also available for your viewing pleasure on &lt;a href="https://github.com/bellanich/pocket-llm" target="_blank" rel="noopener">GitHub&lt;/a>. Consider it your guide to shrinking impossibly large LLMs down to something that fits inside your pocket.&lt;/p></description></item><item><title>Python ML Template</title><link>https://bellanich.github.io/portfolio/python-ml-template/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/python-ml-template/</guid><description>&lt;p>In the past month, I&amp;rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it&amp;rsquo;s also about crafting solutions that rise above them altogether.&lt;/p>
&lt;p>In this case, I found myself asking: &amp;ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project&amp;rdquo;? Well, from putting together &lt;a href="https://github.com/bellanich/python-ml-template" target="_blank" rel="noopener">this Python ML Template repository&lt;/a>, my answer is three. (See the demo above for a quick example.)&lt;/p>
&lt;p>If you&amp;rsquo;re curious about my implementation, checkout &lt;a href="https://github.com/bellanich/python-ml-template/blob/main/README.md" target="_blank" rel="noopener">my project&amp;rsquo;s README file&lt;/a> and &lt;a href="https://github.com/bellanich/python-ml-template/blob/main/docs/0_overview.md" target="_blank" rel="noopener">documentation&lt;/a>. Happy ML feature development!&lt;/p></description></item><item><title>Git It Right: A Complete Cheatsheet</title><link>https://bellanich.github.io/portfolio/git-cheatsheet/</link><pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/git-cheatsheet/</guid><description>&lt;p>As a Machine Learning engineer, I use Git daily, but even useful commands can feel cryptic at times. I struggled to find a cheatsheet that bridged the gap between theory and real-world application. That&amp;rsquo;s why I created &lt;a href="https://nbviewer.org/github/bellanich/git-cheatsheet/blob/1dd32689009de1fdb48b49bbe2c3d437355fad91/git_cheatsheet.pdf" target="_blank" rel="noopener">my own cheatsheet&lt;/a> â€“ a resource that demystifies core concepts alongside practical commands.&lt;/p>
&lt;p>Download the PDF below, or grab &lt;a href="https://github.com/bellanich/git-cheatsheet/blob/main/README.md" target="_blank" rel="noopener">the source code&lt;/a> to customize it for your needs. Let&amp;rsquo;s up our Git game together!&lt;/p>
&lt;!-- Use nbviewer.org to automatically download and embed Git Cheatsheet directly from GitHub -->
&lt;p>&lt;embed src="git_cheatsheet.pdf" width="700" height="550"
type="application/pdf">&lt;/p></description></item></channel></rss>