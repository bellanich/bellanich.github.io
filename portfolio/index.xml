<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Bella Nicholson</title><link>https://bellanich.github.io/portfolio/</link><atom:link href="https://bellanich.github.io/portfolio/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 30 Nov 2025 00:00:00 +0000</lastBuildDate><image><url>https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://bellanich.github.io/portfolio/</link></image><item><title>Frames Per Second: Low-Latency Conditional Image Generation on a 2GB Memory Budget</title><link>https://bellanich.github.io/portfolio/edge-diffusion/</link><pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/edge-diffusion/</guid><description>&lt;p>Running state-of-the-art computer vision entirely on-device isnâ€™t easy â€” and thatâ€™s exactly why I had to try it. During last yearâ€™s &lt;a href="../edge-llm/">adventure in embedding multimodal LLMs on edge devices&lt;/a>, the visceral impact of images really stood out to me. Unlike sequential language, the human brain processes imagery almost instantly, making its effects uniquely potent.&lt;/p>
&lt;p>In recent years, the gravitational pull of Large Language Models (LLMs) has dominated the AI space, thanks to the compounding force of &lt;a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">attention&lt;/a> and &lt;a href="https://arxiv.org/pdf/2001.08361/1000" target="_blank" rel="noopener">scaling laws&lt;/a>. That focus began to shift when Google DeepMind released &lt;a href="https://aistudio.google.com/models/gemini-2-5-flash-image" target="_blank" rel="noopener">Nano Banana&lt;/a> in August 2025, and turned the painstaking process of photography edits into a simple text prompt. Three months later, &lt;a href="https://blog.google/technology/ai/nano-banana-pro/" target="_blank" rel="noopener">Nano Banana Pro&lt;/a> demonstrated a massive leap in generative computer vision capabilities by rendering perfectly legible text pixel by pixel. The implication? The power to instantly generate designer-quality infographics and slide decks.&lt;/p>
&lt;p>Seeing that level of capability running smoothly in the cloud made me wonder: how powerful could a tiny conditional image-generation model be while still fitting into my iPhone (and not crashing it)?&lt;/p>
&lt;figure>
&lt;img src="images/pipeline-diagram.png" style="width: 35%; height: auto;">
&lt;figcaption>
&lt;strong>Figure 1.&lt;/strong> My hyper-optimized three-stage pipeline uses &lt;a href="https://github.com/apple/ml-stable-diffusion">Appleâ€™s CoreML Stable Diffusion model&lt;/a> for on-device conditional image generation. By isolating the heavy diffusion step to background regeneration, the system preserves identity consistency despite the tiny modelâ€™s quality constraints. The result is lightweight, fully on-device image generation averaging ~27 seconds end-to-end.
&lt;/figcaption>
&lt;/figure>
&lt;p>That curiosity quickly launched this hands-on experiment. I challenged myself to run high-capacity, conditional image generation entirely on my iPhone&amp;rsquo;s hardware. Unfortunately, generative quality often breaks down at smaller scales: a tiny model is more likely to turn you into a distant cousin than your doppelgÃ¤nger. To solve this, I engineered a two-step workflow that preserves subject identity while regenerating complex backgrounds and enforcing visual consistency (with some fun filters included). I initially benchmarked &lt;a href="https://huggingface.co/segmind/tiny-sd" target="_blank" rel="noopener">Segmind&amp;rsquo;s Tiny (~1GB) stable diffusion model&lt;/a> due to its tiny memory footprint, but it couldn&amp;rsquo;t generate high-quality outputs under our strict timing and hardware constraints.&lt;/p>
&lt;p>The result is a fully on-device image transformation playground that tests the best of open-source conditional image generation.&lt;/p>
&lt;figure style="display: flex; flex-direction: column; align-items: center; text-align: center;">
&lt;div style="display: flex; justify-content: center; gap: 10px; align-items: flex-end;">
&lt;div style="width: 80%; display: flex; flex-direction: column;">
&lt;img src="images/app-preview-1.PNG" style="width: 100%; height: auto; object-fit: contain;">
&lt;div style="margin-top: 5px; font-size: 0.75em;">(a)&lt;/div>
&lt;/div>
&lt;div style="width: 80%; display: flex; flex-direction: column;">
&lt;img src="images/app-preview-2.PNG" style="width: 100%; height: auto; object-fit: contain;">
&lt;div style="margin-top: 5px; font-size: 0.75em;">(b)&lt;/div>
&lt;/div>
&lt;/div>
&lt;figcaption style="margin-top: 15px;">
&lt;strong>Figure 2.&lt;/strong> My custom iOS app. In the &lt;a href="https://www.gettyimages.com/detail/news-photo/sabrina-carpenter-at-the-2024-governors-ball-held-at-news-photo/2156108404">original image&lt;/a>, Sabrina Carpenter is performing at the 2024 Governors Ball in Queens, New York. My stable diffusion pipeline successfully transports her to the interior of Balboa Park in San Diego, California. One final filter applies a stylized aesthetic, transforming the result into an album-cover candidate.
&lt;/figcaption>
&lt;/figure>
&lt;p>For the full story, including technical details, check out my corresponding &lt;strong>&amp;ldquo;Frames Per Second&amp;rdquo; blog series&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../../post/edge-diffusion-1/">Part 1: The Hunt for a Tiny, High-Quality Diffusion Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-diffusion-2/">Part 2: Quantization, Kernels, and the Path to On-Device Diffusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-diffusion-2/">Part 3: Turning a Tiny Diffusion Model into a Traveling Photobooth&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The complete source code, benchmarks, and project notes are available on &lt;a href="https://github.com/bellanich/pocket-diffusion" target="_blank" rel="noopener">GitHub&lt;/a>. Sometimes it only takes a few frames per second to generate the image you want: no cloud, no fuss, no hassle.&lt;/p></description></item><item><title>Transformers Decoded: A Quick Guide to ML Research</title><link>https://bellanich.github.io/portfolio/transformers-handbook/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/transformers-handbook/</guid><description>&lt;p>I&amp;rsquo;ve had a whirlwind introduction to world of LLMs. Between &lt;a href="../edge-llm/">my adventure in embedding a multi-modal foundation model on various iOS devices&lt;/a> and jetting of to &lt;a href="https://neurips.cc/Conferences/2024" target="_blank" rel="noopener">NeurIPS 2024 in Vancouver&lt;/a> ðŸ‡¨ðŸ‡¦, Iâ€™ve been immersed in all things related to LLMs. I&amp;rsquo;ve spent countless hours talking to the researchers building state-of-the-art LLMs and even more time pouring over online resources to understand the fundamentals.&lt;/p>
&lt;p>To save you the trouble, I&amp;rsquo;ve organized everything I&amp;rsquo;ve learned into &lt;a href="https://bellanich.github.io/media/transformers_handbook.pdf"> a handy study guide: &amp;ldquo;Transformers Decoded: A Quick Guide to ML Research&amp;rdquo; &lt;/a>. This guide explains key concepts, industry trends, and the critical optimization techniques that make LLMs so performant. We pay close attention to crucial inference optimization techniques, including:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Speculative decoding&lt;/strong>: Inference is accelerated by generating multiple candidate outputs with a smaller LLM and verifying them with a larger, more accurate LLM.&lt;/li>
&lt;li>&lt;strong>Flash attention&lt;/strong>: By restructuring the attention operations to minimize memory reads and writes â€” the bottleneck in classical attention â€” we achieve significant speedups.&lt;/li>
&lt;li>&lt;strong>Continuous batching&lt;/strong>: Incoming requests are dynamically batched to maximize hardware utilization and throughput, especially in online serving.&lt;/li>
&lt;/ul>
&lt;p>Further details on these (and other) optimization techniques can be found the PDF below. Let&amp;rsquo;s decode the latest in AI research, together!&lt;/p>
&lt;!-- Load local file -->
&lt;embed src="https://bellanich.github.io/media/transformers_handbook.pdf" width="700" height="550" type="application/pdf"></description></item><item><title>Shrinking the Impossible: Deploying My Own Multi-Modal Edge Foundation Model</title><link>https://bellanich.github.io/portfolio/edge-llm/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/edge-llm/</guid><description>&lt;p>Generative AI tooling has become a staple for everyday tasks â€” from creating presentation visuals to finding the right code syntax. But I&amp;rsquo;ve always felt a little too uneasy trusting cloud-hosted LLMs with my private chats.&lt;/p>
&lt;p>As an ML engineer, I assumed that the only alternative was spinning up an expensive private LLM in the cloud â€” until &lt;a href="https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/" target="_blank" rel="noopener">the Google I/O Connect event (June 2024)&lt;/a>. Thatâ€™s where Google revealed their smallest LLM to date, &lt;a href="https://deepmind.google/technologies/gemini/nano/" target="_blank" rel="noopener">Gemini Nano&lt;/a>, running directly &lt;a href="https://www.androidauthority.com/how-to-get-gemini-nano-on-pixel-8-8a-3450466/" target="_blank" rel="noopener">on the Pixel 8&lt;/a>. Seeing an ultra-private AI assistant on edge was inspiring, but for solo developers like me, the open-source tools werenâ€™t quite ready yet.&lt;/p>
&lt;p>Fast-forward six months, and the landscape has changed. Thanks to projects like &lt;a href="https://llm.mlc.ai" target="_blank" rel="noopener">the Machine Learning Compiler (MLC) framework&lt;/a>, solo developers can now optimize and deploy powerful LLMs on edge devices. Rather than sticking to a unimodal LLM â€” a well-worn path â€” I set my sights higher: deploying a multimodal LLM on the smallest edge devices possible.&lt;/p>
&lt;p>The end result? I successfully embedded &lt;a href="https://huggingface.co/google/gemma-2-2b-it" target="_blank" rel="noopener">the multilingiual Gemma 2B&lt;/a> and &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener">the multi-modal LLaVA-OneVision Qwen2 0.5B&lt;/a> models on my laptop (among other devices). Take a look for yourself:&lt;/p>
&lt;img src="images/demo.gif">
&lt;p>As shown, my embedded model can comfortably discuss the content of an image and give some interesting synopses. Pretty impressive, right? Hereâ€™s a look at how it all works:&lt;/p>
&lt;figure>
&lt;img src="images/model_flowchart.png">
&lt;figcaption>
At 800M parameters, the &lt;a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision Qwen2 0.5B&lt;/a> is ideal for edge deployment but it struggles with complex user instructions. Thus, I used Googleâ€™s ultra-efficient &lt;a href="https://huggingface.co/google/gemma-2b">Gemma 2B model&lt;/a> as a fallback.
&lt;/figcaption>
&lt;/figure>
&lt;p>For the full story, including technical details, check out my corresponding &lt;strong>&amp;ldquo;Shrinking the Impossible&amp;rdquo; blog series&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="../../post/edge-llm-mlc/">Part 1: Optimizing Foundation Models for Edge Devices with MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-vision-encoders/">Part 2: Teaching Chatbots to See with LLaVA, CLIP, and SigLIP&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-embed-llava/">Part 3: Embedding a Custom-Defined LLaVA-OneVision Model with MLC&lt;/a>&lt;/li>
&lt;li>&lt;a href="../../post/edge-llm-app/">Part 4: Deploying My Own Pocket-Sized Multi-Modal Large Language Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The source code is also available for your viewing pleasure on &lt;a href="https://github.com/bellanich/pocket-llm" target="_blank" rel="noopener">GitHub&lt;/a>. Consider it your guide to shrinking impossibly large LLMs down to something that fits inside your pocket.&lt;/p></description></item><item><title>Python ML Template</title><link>https://bellanich.github.io/portfolio/python-ml-template/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/python-ml-template/</guid><description>&lt;p>In the past month, I&amp;rsquo;ve set up 5 different Git repositories from scratch. Each time, I found myself navigating the trade-off between creating a clean project setup and getting started quickly. While software engineering is all about navigating trade-offs, it&amp;rsquo;s also about crafting solutions that rise above them altogether.&lt;/p>
&lt;p>In this case, I found myself asking: &amp;ldquo;How few commands can I really get away with to kickstart a new machine learning (ML) project&amp;rdquo;? Well, from putting together &lt;a href="https://github.com/bellanich/python-ml-template" target="_blank" rel="noopener">this Python ML Template repository&lt;/a>, my answer is three. (See the demo above for a quick example.)&lt;/p>
&lt;p>If you&amp;rsquo;re curious about my implementation, checkout &lt;a href="https://github.com/bellanich/python-ml-template/blob/main/README.md" target="_blank" rel="noopener">my project&amp;rsquo;s README file&lt;/a> and &lt;a href="https://github.com/bellanich/python-ml-template/blob/main/docs/0_overview.md" target="_blank" rel="noopener">documentation&lt;/a>. Happy ML feature development!&lt;/p></description></item><item><title>Git It Right: A Complete Cheatsheet</title><link>https://bellanich.github.io/portfolio/git-cheatsheet/</link><pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate><guid>https://bellanich.github.io/portfolio/git-cheatsheet/</guid><description>&lt;p>As a Machine Learning engineer, I use Git daily, but even useful commands can feel cryptic at times. I struggled to find a cheatsheet that bridged the gap between theory and real-world application. That&amp;rsquo;s why I created &lt;a href="https://nbviewer.org/github/bellanich/git-cheatsheet/blob/1dd32689009de1fdb48b49bbe2c3d437355fad91/git_cheatsheet.pdf" target="_blank" rel="noopener">my own cheatsheet&lt;/a> â€“ a resource that demystifies core concepts alongside practical commands.&lt;/p>
&lt;p>Download the PDF below, or grab &lt;a href="https://github.com/bellanich/git-cheatsheet/blob/main/README.md" target="_blank" rel="noopener">the source code&lt;/a> to customize it for your needs. Let&amp;rsquo;s up our Git game together!&lt;/p>
&lt;!-- Use nbviewer.org to automatically download and embed Git Cheatsheet directly from GitHub -->
&lt;p>&lt;embed src="git_cheatsheet.pdf" width="700" height="550"
type="application/pdf">&lt;/p></description></item></channel></rss>