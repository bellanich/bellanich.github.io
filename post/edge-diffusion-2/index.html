<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="This post unpacks how quantization, ANE-optimized kernels, and smart schedulers shrink a 6GB diffusion model into a fast, mobile-ready package."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-diffusion-2/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-diffusion-2/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-diffusion-2/"><meta property="og:title" content="Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion | Bella Nicholson"><meta property="og:description" content="This post unpacks how quantization, ANE-optimized kernels, and smart schedulers shrink a 6GB diffusion model into a fast, mobile-ready package."><meta property="og:image" content="https://bellanich.github.io/post/edge-diffusion-2/featured.jpeg"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-diffusion-2/featured.jpeg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-11-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-25T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-diffusion-2/"},"headline":"Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion","image":["https://bellanich.github.io/post/edge-diffusion-2/featured.jpeg"],"datePublished":"2025-11-25T00:00:00Z","dateModified":"2025-11-25T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"This post unpacks how quantization, ANE-optimized kernels, and smart schedulers shrink a 6GB diffusion model into a fast, mobile-ready package."}</script><title>Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Nov 25, 2025
</span><span class=middot-divider></span>
<span class=article-reading-time>14 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:483px><div style=position:relative><img src=/post/edge-diffusion-2/featured_huf76651b1fb1c766d235d8a62c40d643e_700303_720x0_resize_q98_lanczos.jpeg alt class=featured-image>
<span class=article-header-caption>Image credit: <a href=https://blog.google/technology/ai/nano-banana-pro/ target=_blank rel=noopener><strong>Nano Banana Pro</strong></a></span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#apples-optimization-strategy>Apple&rsquo;s Optimization Strategy</a><ul><li><a href=#model-compression>Model Compression</a><ul><li><a href=#phase-1-quantization>Phase 1: Quantization</a></li><li><a href=#phase-2-palettization>Phase 2: Palettization</a></li></ul></li><li><a href=#neural-engine-optimization>Neural Engine Optimization</a></li><li><a href=#model-chunking>Model Chunking</a></li><li><a href=#attention-variants>Attention Variants</a><ul><li><a href=#1-the-original-attention-mechanism>1. The Original Attention Mechanism</a></li><li><a href=#2-split-einsum-attention>2. Split Einsum Attention</a></li></ul></li><li><a href=#scheduler-optimization>Scheduler Optimization</a></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#whats-next>What&rsquo;s next?</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>After tackling text with <a href=../../portfolio/edge-llm/>my edge multi-modal LLM project</a> last year, I&rsquo;ve become fascinated with the image side of foundation models. The media frenzy ignited by <a href=https://deepmind.google/models/gemini-image/ target=_blank rel=noopener>Google DeepMind&rsquo;s initial Nano Banana release</a> acts as a testament to how image generation just hits differently than text.</p><p>This begs the ultimate question: Can we deliver a Nano Banana-like experience on the edge? The answer isn&rsquo;t simple. Diffusion models are brutally sensitive to noise. In <a href=../edge-diffusion-1/>my last blog post</a>, <a href=../edge-diffusion-1/#tiny-sd-starting-as-small-as-possible>my naive port of Tiny SD failed spectacularly</a>, yielding noisy and psychedelic outputs.</p><p>As a result, I&rsquo;ve turned my attention to <a href=https://huggingface.co/apple/coreml-stable-diffusion-v1-5 target=_blank rel=noopener>Apple&rsquo;s CoreML Stable Diffusion</a> model, betting its proprietary, hardware-aware design will save this project. How did Apple&rsquo;s engineers successfully squeeze a 6GB model into a sub-2GB, sub-10-second iOS package while maintaining quality? Their secret lies in the careful orchestration of quantization, hardware co-design, and architectural compromises.</p><figure><img src=images/coreml-quick-test.png style=width:40%;height:auto><figcaption><strong>Figure 1.</strong> Prototyping testing. A generated image from the text prompt <code>"A beautiful landscape with mountains and a lake, golden hour lighting"</code> using Apple’s CoreML Stable Diffusion model. The output is a high-quality, realistic rendering of a rustic mountain range at sunset.</figcaption></figure><p>This post dissects exactly how Apple pulled off that optimization miracle. By examining which layers got quantized, how Neural Engine constraints shaped the architecture, and where the remaining quality trade-offs live, we&rsquo;ll be ready to build our edge conditional image generation experience in <a href=../edge-diffusion-3/>this blogpost series&rsquo;s conclusion</a>.</p><h2 id=apples-optimization-strategy>Apple&rsquo;s Optimization Strategy</h2><p>Let&rsquo;s start with the numbers. Apple took <a href=https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 target=_blank rel=noopener>Runway ML&rsquo;s 6GB Stable Diffusion</a> implementation, compressed it by over 70%, and then embedded it onto their proprietary Apple Neural Engine (ANE) hardware. The ANE is the third processor in Apple Silicon, sitting alongside the standard CPU and GPU. It&rsquo;s a dedicated Neural Processing Unit (NPU) purpose-built for the high-throughput matrix operations that define neural network inference.</p><figure><img src=images/original-sd-demo.png style=width:50%;height:auto><figcaption><strong>Figure 2.</strong> An image by generated by the Runway ML's 6GB Stable Diffusion model, which served as the base model for Apple's CoreML implementation (<a href=https://deepinfra.com/runwayml/stable-diffusion-v1-5/api>image credit</a>).</figcaption></figure><figure><img src=images/coreml_readme.png style=width:100%;height:auto><figcaption><strong>Figure 3.</strong> Sample images from Apple's official CoreML Stable Diffusion demo. As seen, there's some quality degradation when compared to Fig. 2, but the majority of model quality is preserved (<a href=https://github.com/apple/ml-stable-diffusion>image credit</a>).</figcaption></figure><p>This hyper-specialized hardware enabled Apple to achieve blisteringly fast results: each denoising step takes only ~0.37–0.39 seconds and a high-quality images are generated within 20 steps (documented <a href=https://github.com/apple/ml-stable-diffusion target=_blank rel=noopener>here</a>). Of course, Apple didn&rsquo;t use a single optimization technique to achieves these impressive runtimes; rather, they employed an entire optimization playbook:</p><ol><li><p><strong>Model precision reduction.</strong> Apple applies a crucial two-step precision reduction process of <strong>(a)</strong> initial quantization from <code>float32</code> to <code>float16</code> to halve memory consumption and <strong>(b)</strong> followed by aggressive palettization to <code>6-bit</code> weights to yield another ~40-50% reduction. That&rsquo;s how we went from a 6GB model to a 1.5GB one.</p></li><li><p><strong>Neural Engine optimization.</strong> CoreML&rsquo;s ANE-specific compilation pipeline fuses common machine learning operators and optimizes tensor memory layouts for the ANE&rsquo;s specialized compute units. <a href=https://machinelearning.apple.com/research/neural-engine-transformers target=_blank rel=noopener>Apple&rsquo;s own benchmarks</a> show that ANE-optimized models achieve up to 10× speedup with 14× reduction in peak memory consumption compared to unoptimized implementations on the iPhone 13.</p></li><li><p><strong>Model chunking.</strong> The pipeline is split into four independently loadable components (<code>.mlmodelc</code> files). iOS dynamically swaps these components in the <code>reduceMemory</code> option to keep peak memory usage below the 2GB limit. The trade-off is increased end-to-end latency due to this just-in-time loading overhead as seen in their <a href=https://github.com/apple/ml-stable-diffusion#performance-benchmark target=_blank rel=noopener>official benchmarks</a>.</p></li><li><p><strong>Attention implementation.</strong> Apple uses a <a href=https://machinelearning.apple.com/research/neural-engine-transformers target=_blank rel=noopener>SPLIT_EINSUM attention variant</a>. It breaks multi-head attention into explicit single-head functions and relies on einsum operations to avoid the reshape and transpose steps that trigger memory copies. Because the ANE excels at fixed 4D tensor layouts, keeping data in this shape is crucial. Combined with Apple’s other optimizations, this approach delivered up to <a href=https://machinelearning.apple.com/research/neural-engine-transformers target=_blank rel=noopener>10× faster inference and 14× lower peak memory on their distilbert benchmark</a>.</p></li><li><p><strong>Scheduler Optimization.</strong> The original model was used <a href=https://arxiv.org/abs/2202.09778 target=_blank rel=noopener>the PNDM (Pseudo-numerical methods for Denoising Models) scheduler</a>, which requires 50+ computational steps. Apple swapped this for the modern <a href=https://arxiv.org/abs/2211.01095 target=_blank rel=noopener>DPMSolverMultistepScheduler</a>. PNDM uses estimates from recent steps to predict the denoising direction, allowing it to learn from recent history to make smarter jumps. This drastically reduces the required denoising steps from <a href=https://arxiv.org/abs/2211.01095 target=_blank rel=noopener>50+ to 20-25 without sacrificing image quality</a>.</p></li></ol><p>Each optimization technique targets a different performance bottleneck, whether it be memory footprint, inference latency, or peak memory usage. Let&rsquo;s now examine each technique in detail.</p><h3 id=model-compression>Model Compression</h3><p>Apple engineers pulled off an impressive 70% size reduction through a two-phase compression strategy. They started with a straightforward precision reduction from <code>float32</code> to <code>float16</code> and followed up with palettization to jump from <code>float16</code> to just <code>6-bit</code>.</p><h4 id=phase-1-quantization>Phase 1: Quantization</h4><p>Model parameters in standard PyTorch models consume 32 bits (float32), making the 1.3B parameter Stable Diffusion consume 5.2GB of RAM. In <strong>quantization</strong>, we lower the bit-precision used to store each weight, sacrificing some information for a smaller memory footprint.</p><p>The first step is to halve the precision to <code>float16</code>. CoreML achieved this using simple datatype casting (no algorithmic quantization, clustering, or lookup tables), which instantly halves memory with minimal accuracy loss. This simple technique works because
<a href=https://apple.github.io/coremltools/docs-guides/source/quantization-neural-network.html target=_blank rel=noopener>float16 is considered the &ldquo;safe&rdquo; floor</a> for neural networks. This precision level retains enough dynamic range (the span of representable values) and precision to accurately encode most model weights.</p><p>This reduces memory consumption from 5.2GB to 2.6GB, which is a great start but above my sub-2GB target. If we naively lower the precision any further, we risk catastrophic information loss and would get corrupted outputs like those from my Tiny SD port. Meaning, we need a more clever quantization technique.</p><h4 id=phase-2-palettization>Phase 2: Palettization</h4><p>Apple wanted to build a generalizable framework for compress <em>any</em> Stable Diffusion checkpoint, including community fine-tunes. Hence, they needed a flexible approach that didn&rsquo;t depend on access to the original training data. This immediately rules out state-of-the-art methods like <a href=https://arxiv.org/abs/2306.00978 target=_blank rel=noopener>AWQ</a> or <a href=https://arxiv.org/abs/2210.17323 target=_blank rel=noopener>GPTQ</a>, which require calibration data to analyze activations and identify the most salient (important) weights.</p><p><a href=https://arxiv.org/abs/1510.00149 target=_blank rel=noopener>Palettization</a> offers a perfect alternative. It achieves aggressive compression without calibration data by using simple, interpretable k-means clustering. This technique actually originates from <a href=https://dl.acm.org/doi/10.1145/800064.801294 target=_blank rel=noopener>color quantization</a> in computer graphics. Essentially, during image compression, we needed to map millions of possible colors in an image to a smaller fixed set of representative values.</p><figure><img src=images/color-quantization.png style=width:70%;height:auto><figcaption><strong>Figure 4.</strong> An example of color quantization, where the original photograph palette was reduced seven distinct colors (<a href=https://demonstrations.wolfram.com/ColorQuantizationOfPhotographicImagesIPaletteFromColorsInThe/>image credit</a>).</figcaption></figure><p>The same logic can be applied to model weights. Here&rsquo;s how it works:</p><ol><li>Analyze the distribution of weights in each layer</li><li>Cluster similar weights using k-means to create a &ldquo;palette&rdquo; of representative values (e.g., 64 values for 6-bit palettization, since $2^6 = 64$)</li><li>Replace each original weight with an index pointing to its nearest palette entry</li><li>Store the compact palette plus the many small indices instead of full-precision weights</li></ol><p>The bit-width determines how many palette entries you get, and thus your compression ratio. Table 1 summarizes the trade-offs in palette entry number selection.</p><style>.centered-table{display:flex;flex-direction:column;align-items:center;margin:2rem 0}.centered-table table{border-collapse:collapse}.centered-table th,.centered-table td{padding:.5rem 1rem;border:1px solid #ddd}.centered-table figcaption{margin-top:.5rem;font-style:normal;text-align:center}</style><div class=centered-table><table><thead><tr><th>Bit Width</th><th>Palette Entries</th><th>Compression vs Float16</th><th>Quality Impact</th></tr></thead><tbody><tr><td>8-bit</td><td>256</td><td>~2×</td><td>Minimal quality loss</td></tr><tr><td>6-bit</td><td>64</td><td>~2.67×</td><td>Acceptable quality trade-off</td></tr><tr><td>4-bit</td><td>16</td><td>~4×</td><td>Noticeable degradation</td></tr><tr><td>2-bit</td><td>4</td><td>~8×</td><td>Severe quality issues</td></tr></tbody></table><figcaption><strong>Table 1. </strong>Palettization bit-width options and their associated trade-offs.</figcaption></div><p>For our selected CoreML Stable Diffusion variant, <a href=https://huggingface.co/apple/coreml-stable-diffusion-v1-5-palettized target=_blank rel=noopener>Apple used 6-bit palettization</a> to achieve a final ~1.5GB model size. For even larger models like <a href=https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 target=_blank rel=noopener>SDXL</a> (6.94 GB), Apple used <a href=https://github.com/apple/ml-stable-diffusion/blob/main/README.md#compression-lower-than-6-bits target=_blank rel=noopener>mixed-bit palettization</a> for stronger model compression. Here, we assign different bit-widths (1, 2, 4, 6, or 8 bits) to different layers based on a sensitivity analysis.</p><h3 id=neural-engine-optimization>Neural Engine Optimization</h3><p>Apple&rsquo;s Neural Engine (NE) debuted <a href=https://machinelearning.apple.com/research/neural-engine-transformers target=_blank rel=noopener>in 2017 inside the iPhone X&rsquo;s A11 Bionic chip</a> to power Face ID. <a href=https://support.apple.com/en-us/102381 target=_blank rel=noopener>The TrueDepth camera fires over 30,000 infrared dots</a> to map your face, and handles that data in real time. That stream in real time was too slow and too power-hungry for the GPU, pushing Apple to develop their own NPU.</p><p>That <a href=https://machinelearning.apple.com/research/neural-engine-transformers target=_blank rel=noopener>first-gen ANE delivered 0.6 teraflops of float16</a> compute. By 2018, with release of <a href=https://www.apple.com/newsroom/2018/09/iphone-xs-and-iphone-xs-max-bring-the-best-and-biggest-displays-to-iphone/ target=_blank rel=noopener>the A12 chip</a> and <a href=https://devstreaming-cdn.apple.com/videos/wwdc/2017/710vxa4hl8hyb72/710/710_core_ml_in_depth.pdf target=_blank rel=noopener>Core ML</a>, Apple opened the Neural Engine to developers, and today it’s baked into every modern iOS device.</p><figure><img src=images/ane-tflops-stats.png style=width:80%;height:auto><figcaption><strong>Figure 5.</strong> The evolution of the Apple Neural Engine from 2017 to 2021. The 16-core Neural Engine on on the A15 Bionic chip (iPhone 13 Pro) has a peak throughput 26 times higher than its original counterpart. (<a href=https://machinelearning.apple.com/research/neural-engine-transformers>image credit</a>).</figcaption></figure><p>But where does <a href=https://developer.apple.com/documentation/coreml target=_blank rel=noopener>Core ML</a> fit into all this? Since ANE is proprietary hardware, there’s no public API to program it directly. Its architecture, instruction set, and compiler are all trade secrets. With no official documentation on ANE-supported operations or optimization methods, most developer knowledge comes from <a href=https://github.com/hollance/neural-engine target=_blank rel=noopener>trial-and-error and reverse engineering</a>. Core ML is the only way iOS developers can access the Neural Engine.</p><p>It consists of two parts:</p><ol><li><a href=https://opensource.apple.com/projects/coreml-tools/ target=_blank rel=noopener>coremltools</a> is an open source Python package that converts models from frameworks like PyTorch and TensorFlow into Core ML&rsquo;s optimized format</li><li>The on-device Core ML framework that loads these compiled models and executes them.</li></ol><p>When you convert a model with <code>coremltools</code>, it figures out which operations can run on the ANE versus the CPU or GPU, applies optimizations, and compiles the model into an efficient format. At runtime, Core ML then routes each operation to the right compute unit to maximize performance and minimize power use.</p><p>CoreML gives you three ways to run unit neural networks on device:</p><ol><li><p><strong>CPU Only.</strong> This is the slowest but most safest option. According to the <a href=https://onnxruntime.ai/docs/execution-providers/CoreML-ExecutionProvider.html#coreml_flag_use_cpu_only target=_blank rel=noopener>ONNX Runtime documentation</a>, CPU-only mode is mainly available for debugging and validation, since it avoids precision differences and guarantees predictable results. Community benchmarks suggest it runs approximately <a href=https://mybyways.com/blog/faster-stable-diffusion-on-mseries-macs target=_blank rel=noopener>7-8x slower</a> than optimal configurations, making it impractical for real-time generation.</p></li><li><p><strong>CPU and GPU.</strong> This combination is capable but not recommended. GPUs were originally built for desktops with unlimited power, so they’re plausible but not ideal for running heavy models on mobile devices. It&rsquo;s typically used for Macs with powerful GPUs or as a fallback for older devices without a Neural Engine.</p></li><li><p><strong>CPU ane ANE.</strong> This is Apple&rsquo;s recommended configuration for deploying intensive models on iPhones and iPads. ANE was specifically designed for ML inference workloads and delivers comparable performance to GPU at a fraction of the power consumption.</p></li></ol><p>In our case, I configured <code>coreml-stable-diffusion-v1-5-palettized</code> to run primarily on the Neural Engine with CPU fallback for unsupported operations. This hybrid approach maximizes performance where it counts while maintaining graceful degradation for edge cases.</p><h3 id=model-chunking>Model Chunking</h3><p>iOS enforces stricter memory constraints than macOS. As noted in <a href=https://apple.github.io/coremltools/docs-guides/source/opt-stable-diffusion.html target=_blank rel=noopener>Apple&rsquo;s CoreML optimization guide</a>,</p><p>ANE&rsquo;s specialized architecture comes with strict model size constraints. iOS enforces stricter per-file memory mapping limits than macOS. While the exact limit is undocumented, <a href=https://apple.github.io/coremltools/docs-guides/source/opt-stable-diffusion.html target=_blank rel=noopener>Apple&rsquo;s optimization guide</a> suggests it&rsquo;s around 1GB based on their compression targets for mobile deployment. Meaning, attempting to load our U-Net component, which is roughly 1.5GB in float16 precision, will iPhone triggers memory allocation failure, even if it&rsquo;d run perfectly on a Mac.</p><p>This makes <a href="https://github.com/apple/ml-stable-diffusion?tab=readme-ov-file#-converting-models-to-core-ml" target=_blank rel=noopener>model chunking</a> essential for mobile deployment. The idea is simple: we split huge weight files into smaller slices that fit within iOS’s memory limits, and let the runtime load each slice on demand. Apple&rsquo;s <code>ml-stable-diffusion</code> repo handles this automatically with the <a href=https://github.com/apple/ml-stable-diffusion target=_blank rel=noopener><code>--chunk-unet</code> conversion flag</a> flag, which divides the U-Net weights into multiple files that stay well under the limit. These chunks are stored in the <code>.mlmodelc</code> format, a pre-compiled, ANE-optimized layout that improve loading time.</p><p>The beauty of Apple&rsquo;s setup is that developers never have to think about model chunking. Core ML handles this behind the scenes. While there&rsquo;s a small cost to pulling in multiple files, we wouldn&rsquo;t be able to run Stable Diffusion on iOS without this approach.</p><h3 id=attention-variants>Attention Variants</h3><p>Apple&rsquo;s CoreML conversion tools offer two attention implementations that compute identical mathematical operations but differ critically in their kernel implementation.</p><h4 id=1-the-original-attention-mechanism>1. The Original Attention Mechanism</h4><p>This implementation uses the standard batched multi-head attention formula:</p><pre><code class=language-python># Shape: [batch, seq_len, heads * head_dim]
Q, K, V = linear_projections(x)

# Reshape to [batch, heads, seq_len, head_dim]
Q = Q.reshape(batch, seq_len, heads, head_dim).transpose(1, 2)
K = K.reshape(batch, seq_len, heads, head_dim).transpose(1, 2)
V = V.reshape(batch, seq_len, heads, head_dim).transpose(1, 2)

# Batched matrix multiplication across all heads
# [batch, heads, seq_len, head_dim]
attention = softmax(Q @ K.transpose(-2, -1) / sqrt(d_k)) @ V

# Reshape back
output = attention.transpose(1, 2).reshape(batch, seq_len, heads * head_dim)
</code></pre><p style=text-align:center;font-size:.75em;margin-top:.5em;color:#66><strong>Figure 6.</strong> Pseudocode of the <a href=https://arxiv.org/abs/1706.03762>original attention</a> mechanism.</p><p>This works well on CPUs and GPUs, which handle dynamic reshaping efficiently. It&rsquo;s <a href=https://github.com/apple/ml-stable-diffusion#performance-benchmark target=_blank rel=noopener>faster on Macs with discrete GPUs</a> (M1 Pro/Max/Ultra) where memory bandwidth isn&rsquo;t the primary bottleneck.</p><h4 id=2-split-einsum-attention>2. Split Einsum Attention</h4><p>ANE penalizes non-contiguous memory access, which makes the reshape/transpose operations shown in Figure 6 computationally expensive. Fortunately, we can rewrite matrix multiplication as a series of <a href=https://mathworld.wolfram.com/EinsteinSummation.html target=_blank rel=noopener>Einstein summations</a> (einsums) as shown in Equation 1 to better utilize ANE.
$$
C_{ik} = \sum_{j} A_{ij} B_{jk} = AB = C
\tag{1}
$$</p><p>By keeping keeps tensors in fixed 3D layouts and using the einsum operation, we avoid generating unnecessary memory copies. The implementation looks something like this:</p><pre><code class=language-python># Shape: [batch, seq_len, heads * head_dim]
Q, K, V = linear_projections(x)

# Split into explicit per-head tensors (no reshape)
Q_heads = [Q[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)]
K_heads = [K[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)]
V_heads = [V[:, :, i*head_dim:(i+1)*head_dim] for i in range(heads)]

# Compute attention per head using einsum (preserves 3D tensor layout)
outputs = []
for Q_h, K_h, V_h in zip(Q_heads, K_heads, V_heads):
    # [batch, seq_len, seq_len]
    scores = torch.einsum('bqd,bkd-&gt;bqk', Q_h, K_h) / sqrt(head_dim)
    attn = softmax(scores, dim=-1)
    out = torch.einsum('bqk,bkd-&gt;bqd', attn, V_h)  # [batch, seq_len, head_dim]
    outputs.append(out)

# Concatenate (cheap operation)
output = torch.cat(outputs, dim=-1)  # [batch, seq_len, heads * head_dim]
</code></pre><p style=text-align:center;font-size:.75em;margin-top:.5em;color:#66><strong>Figure 7.</strong> Pseudocode of the <a href=https://machinelearning.apple.com/research/neural-engine-transformers>einsum attention variant</a>.</p><p>The trade-off is lower parallelism, since we&rsquo;re using explicit per-head loops rather than batched operations. This hurts GPU performance, but <a href=https://machinelearning.apple.com/research/neural-engine-transformers target=_blank rel=noopener>ANE performance is bottlenecked by memory bandwidth</a>. Meaning, the memory savings outweigh the costs of reduced parallelism.</p><h3 id=scheduler-optimization>Scheduler Optimization</h3><p>Diffusion models turn random static into art by clearing away noise. The <strong>scheduler</strong> (also called a sampler or solver) is the control algorithm that orchestrates the denoising loop: it calls the U-Net at each timestep to predict the noise, then uses its mathematical formula to update the image toward a cleaner state. Meaning, the scheduler controls the number of diffusion steps needed for high-quality image generation. If we select a more efficient scheduler, we can improve inference time without degrading quality.</p><p>The <a href=https://huggingface.co/runwayml/stable-diffusion-v1-5 target=_blank rel=noopener>original Stable Diffusion models</a> used the <a href=https://arxiv.org/abs/2202.09778 target=_blank rel=noopener>PNDM (Pseudo Numerical Methods for Diffusion Models)</a> scheduler, which applies a linear multi-step method:</p><p>$$
x_{t-1} = x_t + \sum_{i=0}^{k-1} \alpha_i \cdot \epsilon_\theta(x_{t-i}, t-i)
\tag{2}
$$</p><p>where $x_t$ is the current noisy image at timestep $t$, $\epsilon_\theta$ predicts what noise to remove, and $\alpha_i$ are coefficients that weight predictions from the past $k$ steps. As seen in Equation 2, PNDM treats each timestep as a discrete prediction problem, where each step uses local information (the last few predictions). In this context, larger jumps (more noise removal per step) risk error accumulation, which lowers image quality. <a href=https://arxiv.org/abs/2202.09778 target=_blank rel=noopener>PNDM tends to require ~50 diffusion steps to yield acceptable outputs.</a></p><p>The <a href=https://arxiv.org/abs/2211.01095 target=_blank rel=noopener>DPMSolverMultistepScheduler</a> treats denoising as a continuous process rather than discrete jumps. Since noise is added gradually during training, it can be removed along a smooth, continuous path that written as an Ordinary Differential Equation (ODE):</p><p>$$
\frac{dx_t}{dt} = f(t) x_t + g(t) \epsilon_\theta(x_t, t)
\tag{3}
$$</p><p>This makes diffusion a continuous process and allows the DPM Solver to take larger, more informed steps through the denoising trajectory. As a result, <a href=https://arxiv.org/abs/2211.01095 target=_blank rel=noopener>25 steps with DPM Solver produces quality comparable</a> to 50 steps with PNDM, offering a 2× speedup.</p><p>This made it the DPM Solver an obvious choice for Apple&rsquo;s CoreML implementation, where every second of latency matters. The step count creates a direct quality-speed trade-off:</p><table><thead><tr><th>Strategy</th><th>Steps</th><th>Runtime</th><th>Quality Impact</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>Aggressive</strong></td><td>15-20</td><td>10-15 seconds</td><td>Noticeable artifacts, loss of fine details</td><td>Quick previews, concept iteration</td></tr><tr><td><strong>Balanced</strong></td><td>20-30</td><td>15-25 seconds</td><td>High-quality results, minimal artifacts</td><td>Production deployment</td></tr><tr><td><strong>Conservative</strong></td><td>50+</td><td>35+ seconds</td><td>Marginal improvement over 25 steps</td><td>Not worth the extra latency on mobile</td></tr></tbody></table><p style=text-align:center;font-size:.75em;margin-top:.5em;color:#66><strong>Table 2.</strong> Trade-off between diffusion steps and image quality when using the DPMSolverMultistepScheduler.</p><p>After extensive testing, I settled on 25 steps as my default to properly balance my need for quality and speed.</p><br><h2 id=conclusion>Conclusion</h2><p>Apple&rsquo;s CoreML Stable Diffusion represents a masterclass in optimization engineering. With aggressive quantization, ANE-friendly attention kernels, and smart scheduling, Apple squeezed a 6GB model into a 1.5GB package that can generate an image in under 10 seconds on an iPhone. It&rsquo;s a technical flex that&rsquo;s hard to overstate.</p><p>But here&rsquo;s an uncomfortable truth: optimization doesn’t expand capabilities. Apple solved <em>how</em> to run Stable Diffusion on mobile — not whether it&rsquo;s good enough. Strip away the speedups, and we&rsquo;re left with the a 2022-era model:</p><ul><li><p><strong>Poor identity preservation.</strong> Try img2img at high denoising strength and watch faces dissolve into uncanny abstractions. The model simply can&rsquo;t maintain coherent identity while delivering image transformation.</p></li><li><p><strong>Prompt adherence is weak.</strong> Compared to <a href=https://stablediffusionxl.com/ target=_blank rel=noopener>SDXL</a> or <a href=https://bfl.ai/models/flux-pro target=_blank rel=noopener>Flux</a>, SD 1.5 treats our carefully crafted prompt more like a vague suggestion than a clear set of instructions.</p></li></ul><p>As a result, I can&rsquo;t just port Apple&rsquo;s approach.</p><h3 id=whats-next>What&rsquo;s next?</h3><p>My goal is conditional edge image generation with an explicit need for character consistency. If Apple&rsquo;s optimizations give us the blueprint for mobile deployment, what architecture actually delivers my required capabilities?</p><p>That&rsquo;s what I&rsquo;ll cover in <a href=../edge-diffusion-3/>my next and final blogpost</a>. My goal isn’t just to run fast; it’s to run fast and look good doing it.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/computer-vision/>Computer Vision</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-diffusion-2/&amp;text=Frames%20Per%20Second%20%28Part%202%29:%20Quantization,%20Kernels,%20and%20the%20Path%20to%20On-Device%20Diffusion" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-diffusion-2/&amp;t=Frames%20Per%20Second%20%28Part%202%29:%20Quantization,%20Kernels,%20and%20the%20Path%20to%20On-Device%20Diffusion" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Frames%20Per%20Second%20%28Part%202%29:%20Quantization,%20Kernels,%20and%20the%20Path%20to%20On-Device%20Diffusion&amp;body=https://bellanich.github.io/post/edge-diffusion-2/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-diffusion-2/&amp;title=Frames%20Per%20Second%20%28Part%202%29:%20Quantization,%20Kernels,%20and%20the%20Path%20to%20On-Device%20Diffusion" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Frames%20Per%20Second%20%28Part%202%29:%20Quantization,%20Kernels,%20and%20the%20Path%20to%20On-Device%20Diffusion%20https://bellanich.github.io/post/edge-diffusion-2/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-diffusion-2/&amp;title=Frames%20Per%20Second%20%28Part%202%29:%20Quantization,%20Kernels,%20and%20the%20Path%20to%20On-Device%20Diffusion" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu72f3a3ef3a65e4417b2658a21b6b7f3a_388362_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-diffusion-3/>Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth</a></li><li><a href=/post/edge-diffusion-1/>Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model</a></li><li><a href=/post/edge-llm-app/>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</a></li><li><a href=/post/edge-llm-embed-llava/>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</a></li><li><a href=/post/edge-llm-vision-encoders/>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>