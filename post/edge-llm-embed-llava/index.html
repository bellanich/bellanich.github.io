<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="Armed with some newfound vision transformer knowledge, we're ready to extend the Machine Learning Compiler framework to support a new, tiny but promising multi-modal model."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-llm-embed-llava/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-llm-embed-llava/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-llm-embed-llava/"><meta property="og:title" content="Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC | Bella Nicholson"><meta property="og:description" content="Armed with some newfound vision transformer knowledge, we're ready to extend the Machine Learning Compiler framework to support a new, tiny but promising multi-modal model."><meta property="og:image" content="https://bellanich.github.io/post/edge-llm-embed-llava/featured.png"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-llm-embed-llava/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-12-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-llm-embed-llava/"},"headline":"Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC","image":["https://bellanich.github.io/post/edge-llm-embed-llava/featured.png"],"datePublished":"2024-12-01T00:00:00Z","dateModified":"2024-12-01T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"Armed with some newfound vision transformer knowledge, we're ready to extend the Machine Learning Compiler framework to support a new, tiny but promising multi-modal model."}</script><title>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Dec 1, 2024
</span><span class=middot-divider></span>
<span class=article-reading-time>17 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:527px><div style=position:relative><img src=/post/edge-llm-embed-llava/featured_hu1483236e0b46293401fbb9b33f1cdac2_907167_720x0_resize_lanczos_3.png alt class=featured-image>
<span class=article-header-caption>Image credit: <a href=https://github.com/black-forest-labs/flux target=_blank rel=noopener><strong>Generated using FLUX</strong></a></span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a><ul><li><a href=#selected-architecture>Selected Architecture</a><ul><li><a href=#why-not-only-use-the-llava-model>Why Not Only Use the LLaVA Model?</a></li></ul></li><li><a href=#the-overall-process>The Overall Process</a></li></ul></li><li><a href=#manual-porting-llava-onevision-to-mlc>Manual Porting LLaVA-OneVision to MLC</a><ul><li><a href=#mlcs-clip-vs-our-selected-llava-siglip-implementation>MLC&rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation</a></li><li><a href=#embedding-aggregation>Embedding Aggregation</a><ul><li><a href=#output-features-explained>Output Features Explained</a></li></ul></li><li><a href=#gelu-approximation>GELU Approximation</a></li><li><a href=#embedding-normalization>Embedding Normalization</a></li><li><a href=#summary>Summary</a></li></ul></li><li><a href=#packaging-our-custom-model>Packaging Our Custom Model</a></li><li><a href=#end-result>End Result</a></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#whats-next>What&rsquo;s next?</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>I attended this year&rsquo;s <a href=https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/ target=_blank rel=noopener>Google I/O Connect in Berlin</a>, and seeing
<a href=https://ai.google.dev/edge target=_blank rel=noopener>Googleâ€™s latest work in Edge AI</a> was inspiring. Since then, I&rsquo;ve been on a personal mission to deploy my own edge model. Given how rapidly the open source community is catching up with the tech giants in edge foundation models, I&rsquo;ve decided test the limits of what I can realistically achieve as a solo developer. Can I embed a <strong>multi-modal</strong> foundation model onto my iPhone?</p><p>In my <a href=../edge-llm-mlc/>first blog post</a>, I&rsquo;ve introduced the <a href=https://llm.mlc.ai target=_blank rel=noopener>Machine Learning Compiler Project</a> as the open source solution that will make this possible. In my <a href=../edge-llm-embed-llava/>last blog post</a>, I&rsquo;ve given you the technical background knowledge needed to successfully deploy a multi-modal foundation model.</p><p>Now, we&rsquo;re going to put this theory into practice in a hands-on activity. I&rsquo;m going to show you how to embed a custom foundation model onto an edge device â€” and all the things that can go wrong in the process.</p><h3 id=selected-architecture>Selected Architecture</h3><p>Remember that my end vision is to have a multi-modal foundation model deployed on my iPhone. Ideally, I want it to be multi-lingual so I can get some help the next time that I&rsquo;m lost in a foreign country.</p><p>At a high-level, my implementation will consist of two different models: <strong>(a)</strong> the smallest <a href=https://llava-vl.github.io target=_blank rel=noopener>Large Language and Vision Assistant (LLaVA)</a> model that I can find, and <strong>(b)</strong> an <a href=https://huggingface.co/google/gemma-2b target=_blank rel=noopener>instruction-tuned version of Gemma 2B</a>.</p><p>I&rsquo;ve chosen these models, since <strong>(a)</strong> the MLC Engine natively supports them, <strong>(b)</strong> they&rsquo;re lightweight enough to be compiled on my laptop, and <strong>(c)</strong> their respective families have earned a reputation as high-performers.</p><p>Each model will work on a specific task. Whenever the end user shares an image with our multi-modal chatbot, the mini LLaVA model will generate a text description for Gemma. Gemma will then use this information to respond to the user&rsquo;s original prompt. If no image is shared, our user will only interact with Gemma 2B. Of course, this model specialization will be abstracted away. Our user won&rsquo;t be aware that they&rsquo;re actually conversing with two different foundation models rather than just one.</p><figure><img src=images/model_flowchart.png><figcaption>My multi-modal chat pipeline consists of two models: <b>(1)</b> the eloquent and multilingual <a href=https://huggingface.co/google/gemma-2b>Gemma2B model</a>, and <b>(2)</b> a mini <a hred=https://llava-vl.github.io/blog/2024-08-05-llava-onevision/>LLaVA-OneVision model</a>. LLaVa will act as a translator for Gemma, generating a text description for any user inputted images.</figcaption></figure><h4 id=why-not-only-use-the-llava-model>Why Not Only Use the LLaVA Model?</h4><p>It may sound a bit strange that we&rsquo;re deploying two models instead of one â€” especially since all LLaVA understand text and images. However, we need to make a distinction between <a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf target=_blank rel=noopener>the smallest LLaVa-OneVision model</a>, which is less than 1B parameters in size, and <a href=https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md target=_blank rel=noopener>its much larger 7B+ counterparts</a>.</p><figure><img src=images/llava_model_stats.png width=60%><figcaption>According to the 0.5B LLaVA-OneVision model card, the LLaVa model is just under 900M parameters in total. Given its 80,000+ model downloads, we can assume it does a decent job at its primary task: annotating images (<a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf>source</a>).</figcaption></figure><p>Larger LLaVA models are perfectly capable of holding a coherent conversation and to understanding whatever images we show them. You can find a few interesting examples of large LLaVA models answering questions about images <a href=https://llava-vl.github.io target=_blank rel=noopener>here</a>.</p><figure><img src=images/large_llava_chat.png><figcaption><a href=https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16>The LLaVA-v1.6 series</a> contains models ranging from 7B to 34B parameters. At these sizes, a LLaVA model can easily identify Leonardo da Vinci's world famous masterpiece from a screenshot and give us a quick art history lesson. Unfortunately, we can't expect the same from the smallest LLaVA models. (<a href=https://llava-vl.github.io>image credit</a>).</figcaption></figure><p>However, we need to adjust our expectations for a 900M parameter model. There&rsquo;s only so much that a small model can do â€” and the LLaVA-OneVision Qwen2 O.5B model has been optimized for image annotation rather than instruction tasks. Meaning, we can probably converse directly with it, but may not be thrilled with the quality of its responses.</p><p>If we wanted to deploy a similar application in a cloud environment, then it would probably just make more sense to quantize a 7B LLaVA model and accept a slightly larger monthly bill. However, since we&rsquo;re working on edge, we have tight resource constraints and need to make do with what we have.</p><h3 id=the-overall-process>The Overall Process</h3><p>For each model, we need to:</p><ol><li>Quantize its weights; and</li><li>Apply hardware-specific optimizations to it.</li></ol><p>How simple this process really is comes down to the degree of built-in MLC Engine support. <a href=https://huggingface.co/mlc-ai/gemma-2b-it-q4f32_1-MLC target=_blank rel=noopener>MLC has already pre-quantized an instruction-tuned version of Gemma 2B for us</a>. Hence, deploying Gemma as a stand alone model in an iOS application is relatively straightforward task. Just follow <a href=https://llm.mlc.ai/docs/get_started/quick_start target=_blank rel=noopener>MLC&rsquo;s Quick Start Documentation for how to package Gemma 2B</a> and <a href=https://llm.mlc.ai/docs/deploy/ios.html#deploy-ios target=_blank rel=noopener>their iOS Swift SDK instructions</a>.</p><p>On the other hand, applying the same process to our LLaVA model is a bit trickier. If we go through <a href=https://huggingface.co/mlc-ai target=_blank rel=noopener>the list of pre-quantized models offered by MLC</a>,(as of November 2024) there is no pre-quantized LLaVA model available â€” much less our desired mini LLaVA model.</p><p>Since this process for deployed a pre-quantized model from <a href=https://huggingface.co target=_blank rel=noopener>HuggingFace</a> so well-documented, I&rsquo;m not going to focus on it. Rather, I&rsquo;ll show you how I ported a new model into the MLC framework.</p><h2 id=manual-porting-llava-onevision-to-mlc>Manual Porting LLaVA-OneVision to MLC</h2><p>At this point, you may be a bit confused. I&rsquo;ve stated the MLC Engine supports LLaVA models, but I&rsquo;m also talking about manually porting the 0.5B LLaVA-OneVision model into the MLC Framework.</p><p>What&rsquo;s going on? At an initial glance, it looks like MLC fully supports <a href=https://llava-vl.github.io target=_blank rel=noopener>the LLaVa model family</a>, but that&rsquo;s only partially true. At the time of writing (November 2024), MLC only natively supports specific LLaVA implementations, more specifically those that use <strong>(a)</strong> use <a href=https://www.llama.com target=_blank rel=noopener>Llama</a> or <a href=https://huggingface.co/mistralai/Mistral-7B-v0.1 target=_blank rel=noopener>Mistral model</a> as its text decoder, and <strong>(b)</strong> use a CLIP-trained vision encoder. Unfortunately, all LLaVA variants that meet these requirements are at 7B+ parameters. Meaning, they&rsquo;re too large for my laptop â€” muchless my smartphone â€” to handle.</p><p>As a result, I need to manually port <a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf target=_blank rel=noopener>the much smaller <code>llava-onevision-qwen2-0.5b-ov-hf</code> model definition</a> into the MLC framework. Practically speaking, this means defining this model in the style used in the <code>mlc-llm-cpu</code> Python library, which is only available through <a href=https://mlc.ai/wheels target=_blank rel=noopener>MLC AI&rsquo;s own Python code repository</a>. Afterwards, we need to then recompile this library locally so that it contains our new model definition.</p><p>Once that&rsquo;s done, I can quantize and hardware-optimize my selected LLaVA model just like any other MLC-supported model. As its full name suggests, the 0.5B LLaVA-OneVision model uses the <a href=https://qwenlm.github.io/blog/qwen2/ target=_blank rel=noopener>Qwen2 0.5B LLM</a> as its text decoder. Fortunately for us, MLC already supports Qwen2 0.5B implementation. Meaning, this change is quite easy. We just copy and paste MLC&rsquo;s definition of LLaVA, rename the files, and change a few key-value pairs:</p><p>The original MLC LLaVA model definition looks like this:</p><pre><code class=language-python>from ..llama.llama_model import LlamaConfig, LlamaForCausalLM
from ..mistral.mistral_model import MistralConfig, MistralForCasualLM


CONFIG_MAP = {
    &quot;LlamaForCausalLM&quot;: LlamaConfig,
    &quot;MistralForCausalLM&quot;: MistralConfig,
}

ARCHITECTURE_MAP = {
    &quot;LlamaForCausalLM&quot;: LlamaForCausalLM,
    &quot;MistralForCausalLM&quot;: MistralForCasualLM,
}
</code></pre><p>Now, we just rewrite our LLaVA-OneVision model definition to support <a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf/blob/main/config.json target=_blank rel=noopener>the LLaVA-OneVision Qwen2 0.5B model&rsquo;s architecture definitions</a>:</p><pre><code class=language-python># Defined by MLC
from ..qwen2.qwen2_model import QWen2Config, QWen2LMHeadModel


CONFIG_MAP = {
    &quot;QWen2LMHeadModel&quot;: QWen2Config,
    &quot;Qwen2ForCausalLM&quot;: QWen2Config
}
ARCHITECTURE_MAP = {
    &quot;QWen2LMHeadModel&quot;: QWen2LMHeadModel,
    &quot;Qwen2ForCausalLM&quot;: QWen2LMHeadModel,
}
</code></pre><p>So far, so good. Let&rsquo;s take a closer look at the <a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf/blob/main/config.json target=_blank rel=noopener>the 0.5B LLaVA-OneVision <code>config.json</code> file</a>. We see that this LLaVA model&rsquo;s vision encoder was trained using SigLIP â€” rather than MLC&rsquo;s natively supported CLIP training framework.</p><figure><img src=images/llava_siglip.png width=70%><figcaption>As seen in the LLaVA-OnVision Qwen2 0.5B model's configuration file, the vision encoder was trained using SigLIP. As of now (November 2024), MLC's only natively supports CLIP-trained vision encoder (<a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf>source</a>).</figcaption></figure><p>Meaning, there&rsquo;s no MLC definition for us to just import. We need to write some custom Python model definition using MLC wrappers. Luckily, we already dived into the details of the SigLIP vision encoder in <a href=../edge-llm-vision-encoders/>the previous blog post</a>. So, we&rsquo;re ready to get started.</p><p>I&rsquo;ve included the final SigLIP vision encoder definition on <a href=https://github.com/bellanich/pocket-llm/tree/main/models/patches target=_blank rel=noopener>in this blogpost series&rsquo;s corresponding GitHub repository</a>. For the sake of brevity, I&rsquo;m just going to focus on the technical differences between these two vision encoders and how these changes translate into code.</p><h3 id=mlcs-clip-vs-our-selected-llava-siglip-implementation>MLC&rsquo;s CLIP vs. Our Selected LLaVA SigLIP Implementation</h3><p>Once again, we&rsquo;re going to reference our LLaVA model&rsquo;s trusty <code>config.json</code> file to get some clues about where to start. In particular, we see the key-value pair: <code>"vision_feature_layer": -1</code>, whereas the original LLaVA config uses <code>"vision_feature_layer": -2</code>. This is a hint about how a vision encoder aggregates its sequence of embeddings into a single vector.</p><h3 id=embedding-aggregation>Embedding Aggregation</h3><p>Both LLaVA models use a Vision Transformer, which outputs a sequence of embeddings. For the training of CLIP and SigLIP, we need a single vector. In this specific Huggingface implementation, CLIP and SigLIP do this aggregation in different ways.</p><p>CLIP uses a <em>class embedding</em>, similar to common adaptations of the <a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html target=_blank rel=noopener>Vision Transformer for classification</a>. Here, we add another token to each image sequence, which is fed through the Transformer along with the image tokens. In the end, we pick the aggregated image feature vector as the output of this classification token. By having the class token part of the self-attention layers, it gives the transformer the ability to aggregate information of the image in this token across its layers. In the implementation, we see this class embedding token being added to the image token sequence:</p><pre><code class=language-python>class CLIPVisionEmbeddings(Module):
    def __init__(self, config: CLIPVisionConfig):
        super().__init__()
        # Class Embedding Token, added to each image token sequence.
        self.class_embedding = nn.Parameter((self.embed_dim,))
        ...

    def forward(self, pixel_values: Tensor) -&gt; Tensor:
        patch_embeds = self.patch_embedding(pixel_values)
        ...
        class_embeds = broadcast_to(
            self.class_embedding, shape=(batch_size, 1, self.embed_dim)
        )
        # Add class embedding token to image token sequence.
        embeddings = concat([class_embeds, patch_embeds], dim=1)
        ...
        return embeddings
</code></pre><p>In contrast, SigLIP pools its output features. Hence, the sequence of image tokens remains unchanged in the input and is fed through the layers. We add on top a pooling layer over the sequence dimension to aggregate all the feature information. This can either be done by a simple averaging, or, in case our specific case, with a multi-head attention pooling. This is similar to our self-attention layers, but just with a fixed query.</p><blockquote><p><strong>Note.</strong> We&rsquo;ve removed the code parts that are common to both models. This simplifies the code and allows us to highlight key differences.</p></blockquote><p>In our SigLIP implementation, shown below, we see as a difference to CLIP that there is no class embedding.</p><pre><code class=language-python>class SiglipVisionEmbeddings(Module):
    def __init__(self, config: SiglipVisionConfig):
        super().__init__()
        ...

    def forward(self, pixel_values: Tensor, interpolate_pos_encoding: bool = False) -&gt; Tensor:
        patch_embeds = self.patch_embedding(pixel_values)
        embeddings = patch_embeds
        ...
        return embeddings
</code></pre><h4 id=output-features-explained>Output Features Explained</h4><p>In <a href=../edge-llm-vision-encoders/>the previous blog post</a>, we discuss how we need a whole sequence of image embeddings for LLaVA rather than a single image feature vector. This provides more detailed information to the decoder.</p><p>While we do not make use of the output heads of CLIP and SigLIP respectively, it does affect which layer we select our features from. This is what the config argument <code>vision_feature_layer</code> ($-1$ for SigLIP and $-2$ for CLIP).</p><p>In other words, we choose the last layer in SigLIP, since the model was trained with image embeddings that are literally the weighted average of all image sequence tokens. Thus, the training process ensures that all these image embeddings have valuable information in them.</p><figure><img src=images/attention_example.svg><figcaption>Attention pooling represents a weighted average pooling, where the weights are determined by the normalized dot product between a static query and the keys per token. While this example shows the averaging for text tokens, it has the same idea with image patch tokens in our vision encoder (<a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html>image credit</a>).</figcaption></figure><pre><code class=language-python>class SiglipVisionModel(Module):
    def __init__(self, config: SiglipVisionConfig):
        super().__init__()
        self.vision_model = SiglipVisionTransformer(config)

    def forward(self, pixel_values: Tensor) -&gt; Tensor:
        # Siglip Qwen2 is using last layer, CLIP pre-last due to different
        # Transformer encoder.
        return self.vision_model(pixel_values)[-1]
</code></pre><p>For CLIP, choosing the last layer is suboptimal, because of the usage of a class token. In the CLIP loss, only the output features of the class token are used. Thus, the output features of all other tokens, namely our image embeddings, were not used. In other words, these features did not receive any gradients during training, and we cannot be sure that the model has stored useful information in them. Most likely, the model has specialized the last layer specifically for the class embedding token, making the outputs of the other tokens (possibly) meaningless.</p><p>Hence, we need to go back one more layer (i.e, the pre-last layer), because these tokens did receive gradients during the training by their dependency on the class token in the last self-attention layer. This ensures that these embeddings have strong features and makes them usable in our LLaVA model implementation.</p><pre><code class=language-python>class CLIPVisionModel(Module):
    def __init__(self, config: CLIPVisionConfig):
        super().__init__()
        self.vision_model = CLIPVisionTransformer(config)

    def forward(self, pixel_values: Tensor) -&gt; Tensor:
        return self.vision_model(pixel_values)[-2]
</code></pre><h3 id=gelu-approximation>GELU Approximation</h3><p>The <a href=https://arxiv.org/abs/1606.08415 target=_blank rel=noopener>Gaussian Error Linear Unit (GELU)</a> is a very popular activation function for Transformers, and is used in both of our CLIP and SigLIP implementations. However, there are some specific details in about how we can implement the GELU activation.</p><p>The &ldquo;true&rdquo;, precise implementation of GELU involves the cumulative distribution function (CDF) of the Gaussian distribution $\Phi(x)$:</p><center><p>$\text{gelu}(x)=x\cdot\Phi(x)$</p></center><p>This CDF is, however, expensive to implement and in particular for edge-devices, where every inference optimization counts, it&rsquo;s sub-optimal. Instead, people commonly use GeLU approximations that are good enough. The standard approximation, often used during training and in frameworks like <a href=https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.gelu.html target=_blank rel=noopener>JAX</a> and <a href=https://pytorch.org/docs/stable/generated/torch.nn.GELU.html target=_blank rel=noopener>PyTorch</a>, is the tanh-approximation:</p><center><p>$\text{gelu}(x)\approx 0.5x\left(1+\tanh\left[\sqrt{\frac{2}{\pi}}(x+0.044715\cdot x^3)\right]\right)$</p></center><p>This is also being used in the Huggingface implementation for SigLIP, and we port it over as shown below:</p><pre><code class=language-python>class QuickGELU(Module):  # SigLIP implementation
    def forward(self, input_tensor: Tensor) -&gt; Tensor:
        c = (2 / math.pi)**0.5
        return 0.5 * input_tensor * (
          1 + tanh(c * (input_tensor + 0.044715 * input_tensor**3))
        )
</code></pre><p>In the MLC implementation of CLIP, another approximation is used. This one involved the sigmoid function, and is simply:</p><center><p>$\text{gelu}(x)\approx x\cdot \sigma(1.702x)$</p></center><p>CLIP</p><pre><code class=language-python>class QuickGELU(Module):
    def forward(self, input_tensor: Tensor) -&gt; Tensor:
        return input_tensor * sigmoid(input_tensor * 1.702)
</code></pre><p>While the Sigmoid GeLU approximation is simpler and even cheaper to calculate, it is also less accurate. Thus, we have to make a tradeoff between efficiency and accuracy. Since we our selected SigLIP vision encoder was trained using the tanh-approximation, we&rsquo;ll stick with it. Differences between <a href=https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/ target=_blank rel=noopener>the GeLU function implementation during training and inference time can cause a slight but noticeable drop in performance</a>.</p><figure style=display:flex;flex-direction:column;align-items:center;text-align:center><div style=display:flex;justify-content:center;gap:10px><img src=images/gelu_approximation_comparison.png style=width:45%>
<img src=images/gelu_approximation_comparison_zoomed.png style=width:45%></div><figcaption>A visualization of the different implementations of the GELU activation function. The original GeLU function is in red, the tanh approximation is in purple, and the sigmoid approximation is in green. As you can see, all of them are quite similar. For the sigmoid activation, there is a noticeable difference for the negative range -1.5 and -4. However, for the tanh approximation, we need to zoom in closely to see the difference, showcasing why the tanh approximation is often used as a close match.</figcaption></figure><h3 id=embedding-normalization>Embedding Normalization</h3><p>Another minor design choice is whether we normalize the embedding features before feeding them into the main Transformer model. Both models use the pre-activation Transformer implementation, which applies a <code>LayerNorm </code>before each Self-Attention and MLP layer. However, we can also apply a LayerNorm on the embeddings themselves, or leave the model to learn the scaling of the residual part.</p><p>In the CLIP implementation, we find a LayerNorm applied to the embeddings before feeding it through the layers.</p><pre><code class=language-python>class CLIPVisionTransformer(Module):
    def __init__(self, config: CLIPVisionConfig):
        super().__init__()
        embed_dim = config.hidden_size
        self.embeddings = CLIPVisionEmbeddings(config)
        self.pre_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)
        self.encoder = CLIPEncoder(config)
        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)

    def forward(self, pixel_values: Tensor) -&gt; Tensor:
        hidden_states = self.embeddings(pixel_values)
        hidden_states = self.pre_layernorm(hidden_states)
        encoder_outputs = self.encoder(inputs_embeds=hidden_states)
        return encoder_outputs
</code></pre><p>In contrast, in the SigLIP implementation, this normalization is missing. However, it is not expected to cause a major performance difference.</p><pre><code class=language-python>
class SiglipVisionTransformer(Module):
    def __init__(self, config: SiglipVisionConfig):
        super().__init__()
        embed_dim = config.hidden_size
        self.embeddings = SiglipVisionEmbeddings(config)
        self.encoder = SiglipEncoder(config)
        # Defined but not actually used.
        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)

    def forward(self, pixel_values: Tensor) -&gt; Tensor:
        hidden_states = self.embeddings(pixel_values)
        encoder_outputs = self.encoder(inputs_embeds=hidden_states)
        return encoder_outputs
</code></pre><h3 id=summary>Summary</h3><p>Overall, our SigLIP implementation has some small, but crucial differences compared to MLC&rsquo;S CLIP implementation. The table below summarizes the differences that we needed to account for.</p><table><thead><tr><th></th><th>MLC LLaVA Model - CLIP</th><th>0.5B LLaVA-OneVision Qwen2 0.5B Model - SigLIP</th><th></th><th></th></tr></thead><tbody><tr><td>Output Feature Aggregation</td><td>Class Token</td><td>Attention Pooling</td><td></td><td></td></tr><tr><td>Feature Layer</td><td>Pre-Last Layer</td><td>Last Layer</td><td></td><td></td></tr><tr><td>Embedding Normalization</td><td>Yes</td><td>No</td><td></td><td></td></tr><tr><td>GELU Implementation</td><td>Sigmoid Approximation</td><td>Tanh Approximation</td><td></td><td></td></tr></tbody></table><p>Now that we&rsquo;ve implemented SigLIP in the MLC framework, it is straight forward to integrate the LLaVA-OneVision models into the MLC framework. We can now proceed with quantizing and optimizing our model for deployment on an edge device.</p><h2 id=packaging-our-custom-model>Packaging Our Custom Model</h2><p>Once we&rsquo;ve extended the <code>mlc-llm-cpu</code> library to include our custom model definition, then we can proceed as normally.</p><p>First, we want to quantize it our newly ported LLaVA model. To do so, we run the following commands in our MLC LLM project&rsquo;s repo directory:</p><pre><code class=language-bash># Create directory
mkdir -p dist/models &amp;&amp; cd dist/models

# Clone Original LLaVA model's weights from HuggingFace
git lfs install
git clone https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf

# Apply the `q4f16_1` quantization method to our model
mlc_llm convert_weight ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \
       --quantization q4f16_1 \
       -o dist/llava-onevision-qwen2-0.5b-ov-hf
       --model-type llava_onevision
</code></pre><p>Fortunately, the command ran successfully.</p><figure><img src=images/quantize_llava.png width=100%><figcaption>After defining my target LLaVA-OneVision model as a new MLC `model-type`, I was able to easily quantize this model.</figcaption></figure><p>Next, we need to apply some hardware-specific optimizations to our model.</p><pre><code class=language-bash># Generate a MLC config file for our quantized model
mkdir dist/libs
mlc_llm gen_config ./dist/models/llava-onevision-qwen2-0.5b-ov-hf/ \
  --quantization q4f16_1 \
  --conv-template redpajama_chat \
  --context-window-size 768 \
  -o dist/llava-onevision-qwen2-0.5b-ov-hf

# Optimize LLaVA OneVision for an iOS app implementation
mlc_llm compile ./dist/llava-onevision-qwen2-0.5b-ov-hf/mlc-chat-config.json \
  --device iphone \
  -o dist/libs/llava-onevision-qwen2-0.5b-ov-hf-iphone.tar
</code></pre><p>Finally, we need to package the quantized and optimized model for my iOS App. To make my life easier, I&rsquo;ve uploaded the pre-quantized LLaVA-OneVision Qwen2 0.5B model to <a href=https://huggingface.co/bella-nich/llava-onevision-qwen2-0.5b-ov-q4f16_1-mlc target=_blank rel=noopener>my personal HuggingFace account</a>.</p><p>My <code>mlc-package-config.json file</code> located in the <code>ios/MLCChat/</code> subdirectory (following <a href=https://github.com/mlc-ai/mlc-llm target=_blank rel=noopener>MLC LLM&rsquo;s project structure</a>) now looks like this:</p><pre><code>{
    &quot;device&quot;: &quot;iphone&quot;,
    &quot;model_list&quot;: [
        {
            &quot;model&quot;: &quot;HF://bella-nich/llava-onevision-qwen2-0.5b-ov-q4f16_1-mlc&quot;,
            &quot;model_id&quot;: &quot;llava-onevision-qwen2-0.5b-ov-hf&quot;,
            &quot;estimated_vram_bytes&quot;: 1000000000,
            &quot;overrides&quot;: {
                &quot;prefill_chunk_size&quot;: 128
            }
        },
    ]
}
</code></pre><p>So, I&rsquo;m going to package my quantized and optimized LLaVA-OneVision model into a proper iOS app.</p><pre><code class=language-bash># Words
cd /path/to/MLCChat  # e.g., &quot;ios/MLCChat&quot;
export MLC_LLM_SOURCE_DIR=/path/to/mlc-llm  # e.g., &quot;../..&quot;
mlc_llm package
</code></pre><figure><img src=images/package_llava.png><figcaption>I was able to successfully package my newly ported LLaVA-OneVision Qwen2 0.5B model without any errors. Meaning, there's a chance that everything was quantized and compiled correctly.</figcaption></figure><h2 id=end-result>End Result</h2><p>While the lack of compilation errors is promising, the only way to validate this entire process is talk to the embedded LLaVA model. So, I built and deployed my packaged iOS application.</p><p>Once I do, I&rsquo;m greeted with a few strange but mostly understandable sentences.</p><figure><img src=images/llava_makes_sense.png width=60%><figcaption>My embedded LLaVA-OneVision Qwen2 0.5B model is able to form mostly coherent sentences. However, its sense of humor doesn't seem fully developed.</figcaption></figure><p>In other words, LLaVA isn&rsquo;t pure spouting gibberish. I take this as a sign that the quantization and model compilation processes have gone well. Of course, as the longer the conservation goes on, the less coherent LLaVA becomes. Pretty soon LLaVA is giving me random responses strung together.</p><figure><img src=images/llava_doesnt_make_sense.png width=65%><figcaption>After my dissatisfaction with the embedded model's sense of humor, I decided to see if LLaVA can tell me a fun fact. To my surprise, I'm greeted with a sudden and odd request.</figcaption></figure><p>The chat snippets of <code>&lt;im_start></code>, <code>&lt;im_end></code>, and <code>assistant</code> give us clues about how this LLaVA model was tuned for image annotation tasks. More specifically, this tells us about the structure of LLaVA-OneVision Qwen2 0.5 B&rsquo;s <a href=https://huggingface.co/docs/transformers/main/en/chat_templating target=_blank rel=noopener>chat template</a>. A chat template restructures our current conversation, which is a list of string, into a single, tokenizable format that the model expects. Here, we can see that the chat template <code>assistant</code> role is prompting LLaVA to continue but in ways that are completely disconnected from my original text-only prompts.</p><p>The good news is that chat template <code>assistant</code> role should be more useful when we provide LLaVA image inputs. However, this LLaVA model&rsquo;s current performance (in a task that it wasn&rsquo;t fine-tuned for) highlights the differences between >1B and a 7B+ parameter models. Larger foundation models are simply more versatile than smaller ones.</p><h2 id=conclusion>Conclusion</h2><p>In this blogpost, I&rsquo;ve shown you everything that it takes to embed an unsupported model onto an edge device using the Machine Learning Engine Compiler framework. As you can see, the devil is in the details. You need to be well-versed in the different permutations of a given neural architecture&rsquo;s implementation and able to spot those differences in the wild.</p><p>Of course, the only thing that&rsquo;s more important than getting this process right is to choose the correct model to embed. The smaller we go in size, the more portable our foundation model becomes, but that portability comes at the cost of performance.</p><h3 id=whats-next>What&rsquo;s next?</h3><p>While embedding a custom model is an exciting milestone, we&rsquo;re not done yet. As you can see, the embedded LLaVA model works but it doesn&rsquo;t make for a scintillating conversation partner. Hence, we need to get Gemma 2B and the LLaVA-OneVision Qwen2 0.5B model to work together â€” which is exactly what I do in <a href=../edge-llm-app/>my next (and final) blogpost in this series</a>. Stay tuned!</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/embedded-systems/>Embedded Systems</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-llm-embed-llava/&amp;text=Shrinking%20the%20Impossible%20%28Part%203%29:%20Embedding%20a%20Custom-Defined%20LLaVA-OneVision%20Model%20with%20MLC" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-llm-embed-llava/&amp;t=Shrinking%20the%20Impossible%20%28Part%203%29:%20Embedding%20a%20Custom-Defined%20LLaVA-OneVision%20Model%20with%20MLC" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Shrinking%20the%20Impossible%20%28Part%203%29:%20Embedding%20a%20Custom-Defined%20LLaVA-OneVision%20Model%20with%20MLC&amp;body=https://bellanich.github.io/post/edge-llm-embed-llava/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-llm-embed-llava/&amp;title=Shrinking%20the%20Impossible%20%28Part%203%29:%20Embedding%20a%20Custom-Defined%20LLaVA-OneVision%20Model%20with%20MLC" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Shrinking%20the%20Impossible%20%28Part%203%29:%20Embedding%20a%20Custom-Defined%20LLaVA-OneVision%20Model%20with%20MLC%20https://bellanich.github.io/post/edge-llm-embed-llava/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-llm-embed-llava/&amp;title=Shrinking%20the%20Impossible%20%28Part%203%29:%20Embedding%20a%20Custom-Defined%20LLaVA-OneVision%20Model%20with%20MLC" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_huc41288cec43c79590afb2ef028d6600d_3820530_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-llm-app/>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</a></li><li><a href=/post/edge-llm-vision-encoders/>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</a></li><li><a href=/post/edge-llm-mlc/>Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC</a></li><li><a href=/post/reinforcement-learning/>Reinforcement Learning: Investigating Gradient Stability in Policy Based Methods</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€”
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>