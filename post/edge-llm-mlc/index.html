<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="The open-source Machine Learning Compiler Engine project is transforming foundation models into efficient and portable powerhouses."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-llm-mlc/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-llm-mlc/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-llm-mlc/"><meta property="og:title" content="Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC | Bella Nicholson"><meta property="og:description" content="The open-source Machine Learning Compiler Engine project is transforming foundation models into efficient and portable powerhouses."><meta property="og:image" content="https://bellanich.github.io/post/edge-llm-mlc/featured.png"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-llm-mlc/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-11-29T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-29T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-llm-mlc/"},"headline":"Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC","image":["https://bellanich.github.io/post/edge-llm-mlc/featured.png"],"datePublished":"2024-11-29T00:00:00Z","dateModified":"2024-11-29T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"The open-source Machine Learning Compiler Engine project is transforming foundation models into efficient and portable powerhouses."}</script><title>Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Nov 29, 2024
</span><span class=middot-divider></span>
<span class=article-reading-time>16 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:514px><div style=position:relative><img src=/post/edge-llm-mlc/featured_hudfe0e2e68198e39cd805dbeb68f963bf_1096255_720x0_resize_lanczos_3.png alt class=featured-image>
<span class=article-header-caption>Image credit: <a href=https://github.com/black-forest-labs/flux target=_blank rel=noopener><strong>Generated using FLUX</strong></a></span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a><ul><li><a href=#project-inspiration>Project Inspiration</a></li><li><a href=#why-edge-foundation-models-matter>Why Edge Foundation Models Matter</a></li><li><a href=#challenges-of-running-foundation-models-on-edge-devices>Challenges of Running Foundation Models on Edge Devices</a><ul><li><a href=#my-gemma-debacle>My Gemma Debacle</a></li><li><a href=#the-memory-struggle-is-real>The Memory Struggle Is Real</a></li></ul></li><li><a href=#mlc-llm-a-quantum-leap-in-deploying-edge-foundation-models>MLC LLM: A Quantum Leap in Deploying Edge Foundation Models</a></li></ul></li><li><a href=#how-does-mlc-llm-work>How does MLC LLM work?</a><ul><li><a href=#quantization>Quantization</a><ul><li><a href=#what-is-quantization>What is Quantization?</a></li><li><a href=#quantization-methods>Quantization Methods</a><ul><li><a href=#1-post-training-quantization-ptq>1. Post-Training Quantization (PTQ)</a></li><li><a href=#2-quantization-aware-training>2. Quantization-Aware Training</a></li></ul></li><li><a href=#quantizing-transformers>Quantizing Transformers</a></li><li><a href=#out-of-the-box-mlc-llm-solutions>Out of the Box MLC LLM Solutions</a></li><li><a href=#custom-solutions>Custom Solutions</a></li></ul></li><li><a href=#hardware-optimizations>Hardware Optimizations</a><ul><li><a href=#just-in-time-model-compilation>Just-in-Time Model Compilation</a></li><li><a href=#mlc-llm-implementation>MLC LLM Implementation</a></li></ul></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#whats-next>What&rsquo;s next?</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>Ever since ChatGPT went mainstream, I‚Äôve been captivated by the rapid advancements in large language models (LLMs). As a machine learning engineer, I‚Äôve been eagerly awaiting my chance to experiment with these groundbreaking models. Yet, the reality of deploying and managing the required infrastructure ‚Äî and its massive cost ‚Äî always made me pause.</p><h3 id=project-inspiration>Project Inspiration</h3><p>This June at the <a href=https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/ target=_blank rel=noopener>Google I/O Connect Event in Berlin</a>, I realized my vision of working with server-free LLMs wasn‚Äôt as far-fetched as I‚Äôd thought. Google showcased <a href=https://deepmind.google/technologies/gemini/nano/ target=_blank rel=noopener>Geminin nano, a powerful LLM integrated directly into their latest Android devices</a>. While it wasn‚Äôt accessible to developers yet, it was a glimpse of what might soon be possible.</p><figure><img src=images/gemini_nano.png width=75%><figcaption>Google originally launched Gemini Nano on the Pixel 8 Pro in December 2023. This edge LLM has been gradually rolled out to other popular Android phones, including Samsung's Galaxy S24 series (<a href="https://www.yahoo.com/tech/google-making-gemini-nano-available-152409469.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAHbJFvaHe7eD1PbQpKUqhsrT8SThokdhZahaoc7cwPDe_CDZhRjsXmtWYQBrMe6qSuBdqUYns1O1ykdkfbAzILy3JmKegzVfSvByqwDsrx7YXxFfNXvM9-z5gPBhkVHS4I6eFneAMIGbStXAkKunwr-kqduoZ5jQb8CaGeTrpVNe">source</a>,
<a href=https://deepmind.google/technologies/gemini/nano/>image credit</a>).</figcaption></figure><p>Inspired by this progress, I set out to test the limits of edge LLMs. Could I, as a solo enthusiast, deploy a LLM on an edge device like my iPhone? To make the challenge even more intriguing, I decided to aim for a multi-modal LLM. After all, who wouldn‚Äôt want a private chatbot that understands your phone‚Äôs photo gallery and keeps your secrets safe?</p><p>In this 4-part blog series, I document my experiment to prove that you don‚Äôt need a sprawling server farm or a high-end workstation to dive into the latest AI technology. With a bit of machine learning knowledge, solid documentation, and plenty of determination, you can get started with state-of-the-art models without a painful cloud bill (looking at you, AWS).</p><h3 id=why-edge-foundation-models-matter>Why Edge Foundation Models Matter</h3><p>Foundation models have traditionally been too massive to run on edge devices like smartphones, IoT gadgets, or embedded systems. As a result, they‚Äôre typically hosted on centralized servers, which introduces several challenges:</p><ol><li><p><strong>Cost barriers.</strong> Deploying and serving large-scale models (think 10B+ parameters) in the cloud is prohibitively expensive, often costing millions in infrastructure and energy. This creates a significant barrier for students, hobbyists (like me), and smaller organizations looking to experiment with AI. By running models locally on edge devices, the need for expensive server infrastructure disappears, democratizing access to this cutting-edge technology.</p></li><li><p><strong>Unreliable performance.</strong> Cloud-based inference depends on a steady internet connection to send data to servers and retrieve results. This back-and-forth can cause frustrating delays, especially in areas with poor connectivity. Edge models, which run directly on local devices, bypass these issues. They deliver faster responses and work well even without an internet connection.</p></li><li><p><strong>Security concerns.</strong> Cloud-based systems inherently require sending data to remote servers, which comes with inherent risks. For users, their personal chat data could be exposed in a security breach or misused without consent. Businesses, meanwhile, must navigate strict regulations like GDPR or HIPAA when transferring sensitive data off-device. By processing data locally, edge models eliminate these risks, ensuring that your personal information stays private.</p></li></ol><p>In short, edge foundation models break down cost barriers, improve reliability, and address privacy concerns. They make AI more accessible for curious minds and businesses alike while offering end users more peace of mind.</p><h3 id=challenges-of-running-foundation-models-on-edge-devices>Challenges of Running Foundation Models on Edge Devices</h3><p>By now, you might be thinking, Edge foundation models sound amazing! Why isn‚Äôt everyone using them? Well, as with most things in life (and AI), there‚Äôs a catch. Running foundation models on edge devices isn‚Äôt exactly a walk in the park. Let me walk you through some of the challenges, starting with my own cautionary tale.</p><h4 id=my-gemma-debacle>My Gemma Debacle</h4><p>About six months ago, I got my hands on Google‚Äôs shiny new <a href=https://huggingface.co/google/gemma-2-2b-it target=_blank rel=noopener>instruction-tuned Gemma 2B model</a>. Gemma, for those unfamiliar, is the ‚Äúbaby‚Äù version of <a href=https://deepmind.google/technologies/gemini/ target=_blank rel=noopener>DeepMind‚Äôs Gemini family</a> ‚Äî a lightweight, open-weight LLM designed for resource-constrained environments.</p><figure><img src=images/gemma2b_ranking.png width=100%><figcaption>Why Gemma2B? In a nutshell, it's very impressive for its size. In benchmarks tasks, <a href=https://arxiv.org/pdf/2408.00118>Gemma 2B's performance is comparable those of 7B - 9B LLMs</a>. It also <a href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard>consistently tops the HuggingFace leaderboard for smaller LLMs</a>.</figcaption></figure><figure><img src=images/gemma2b_languages.png width=60%><figcaption>Gemma 2B calls itself a polyglot, claiming fluency in a bunch of languages. If that checks out, it‚Äôs a pretty exciting pick for global business apps ‚Äî just the kind of thing a solo developer (like me) could put to work.</figcaption></figure><p>Basically, Gemma 2B was designed for laptops, desktops, and modest cloud setups. It sounded perfect for my trusty MacBook Air M2 (8GB of RAM).</p><p>Spoiler alert: it wasn‚Äôt.</p><p>I excitedly set up Gemma and attempted to serve my first request. My MacBook? It practically waved a white flag and crashed halfway through.</p><p>Let‚Äôs do the math to see why this happened. The Gemma 2B model has (you guessed it) 2 billion parameters. Using the standard float32 data type, the model parameters alone would require $2B\text{ parameters} * \frac{4 \text{ bytes}}{\text{parameter}} = 8 \text{ billion bytes} = 8 \text{ GB of RAM}$.</p><p>But that‚Äôs not all my machine needs to handle:</p><ul><li>I need extra memory for activations (the intermediate calculations during inference).</li><li>The operating system (in my case, macOS) also needs a hefty chunk of RAM to do its thing</li></ul><p>In short, my poor MacBook was way out of its depth. Even with more efficient data types like float16 or bfloat16 (which halve memory usage), the combined memory demands of the model, activations, and system processes were just too much. Now, imagine trying to squeeze this kind of workload onto a smartphone with even less RAM. You‚Äôd be lucky if your phone didn‚Äôt catch fire (kidding&mldr;mostly).</p><h4 id=the-memory-struggle-is-real>The Memory Struggle Is Real</h4><p>Edge devices are, by design, resource-constrained. They‚Äôre great for portability, but they aren‚Äôt built to handle the sheer memory and compute demands of large language models. Even lightweight models like Gemma, which aim to close this gap, can still overwhelm devices with limited RAM or processing power.</p><p>But don‚Äôt despair! Engineers and researchers are tackling these challenges head-on. By using model compression techniques like quantization, pruning, and distillation, they‚Äôve managed to shrink memory and compute requirements significantly. Add to this a new wave of hardware optimization techniques, and edge deployment is more feasible than ever.</p><p>Now, innovative tools are building on these advancements to make edge LLMs not just possible, but practical for a wide range of devices. Curious about how these breakthroughs are unfolding in real-world applications? Let me introduce you to one powerful solution: the MLC LLM framework.</p><h3 id=mlc-llm-a-quantum-leap-in-deploying-edge-foundation-models>MLC LLM: A Quantum Leap in Deploying Edge Foundation Models</h3><p>Fast forward to November 2024, I decided to try the same task as before but with <a href=https://llm.mlc.ai target=_blank rel=noopener>the Machine Learn Compiler (MLC) LLM Engine</a>. This time, I deployed <a href=https://huggingface.co/mlc-ai/gemma-2b-it-q4f16_1-MLC target=_blank rel=noopener>a pre-quantized version of this Gemma 2B model</a> onto an edge device ‚Äî specifically, an iOS app. The results blew me away.</p><p>For starters, I encountered zero performance issues on my MacBook Air. The pre-quantized and hardware-optimized Gemma model ran smoothly and efficiently, without any of the lag or crashes I had faced with six months earlier.</p><p>But here&rsquo;s where things really got exciting: the quality of the responses. They were practically indistinguishable from the likes of massive, cloud-based LLMs in an everyday conversation. Curious to see how well this mini-model handled other languages, I threw some Spanish and German at it. To my non-discerning eye, the results looked spot-on. (I‚Äôd love to hear what native speakers think, though.)</p><figure><img src=images/gemma2b_spanish.png width=100%><figcaption>I decided to practice my very rusty Spanish and seek vacation inspiration in one-go. Here are Gemma's recommendations for a visit to Valencia, Spain üá™üá∏ in Winter.</figcaption></figure><figure><img src=images/gemma2b_german.png width=80%><figcaption>Christmas time means Christmas Markets üéÑ ‚Äî especially when you live so close to Germany üá©üá™. So, I decided to see if Gemma could give me any fun suggestions in German.</figcaption></figure><p>Now, you might be wondering: How did MLC manage to pull this off? Let‚Äôs take a step back and dive into the tech behind this feat.</p><br><h2 id=how-does-mlc-llm-work>How does MLC LLM work?</h2><p>At a high-level, <a href=https://github.com/mlc-ai/mlc-llm target=_blank rel=noopener>the MLC LLM project</a> makes it possible to embed smaller LLMs (under 10B parameters) on edge devices through a streamlined, three-step process:</p><ol><li><strong>Quantize LLM weights</strong> as the model is downloaded. This prevented my machine from crashing due to insufficient memory.</li><li><strong>Embed the quantized model</strong> with hardware-specific optimizations applied during the model compilation stage.</li><li><strong>Provide a simple, pre-built user interface</strong> to interact with your newly embedded foundation model.</li></ol><figure><img src=./images/mlc_llm_workflow.png><figcaption>The MLCEngine embeds LLMs across different software platforms through model quantization and hardware-specific optimization (<a href=[url](https://llm.mlc.ai)>image credit</a>)</figcaption></figure><p>MLC offers a user-friendly, open-source chat application for both Android and iOS. Alternatively, it implements its own version of <a href=https://platform.openai.com/docs/api-reference/introduction target=_blank rel=noopener>OpenAI&rsquo;s Python API</a>, making it easy to integrate the optimized LLM into your own existing projects.</p><h3 id=quantization>Quantization</h3><p><a href=https://llm.mlc.ai/docs/get_started/introduction.html#chat-cli target=_blank rel=noopener>MLC LLM caches pre-quantized model weights and compiled model library locally</a>, which means you <strong>only</strong> need to <strong>download and quantize the model once</strong>. After that, the quantized model is ready to run on your device without requiring repeated downloads. This saves both time and bandwidth, making the process smoother and more efficient.</p><h4 id=what-is-quantization>What is Quantization?</h4><p>In simple terms, quantization is the process of reducing the precision of the numbers that represent a model‚Äôs parameters. The goal? Shrink the model‚Äôs memory footprint while keeping its performance as close to the original as possible. The real magic happens when you see the cost savings‚Äîquantization can cut your cloud compute bills by half, a quarter, or even a sixth, without any noticeable drop in performance. For massive models like LLMs, those savings can really add up.</p><blockquote><p>Take the example of <a href=https://www.yurts.ai target=_blank rel=noopener>yurts</a>, a contractor for the U.S. government. They <a href=https://www.yurts.ai/blog/enhancing-enterprise-efficiency-quantization-for-cost-effective-llm-deployment target=_blank rel=noopener>slashed its monthly cloud computing bill from USD 24,000 to USD 4,000 for a 70B parameter LLM</a> by using a quantization method called <a href=https://huggingface.co/docs/transformers/main/en/quantization/awq target=_blank rel=noopener>Activation-aware Weight Quantization (AWQ)</a>. Pretty impressive, right?</p></blockquote><h4 id=quantization-methods>Quantization Methods</h4><p>When it comes to quantizing a model, there are a few common methods, but the two main approaches are:</p><h5 id=1-post-training-quantization-ptq>1. Post-Training Quantization (PTQ)</h5><p>After a model is trained, you can apply quantization to reduce the bit-width of its weights. The best part? It‚Äôs quick, easy, and requires minimal changes to the original model, while still offering significant memory savings.</p><p>One common PTQ technique is grouped quantization, where the model‚Äôs weights are grouped based on features like their layer or importance. Each group is quantized separately, making the process more tailored and efficient. This method has been around since the late 1990s and continues to evolve as a way to balance performance and memory efficiency.</p><p>Some weight groups are more sensitive to quantization errors and need higher precision (more bits) to maintain accuracy. Others can handle lower precision without a noticeable hit to performance.</p><p>With the rise of foundation models, more specific implementations of grouped quantization have emerged. For an in-depth look, check out <a href=https://arxiv.org/abs/2212.09720 target=_blank rel=noopener>&ldquo;The case for 4-bit precision: k-bit Inference Scaling Laws&rdquo;</a> and <a href=https://arxiv.org/abs/2206.09557 target=_blank rel=noopener>&ldquo;The LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models&rdquo;</a>.</p><p>More recently, techniques like <a href=https://huggingface.co/docs/transformers/en/quantization/awq target=_blank rel=noopener>Activation-aware Weight Quantization (AWQ)</a> have taken this dynamic quantization approach even further. AWQ uses activation statistics to pinpoint the most important individual weights and ensures they aren‚Äôt over-quantized, allowing for better compression without sacrificing performance.</p><h5 id=2-quantization-aware-training>2. Quantization-Aware Training</h5><p>This method goes a step further by training the model with lower precision in mind from the start. By optimizing the model for reduced precision during training, you often get better results than you would with post-training quantization. Essentially, it allows the model to ‚Äúlearn‚Äù how to perform well with less precision, resulting in better overall performance. However, as we focus on deploying pre-trained models, we won‚Äôt explore this method further.</p><h4 id=quantizing-transformers>Quantizing Transformers</h4><p>When quantizing Transformers, it‚Äôs not just the weights that need attention‚Äîactivations play a big role too. Activations are the intermediate values generated during the model&rsquo;s forward pass as it processes the input data. In a Transformer, these are the values produced at each layer as it handles each token. Just like with weights, activations can also be compressed during quantization, which further reduces memory usage.</p><p>But memory management doesn‚Äôt end with weights and activations. For Transformers, there‚Äôs also <a href=https://huggingface.co/docs/transformers/kv_cache#what-is-cache-and-why-we-should-care target=_blank rel=noopener>the key-value (KV) cache</a> ‚Äî this stores the context of the input sequence as the model processes longer inputs. As the model processes longer and longer inputs, it needs more memory to store the increasing number of keys and values. To keep things efficient,<a href=https://llm.mlc.ai/docs/compilation/compile_models.html#generate-mlc-chat-config target=_blank rel=noopener> MLC LLM provides additional memory optimization techniques, like sliding windows</a>, which help manage memory usage even when dealing with longer sequences.</p><figure><img src=./images/kv_cache.gif><figcaption>The key-value (KV) cache in Transformers preserves the context of processed tokens, enabling the model to "remember" earlier parts of a conversation. Yet, as the conversation grows, the cache scales up rapidly, incurring risks of out-of-memory errors on edge devices if not handled properly. (<a href=https://jalammar.github.io/illustrated-gpt2/>image credit</a>)</figcaption></figure><h4 id=out-of-the-box-mlc-llm-solutions>Out of the Box MLC LLM Solutions</h4><p>As you can probably guess, the MLC Engine only implements post-training quantization (since we have no control over an open sourced LLM&rsquo;s training process). In particular, <a href=https://github.com/mlc-ai/mlc-llm/blob/main/docs/compilation/configure_quantization.rst target=_blank rel=noopener>MLC LLM implements the grouping quantization methods</a> shown below.</p><figure><div style=padding-left:50px;padding-right:20px><table><thead><tr><th>Method Name</th><th>Weight Quantization</th><th>Activation Quantization</th><th>Version No.</th><th>Stable?</th></tr></thead><tbody><tr><td>q0f16</td><td>None</td><td>16 bits</td><td>-</td><td>Yes</td></tr><tr><td>q0f32</td><td>None</td><td>32 bits</td><td>-</td><td>Yes</td></tr><tr><td>q3f16_1</td><td>3 bits</td><td>16 bits</td><td>1</td><td>Yes</td></tr><tr><td>q4f16_1</td><td>4 bits</td><td>16 bits</td><td>1</td><td>Yes</td></tr><tr><td>q4f32_1</td><td>4 bits</td><td>32 bits</td><td>1</td><td>Yes</td></tr><tr><td>q4f16_awq</td><td>4 bits</td><td>16 bits</td><td>1</td><td><strong>No</strong></td></tr></tbody></table></div><figcaption>Table 1: An Overview of MLC's implemented quantization methods (<a href=https://github.com/mlc-ai/mlc-llm/blob/main/docs/compilation/configure_quantization.rst>source</a>).</figcaption></figure><p>MLC Enginer also offers an AWQ implementation (called <code>q4f16_awq</code>), but it&rsquo;s currently <strong>unstable</strong> so use it at your own risk.</p><p>Of course, the folks behind MLC have already gone and quantized most of the very popular open-source LLMs. You can download these pre-quantized model weights from <a href=https://huggingface.co/mlc-ai target=_blank rel=noopener>their official MLC AI&rsquo;s HuggingFace account</a>.</p><p>If you want to quantize a new model, then there&rsquo;s a little more work involved. MLC right now supports quantization of these model types: <code>baichuan</code>, <code>bert</code>, <code>chatglm3</code>, <code>cohere</code>, <code>eagle</code>, <code>gemma</code>, <code>gemma2</code>, <code>gpt2</code>, <code>gpt_bigcode</code>, <code>gpt_neox</code>, <code>internlm</code>, <code>internlm2</code>, <code>llama</code>, <code>llava</code>, <code>medusa</code>, <code>minicpm</code>, <code>mistral</code>, <code>mixtral</code>, <code>orion</code>, <code>phi</code>, <code>phi3</code>, <code>phi3v</code>, <code>qwen</code>, <code>qwen2</code>, <code>qwen2_moe</code>, <code>rwkv5</code>, <code>rwkv6</code>, <code>stable_lm</code>, and <code>starcoder2</code>.</p><p>So, if you want to quantize of these model types yourself, then all you have to do is <a href=https://llm.mlc.ai/docs/get_started/introduction.html#id8 target=_blank rel=noopener>run a few simple commands</a>.</p><h4 id=custom-solutions>Custom Solutions</h4><p>If you want to quantize an unsupported model type, you&rsquo;ll need to extend MLC LLM&rsquo;s source code. This involves inferring your target model&rsquo;s architecture from its source <code>config.json</code> file <a href=https://huggingface.co target=_blank rel=noopener>on HuggingFace</a> and wrapping its original Python definition (e.g., from <a href=https://pypi.org/project/transformers/ target=_blank rel=noopener>the <code>transformers</code> Python library</a>) with MLC LLM&rsquo;s wrappers. I ended up having to do this to support multi-modal functional in <a href=../edge-llm-embed-llava/>the 3rd blog post in this series</a>.</p><br><h3 id=hardware-optimizations>Hardware Optimizations</h3><p>Quantization is just one part of MLC‚Äôs bag of tricks. The other? Squeezing every last drop of performance out of your hardware through smart optimizations. See, your LLM model might start as high-level Python code, but it doesn‚Äôt interact directly with your device‚Äôs hardware. There‚Äôs a crucial middle step where MLC translates that model into something your CPU or GPU can actually understand‚Äîand it does this in the most efficient way possible.</p><h4 id=just-in-time-model-compilation>Just-in-Time Model Compilation</h4><p><strong>Just-in-Time (JIT) model compilation</strong> is the secret sauce behind MLC‚Äôs stellar efficiency. Instead of pre-compiling everything in advance or running the model eagerly line-by-line, JIT optimizes your model right before it executes, ensuring it‚Äôs perfectly suited to your specific hardware.</p><p>JIT strikes a balance between two compiler approaches:</p><ol><li><strong>Interpreted execution</strong> processes code step-by-step as it runs. This makes the code super flexible and easy to debug, but leaves no room for optimizations. In other words, it&rsquo;s painfully slow.</li><li><strong>Ahead-of-Time (AOT) compilation</strong> pre-compiles everything into a fixed version before execution. This is much faster, but comes with a catch: we assume a one-size-fits-all solution. If the model encounters unexpected conditions or hardware variations, AOT‚Äôs rigid approach can leave performance on the table because it can‚Äôt take full advantage of the specific device running the code.</li></ol><p>JIT avoids these pitfalls by waiting until runtime to optimize. It tailors the model‚Äôs code to your hardware and execution context just before runtime, ensuring maximum efficiency. Here&rsquo;s how this process works:</p><ol><li><strong>Tracing or scripting.</strong> First, the engine analyzes your model‚Äôs high-level code and maps out its computation graph and operations. Think of it as creating a blueprint for what the model will do</li><li><strong>Optimization.</strong> Next, the engine gets to work refining that blueprint. It fuses operations, removes redundancies, and inlines functions, streamlining execution wherever possible. (It‚Äôs like an architect revising a blueprint for a more efficient construction process.)</li><li><strong>Low-level code generation.</strong> Once the optimizations are done, the graph is compiled into low-level machine code tailored to your specific hardware‚Äîwhether that‚Äôs a CPU, GPU, or something fancier.</li><li><strong>Execution.</strong> Finally, the optimized code is executed, running faster and using less memory thanks to all the pre-launch optimizations.</li></ol><p>MLC uses JIT model compilation to get the most out of your edge device&rsquo;s limited resources. And the best part? This process is abstracted away into <a href=https://llm.mlc.ai/docs/compilation/compile_models.html target=_blank rel=noopener>a few simple CLI commands</a>.</p><h4 id=mlc-llm-implementation>MLC LLM Implementation</h4><p>Deep neural networks are computationally demanding. Hence, most deep learning frameworks include built-in JIT compilation extensions. For example, <a href="https://github.com/openxla/xla?tab=readme-ov-file" target=_blank rel=noopener>Accelerated Linear Algebra (XLA)</a>, the backbone of JAX, offers <a href=https://openxla.org/xla target=_blank rel=noopener>cross-framework JIT support</a>. Looking specifically at PyTorch, <a href=https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html target=_blank rel=noopener><code>torch.compile</code> provides a general-purpose solution that supports both training and inference</a>.</p><p>However, MLC takes it a step further by leveraging <a href=https://tvm.apache.org target=_blank rel=noopener>Apache&rsquo;s Tensor Virtual Machine (TVM)</a> for even deeper hardware-level optimizations.</p><figure><img src=./images/tvm_flexible.png><figcaption>We can think of TVM as an improvement on PyTorch's optimizations, offering advanced optimizations and hardware-specific tuning that PyTorch's JIT lacks. Additionally, TVM is easy to use due to the separation of its compiler and runtime components. This makes it possible for me to compile a ML model on one machine (e.g., a MacBook) and deploy it on another (e.g., a Raspberry Pi). (<a href=https://tvm.apache.org/docs/how_to/deploy/index.html>image credit</a>)</figcaption></figure><p><a href=https://mlc.ai/chapter_integration/index.html target=_blank rel=noopener>TVM works by providing a Python API for tensor operations like matrix multiplications, sums, and type conversions.</a> It also makes it a breeze to port models from PyTorch. Once we have the model in TVM, we can translate it into C++ as we optimize it for execution.</p><p>Here‚Äôs how exactly TVM supercharges model optimization:</p><ol><li><p><strong>Operation Fusion.</strong> TVM combines smaller operations (like element-wise additions or multiplications) into larger, more efficient ones.</p><blockquote><p><strong>Example.</strong> Instead of calculating ReLU(x) followed by Add(x, y), TVM can combine them into a single, efficient kernel, saving memory and time.</p></blockquote></li><li><p><strong>Memory Layout Optimization:.</strong> TVM fine-tunes memory access patterns to align with the hardware‚Äôs strengths. For example, GPUs perform better when accessing data in large, coalesced blocks, while CPUs benefit from loop optimizations that prevent cache misses.</p></li><li><p><strong>Kernel Selection and Tuning.</strong> A &ldquo;kernel&rdquo; is a specialized function designed to perform specific operations, like matrix multiplication. TVM either selects the best pre-tuned kernels or auto-tunes them for maximum performance on the target hardware.</p></li></ol><p>These optimizations make it possible to (hypothetically) fit a 7B+ parameter model onto an iPhone. But of course, there‚Äôs a trade-off: the more optimizations we apply, the less flexible the model becomes. Debugging also gets trickier ‚Äî any issues that arise are often low-level errors, especially when input sizes change.</p><p>Despite these challenges, the benefits far outweigh the costs. Without TVM, deploying models on edge devices would be much more difficult.</p><br><h2 id=conclusion>Conclusion</h2><p>In the past six months, the AI research community has made groundbreaking strides in optimizing foundation models for edge devices. Back in June 2024, my personal machine crashed when I tried to run the Gemma 2B model locally ‚Äî without quantization or hardware optimizations. But thanks to the rapid progress in this field, even solo enthusiasts like myself can now, as of November 2024, easily deploy the same model (or even larger ones) locally‚Äîwithout needing to become compiler engineers.</p><p>In this blog post, I‚Äôve introduced the Machine Learning Compiler (MLC) as a powerful new tool to make this possible. I‚Äôve also walked you through its inner workings and provided essential background knowledge to help you get started.</p><h3 id=whats-next>What&rsquo;s next?</h3><p>In <a href=../edge-llm-vision-encoders>my next blog post</a>, we‚Äôll dive into how we can extend the MLC Engine to support embedding LLMs that aren‚Äôt natively supported. After all, our goal is to deploy a <strong>multi-modal</strong> LLM on an iPhone.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/embedded-systems/>Embedded Systems</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-llm-mlc/&amp;text=Shrinking%20the%20Impossible%20%28Part%201%29:%20Optimizing%20Foundation%20Models%20for%20Edge%20Devices%20with%20MLC" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-llm-mlc/&amp;t=Shrinking%20the%20Impossible%20%28Part%201%29:%20Optimizing%20Foundation%20Models%20for%20Edge%20Devices%20with%20MLC" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Shrinking%20the%20Impossible%20%28Part%201%29:%20Optimizing%20Foundation%20Models%20for%20Edge%20Devices%20with%20MLC&amp;body=https://bellanich.github.io/post/edge-llm-mlc/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-llm-mlc/&amp;title=Shrinking%20the%20Impossible%20%28Part%201%29:%20Optimizing%20Foundation%20Models%20for%20Edge%20Devices%20with%20MLC" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Shrinking%20the%20Impossible%20%28Part%201%29:%20Optimizing%20Foundation%20Models%20for%20Edge%20Devices%20with%20MLC%20https://bellanich.github.io/post/edge-llm-mlc/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-llm-mlc/&amp;title=Shrinking%20the%20Impossible%20%28Part%201%29:%20Optimizing%20Foundation%20Models%20for%20Edge%20Devices%20with%20MLC" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_huc41288cec43c79590afb2ef028d6600d_3820530_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-llm-app/>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</a></li><li><a href=/post/edge-llm-embed-llava/>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</a></li><li><a href=/post/edge-llm-vision-encoders/>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</a></li><li><a href=/post/reinforcement-learning/>Reinforcement Learning: Investigating Gradient Stability in Policy Based Methods</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> ‚Äî
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>