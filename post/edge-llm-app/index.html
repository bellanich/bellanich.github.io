<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="After painstakingly embedding a mini multi-modal LLaVA model, I'm ready to properly deploy it as an iOS app and enjoy the fruits of my labor. Let's see if we can truly shrink the impossible."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-llm-app/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-llm-app/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-llm-app/"><meta property="og:title" content="Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model | Bella Nicholson"><meta property="og:description" content="After painstakingly embedding a mini multi-modal LLaVA model, I'm ready to properly deploy it as an iOS app and enjoy the fruits of my labor. Let's see if we can truly shrink the impossible."><meta property="og:image" content="https://bellanich.github.io/post/edge-llm-app/featured.jpg"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-llm-app/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-12-02T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-02T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-llm-app/"},"headline":"Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model","image":["https://bellanich.github.io/post/edge-llm-app/featured.jpg"],"datePublished":"2024-12-02T00:00:00Z","dateModified":"2024-12-02T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"After painstakingly embedding a mini multi-modal LLaVA model, I'm ready to properly deploy it as an iOS app and enjoy the fruits of my labor. Let's see if we can truly shrink the impossible."}</script><title>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Dec 2, 2024
</span><span class=middot-divider></span>
<span class=article-reading-time>10 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:301px><div style=position:relative><img src=/post/edge-llm-app/featured_huba25c720ff2b6e951cfa8209f3952001_537920_720x0_resize_q98_lanczos.jpg alt class=featured-image>
<span class=article-header-caption>Image: My Embedded Multi-Modal Model</span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a><ul><li><a href=#my-game-plan>My Game Plan</a></li></ul></li><li><a href=#deployment>Deployment</a><ul><li><a href=#attempt-1-deploying-my-multi-modal-model-as-an-ios-app>Attempt 1: Deploying my multi-modal model as an iOS app</a></li><li><a href=#what-if-the-mlc-chat-engine-supported-uploading-images>What if the MLC Chat Engine supported uploading images?</a></li><li><a href=#changing-directions>Changing Directions</a></li><li><a href=#attempt-2-deploying-my-multi-modal-model-as-a-python-cli>Attempt 2: Deploying my multi-modal model as a Python CLI</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul><h2 id=introduction>Introduction</h2><p>If you&rsquo;ve been following along in this blog post series, you know that I got a little too inspired at <a href=https://developers.googleblog.com/en/bringing-the-io-magic-to-berlin/ target=_blank rel=noopener>this year&rsquo;s Google I/O Connect event</a>. After hearing about Google&rsquo;s incredible feat of <a href="https://support.google.com/pixelphone/community-guide/277503565/enable-gemini-nano-on-device-on-google-pixel-8-8a?hl=en" target=_blank rel=noopener>embedding Gemini Nano into their Pixel 8 phones</a>, I&rsquo;ve been not-so-patiently waiting for the open source community to release tools that make similar feats possible for solo developers like me. Fortunately, <a href=https://llm.mlc.ai target=_blank rel=noopener>the Machine Learning Compiler (MLC) project</a> has matured enough that my dreams might just be possible.</p><p>Hence, I&rsquo;ve been a personal quest to assess the limits of this new and exciting technology. To keep things interesting, I&rsquo;ve decided to embed a multi-modal LLM. I want to test a less established setup, but I&rsquo;m also secretly hoping to have a nice souvenir after everything is said and done. (Personally, I could really use a multi-lingual and multi-modal chatbot during my next holidays — especially once that doesn&rsquo;t eat up my monthly data plan.)</p><p>At this point, I&rsquo;m very close to this end goal. In <a href=../edge-llm-mlc/>my first blog post in this series</a>, I introduced you to the MLC framework, which has been doing most of the heavy-lifting. I also gave you a quick crash course in what it takes to embed a large machine learning model on a resource-constrained device. In <a href=../edge-llm-embed-llava/>the 2nd blogpost</a>, I gave you a primer on vision transformers — specifically, what we need to know to embed <a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf target=_blank rel=noopener>this tiny and multi-modal LLaVA-OneVision Qwen2 0.5B model</a>. In <a href=../edge-llm-embed-llava/>my last blog post</a>, I used this LLaVA model as an example to show you how to extend the MLC framework to support a custom model definition.</p><p>After going through this entire process, I have a fully functional - but not very logical - LLaVA model running on my laptop. As the name suggests, this LLaVA model belongs to the high-performing, open source, and <a href=https://llava-vl.github.io target=_blank rel=noopener>multi-modal Large Language and Vision Assistant (LLaVA) model family</a>. Fortunately, for us, this model has less than 0.5B parameters. Meaning, it should be small enough to squeeze onto an iPhone. Here&rsquo;s what its chat conversations look like:</p><figure><img src=images/llava_makes_sense.png width=60%><figcaption>At the start of a very simple discussion, my embedded LLaVA-OneVision model looks promising.</figcaption></figure><figure><img src=images/llava_doesnt_make_sense.png width=65%><figcaption>Unfortunately, it doesn't take too long to see the cracks in LLaVA's logical reasoning — and to watch them crumble.</figcaption></figure><p>Given this LLaVA model&rsquo;s lack of coherence, we don&rsquo;t want it talking directly to our end user. Rather, we&rsquo;ll let it stick to what it does best (image annotation) and call in a slightly large LLM for backup.</p><h3 id=my-game-plan>My Game Plan</h3><p>So, what&rsquo;s next? If you recall the diagram that I shared with you in the last time, I need to deploy a two-model ensemble. My plan is to use LLaVa-OneVision&rsquo;s image annotation capabilities to expand <a href=https://arxiv.org/pdf/2408.00118 target=_blank rel=noopener>Gemma 2B&rsquo;s ridiculously good conversational capabilities</a>. Essentially, LLaVA will describe the user provided image to Gemma in plain text. Gemma will receive the original user prompt and LLaVA&rsquo;s image description. Between these two, Gemma should have enough information to return a lucid and logical response.</p><figure><img src=images/model_flowchart.png><figcaption>My multi-modal chat pipeline consists of two models: <b>(1)</b> the eloquent and multilingual <a href=https://huggingface.co/google/gemma-2b>Gemma2B model</a>, and <b>(2)</b> a <a href=https://llava-vl.github.io/blog/2024-08-05-llava-onevision/>mini LLaVA-OneVision model</a>. LLaVa will act as a translator for Gemma, generating a text descriptions for any provided images.</figcaption></figure><h2 id=deployment>Deployment</h2><p>My original vision was to have an edge multi-modal embedded onto my iPhone. That way, I could chat with my personal travel assistant anytime and anywhere — no internet connection required.</p><h3 id=attempt-1-deploying-my-multi-modal-model-as-an-ios-app>Attempt 1: Deploying my multi-modal model as an iOS app</h3><p>Before I can package Gemma 2B and LLaVA together as a single model, I first need to figure out how to input images into MLC&rsquo;s simple, pre-built chat iOS application, and this is where I run into my first roadblock.</p><p>If you&rsquo;re observant, you might have noticed the grayed out text &ldquo;Upload picture to chat&rdquo; in my conversation screenshots with LLaVA. One could logically conclude that this means that MLC has some sort of user interface for uploading images. However, after pouring over the Swift code, I couldn&rsquo;t find any such feature. Before I start cobbling something together in Swift, I decide to do a quick sanity check: Can I pass an image URL to the MLC Chat Engine?</p><p>Well&mldr;not really. I&rsquo;d have to go in deep to the MLC&rsquo;s low code and Swift implementations to make that possible. That&rsquo;s a rabbit hole that I don&rsquo;t want to fall into, so it&rsquo;s time for me to pivot directions.</p><h3 id=what-if-the-mlc-chat-engine-supported-uploading-images>What if the MLC Chat Engine supported uploading images?</h3><p>Let&rsquo;s suppose that I was able to the the MLC Chat Engine iOS implementation so that it:</p><ol><li>It now supports serving images to models.</li><li>There&rsquo;s a cool and sleek user interface for uploading said images.</li></ol><p>Could I deploy any of the Apple products in my household? Well, I decided to do an initial test on my housemate&rsquo;s iPad Pro 11 inch (V1). I copied over and downloaded my iOS app. While I was able to open the app&rsquo;s homepage and start my conversation with Gemma 2B, the app would crash before I could even ask Gemma a followup question.</p><p>Perplexed, I checked my iOS app&rsquo;s memory consumption in Xcode after I opened a new chat with Gemma. Well, it turns out that Gemma consumes a bit over 2GB of RAM. Given that this borrowed iPad only has 4GB of RAM in total, it&rsquo;s not a surprise that the iOS application was crashing. Even when I closed all other applications, other background processes were consuming this iPad&rsquo;s limited and valuable memory.</p><figure style=text-align:center><div style=margin-bottom:20px><img src=images/gemma_memory.png style=width:95%;max-width:900px><div style=font-size:16px;margin-top:8px>(a) Gemma 2B</div></div><div><img src=images/llava_memory.png style=width:95%;max-width:900px><div style=font-size:16px;margin-top:8px>(b) LLaVA-OneVision</div></div><figcaption style=margin-top:20px;font-size:16px>The memory footprints of my embedded Gemma 2B and LLaVA-OneVision models on my 8GB M2 MacBook Air.</figcaption></figure><p>In contrast, the LLaVA-OneVision model has a much smaller memory footprint at 755MB. So, yes, it&rsquo;s technically possible to deploy only the LLaVA-OneVision model on this given iPad; however, it&rsquo;s probably not worth it. As we&rsquo;ve seen from this blog post&rsquo;s introduction, the embedded LLaVA-OneVision model isn&rsquo;t capable of holding a coherent conversation for very long. Meaning, Gemma&rsquo;s memory consumption is a definitive roadblock in my app ambitions.</p><p>As a final check, I tried the same exercise with my iPhone 13 and got similar results. This is expected, given that both devices have the same memory capacity. Of course, all the Apple devices I could find are on the older side. Apple&rsquo;s newer iPhones and iPads have a large enough RAM that I should be able to deploy Gemma 2B and LLaVA-OneVision Qwen2 0.5B together.</p><figure><div style=padding-left:250px;padding-right:20px><table><thead><tr><th>Device</th><th>Available RAM</th></tr></thead><tbody><tr><td>iPhone X</td><td>3GB</td></tr><tr><td>iPhone 13</td><td>4G</td></tr><tr><td>iPad Pro V1</td><td>4G</td></tr><tr><td>iPhone 16</td><td>8G</td></tr><tr><td>iPad Air 6</td><td>8G</td></tr></tbody></table></div><figcaption>Table 1: The RAM available on selected Apple products. The top 3 rows describe devices that I have immediate access to, while the last two describe Apple's latest iPhone and iPad offerings (<a href=https://9to5mac.com/2023/11/18/iphone-ram-list/>source</a>).</figcaption></figure><p>The available RAM on Apple&rsquo;s latest iPhone and iPad models is a testament to its commitment to <a href=https://www.apple.com/newsroom/2024/10/apple-intelligence-is-available-today-on-iphone-ipad-and-mac/ target=_blank rel=noopener>cram AI tools and features into every part of the Apple user experience</a>. Of course, it&rsquo;s also wild to think that the latest iPhone is just as computationally strong — at least in RAM — as my laptop. This also means that if I could feed images into the iOS MLC Chat Engine, then I should be able to very easily deploy my multi-modal model setup on an iPhone — just not my phone.</p><p>That being said, I&rsquo;m not in a rush to get the latest iPhone. I just have something to look forward to when its inevitably time to replace my current one.</p><h3 id=changing-directions>Changing Directions</h3><p>At this point, I&rsquo;ve ruled out the possibility of deploying my multi-modal foundation model on any Apple device, especially those within my immediate reach.</p><p>Now, I&rsquo;m trying to contend with two different mysteries:</p><ol><li>Why does MLC natively support some LLaVA models, but doesn&rsquo;t support serving images? The main added value of the LLaVA model family is their exceptional ability to handle multi-modal inputs.</li><li>Does the MLC Engine support serving a model images in any other platform-specific chat engine implementation?</li></ol><p>Realistically, I don&rsquo;t think that I&rsquo;m going to get an answer to my first question unless I manage to chase down some of the <a href=https://github.com/mlc-ai/mlc-llm/graphs/contributors target=_blank rel=noopener>MLC project&rsquo;s main contributors</a>. Fortunately, the second question is easier to answer. If you recall from <a href=../edge-llm-mlc/>my first blog post</a>, MLC implements their own version of <a href=https://platform.openai.com/docs/api-reference/introduction target=_blank rel=noopener>OpenAI&rsquo;s Python API</a>.</p><p>Since the original Python API supports serving images, there&rsquo;s a good chance that the MLC implementation might offer the same functionality.</p><figure><img src=images/openai_image_url.png><figcaption>The Machine Learning Compiler bases their Python API off of OpenAI's well-established implementation. As we can see in the official OpenAI Python API documentation, the OpenAI implementation does support serving a model images (<a href=https://platform.openai.com/docs/guides/fine-tuning#vision>source</a>).</figcaption></figure><p>Sure enough, I look through when I look through the <code>mlc-llm-cpu</code> library&rsquo;s source code, I find the same <code>image_url</code> variable in their conversation protocol definition.</p><pre><code class=language-python>class Conversation(BaseModel):
    ...

    def as_prompt(self, config=None) -&gt; List[Any]:
        ...
            for item in content:
                assert isinstance(item, dict), &quot;Content should be a string or a list of dicts&quot;
                assert &quot;type&quot; in item, &quot;Content item should have a type field&quot;
                if item[&quot;type&quot;] == &quot;text&quot;:
                    message = self.role_templates[role].replace(
                        MessagePlaceholders[role.upper()].value, item[&quot;text&quot;]
                    )
                    message_list.append(message)
                # MLC supports passing image URLs via its Python API
                elif item[&quot;type&quot;] == &quot;image_url&quot;:
                    assert config is not None, &quot;Model config is required&quot;
                    image_url = _get_url_from_item(item)
                    message_list.append(data.ImageData.from_url(image_url, config))
                    message_list.append(&quot;\n&quot;)
                else:
                    raise ValueError(f&quot;Unsupported content type: {item['type']}&quot;)

            message_list.append(separator)

        ...
        return prompt

</code></pre><p>Meaning, I still can implement my multi-modal chat pipeline via a Python script.</p><br><h3 id=attempt-2-deploying-my-multi-modal-model-as-a-python-cli>Attempt 2: Deploying my multi-modal model as a Python CLI</h3><p>At this point, I just want to see if I could successfully deploy the embedded multi-modal foundation model on some device that I own. So, I wrote a very simple Python script that lets a user interact with my chat pipeline via the commandline. If the user provides an image filepath, the model will check if it&rsquo;s there. If so, underneath the hood, LLaVA is annotating this image for Gemma 2B. If not, my chatbot will kindly ask the user to check if the image is really there. Otherwise, Gemma 2B will be doing all the talking.</p><p>Here&rsquo;s the end result:</p><figure><img src=images/demo.gif><figcaption>My embedded multi-modal model recognizes the cute, cartoon husky in the supplied photo. It's also capable of generating an interesting story synopsis for what could possibly become a new international bestseller.</figcaption></figure><p>As you can see, the LLaVA model was able to recognize the Husky in the cartoon image and pass this information along to Gemma. Afterwards, Gemma was able to quickly help me draft an initial synopsis for about a day in the life of Barnaby, a sea-faring Husky who travels the world in his tiny yellow submarine. If we had a bit more compute power and a stronger LLaVA model, perhaps we could also extend this system to support image generation. If so, do think you &ldquo;Barnaby&rsquo;s Deep Sea Adventures&rdquo; could become an international bestseller?</p><blockquote><p>For the exact source code used in this demo, please refer to <a href=https://github.com/bellanich/pocket-llm/tree/main/src target=_blank rel=noopener>this blog post&rsquo;s corresponding GitHub repository</a>.</p></blockquote><h2 id=conclusion>Conclusion</h2><p>It&rsquo;s truly incredible seeing how quickly embedded systems technology has progressed in this domain. The tech giants are definitely in a comfortable place when it comes to deploying sophisticated edge foundation models, but the open source tooling is still in its early phase. Through this project, I&rsquo;ve defined stumbled over a few vague, low-level code errors of my own. Tools like the MLC framework (in their current iteration) well-suited for those of us who are:</p><ol><li>Well-versed in the latest deep learning research</li><li>Comfortable debugging low code system errors</li><li>Reasonably proficient in Swift (or similar programming languages)</li><li>Own latest edge devices</li></ol><p>Of course, that&rsquo;s a lot of conditions. Nonetheless, it&rsquo;s incredible seeing how projects like the Machine Learning Compiler have democratized access to foundation models. Whether you&rsquo;re moonlighting as an entrepreneurial or are a curious university student, you can get easily started deploying your own single-modal LLMs on edge today. Maybe in another 6 months, the tooling will have advanced to the point that it fully supports multi-modal chat — including image generation.</p><p>In the meantime, I plan to watch these industry development closely. While I&rsquo;ve put my hopes of a private, multi-modal travel assistant on the shelf, I haven&rsquo;t abandoned them entirely. Stay tuned to see what I&rsquo;ll do next. 👋</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/embedded-systems/>Embedded Systems</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-llm-app/&amp;text=Shrinking%20the%20Impossible%20%28Part%204%29:%20Deploying%20My%20Own%20Pocket-Sized%20Multi-Modal%20Large%20Language%20Model" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-llm-app/&amp;t=Shrinking%20the%20Impossible%20%28Part%204%29:%20Deploying%20My%20Own%20Pocket-Sized%20Multi-Modal%20Large%20Language%20Model" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Shrinking%20the%20Impossible%20%28Part%204%29:%20Deploying%20My%20Own%20Pocket-Sized%20Multi-Modal%20Large%20Language%20Model&amp;body=https://bellanich.github.io/post/edge-llm-app/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-llm-app/&amp;title=Shrinking%20the%20Impossible%20%28Part%204%29:%20Deploying%20My%20Own%20Pocket-Sized%20Multi-Modal%20Large%20Language%20Model" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Shrinking%20the%20Impossible%20%28Part%204%29:%20Deploying%20My%20Own%20Pocket-Sized%20Multi-Modal%20Large%20Language%20Model%20https://bellanich.github.io/post/edge-llm-app/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-llm-app/&amp;title=Shrinking%20the%20Impossible%20%28Part%204%29:%20Deploying%20My%20Own%20Pocket-Sized%20Multi-Modal%20Large%20Language%20Model" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu72f3a3ef3a65e4417b2658a21b6b7f3a_388362_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-llm-embed-llava/>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</a></li><li><a href=/post/edge-llm-vision-encoders/>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</a></li><li><a href=/post/edge-llm-mlc/>Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC</a></li><li><a href=/post/reinforcement-learning/>Reinforcement Learning: Investigating Gradient Stability in Policy Based Methods</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>