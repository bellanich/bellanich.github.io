<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="A tiny diffusion model, a mobile device, and a surprising amount of magic — here’s how I built a pocket-sized photobooth that can whisk real people into new worlds in under 30 seconds."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-diffusion-3/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-diffusion-3/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-diffusion-3/"><meta property="og:title" content="Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth | Bella Nicholson"><meta property="og:description" content="A tiny diffusion model, a mobile device, and a surprising amount of magic — here’s how I built a pocket-sized photobooth that can whisk real people into new worlds in under 30 seconds."><meta property="og:image" content="https://bellanich.github.io/post/edge-diffusion-3/featured.jpeg"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-diffusion-3/featured.jpeg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-11-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-26T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-diffusion-3/"},"headline":"Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth","image":["https://bellanich.github.io/post/edge-diffusion-3/featured.jpeg"],"datePublished":"2025-11-26T00:00:00Z","dateModified":"2025-11-26T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"A tiny diffusion model, a mobile device, and a surprising amount of magic — here’s how I built a pocket-sized photobooth that can whisk real people into new worlds in under 30 seconds."}</script><title>Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Nov 26, 2025
</span><span class=middot-divider></span>
<span class=article-reading-time>11 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:483px><div style=position:relative><img src=/post/edge-diffusion-3/featured_hue94e061b0176e26eb706b0040898b634_698751_720x0_resize_q98_lanczos.jpeg alt class=featured-image>
<span class=article-header-caption>Image credit: <a href=https://blog.google/technology/ai/nano-banana-pro/ target=_blank rel=noopener><strong>Nano Banana Pro</strong></a></span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#a-three-stage-approach>A Three-Stage Approach</a><ul><li><a href=#stage-1-person-segmentation-1s>Stage 1: Person Segmentation (~1s)</a></li><li><a href=#stage-2-conditional-background-generation>Stage 2: Conditional Background Generation</a><ul><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#thread-safety-and-process-survival>Thread Safety and Process Survival</a></li></ul></li><li><a href=#stage-3-compositing--style-filters-1s>Stage 3: Compositing & Style Filters (&lt;1s)</a></li></ul></li><li><a href=#lessons-from-the-edge>Lessons from the Edge</a><ul><li><a href=#1-at-small-scales-hardware-aware-wins>1. At small scales, hardware-aware wins</a></li><li><a href=#2-if-your-prompt-loses-focus-stable-diffusion-will-too>2. If your prompt loses focus, Stable Diffusion will too</a></li><li><a href=#3-architecture-solves-capability-gaps>3. Architecture solves capability gaps</a></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#future-directions>Future Directions</a></li><li><a href=#the-joy-of-building-small>The Joy of Building Small</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>I&rsquo;m on a personal mission to recreate the Nano Banana experience on the edge&mldr;or at least get as close as physics and open-source tools will allow. In <a href=../edge-diffusion-1/>my first blogpost</a>, I explained why Apple&rsquo;s CoreML Stable Diffusion (SD) is my best bet. In <a href=../edge-diffusion-2/>the last post</a>, I broke down how Apple squeezed a 6GB model down to 1.5GB while still delivering sub-10-second generation on iOS.</p><p>But there&rsquo;s catch: Apple’s implementation breaks my use case. If I try a simple img2img portrait edit, the subject&rsquo;s identity collapses. As seen in Figure 1, once I make the denoising strength high enough to change the background, my subject morphs into a loosely related stranger who just happens to be wearing a similar outfit.</p><figure><img src=images/failed-portrait-edit.png style=width:100%;height:auto><figcaption><strong>Figure 1.</strong> Apple's CoreML Stable Diffusion models fails to maintain character consistency. We attempt to transport the lovely Sabrina Carpenter from the 2025 MTV Video Music Awards (<a href=https://www.gettyimages.com/detail/news-photo/sabrina-carpenter-winner-of-the-the-best-album-award-for-news-photo/2233699406?>image credit</a>) to a festive holiday backdrop. The background and prop swaps are convincing, but the resulting blonde is definitely not Sabrina.</figcaption></figure><p>If I rely solely on Apple&rsquo;s CoreML Stable Diffusion model, I run into an impossible trade-off:</p><ul><li><strong>Low strength (0.3-0.5):</strong> Character consistency is maintained, but the background barely changes</li><li><strong>High strength (0.7-0.9):</strong> Background transforms perfectly to align with the given text prompt; however, the person pictures just becomes unrecognizable.</li></ul><p>This is hardly a surprise, since <strong>(a)</strong> the original Nano Banana model (released in August 2025) broke the internet for its ability to maintain character consistency and <strong>(b)</strong> we&rsquo;re working with a hyper-optimized version of a 2022 model. The problem with our approach is that Stable Diffusion can&rsquo;t distinguish between &ldquo;keep this&rdquo; and &ldquo;change that&rdquo;. It&rsquo;s trying to equally transform each pixel. Hence, it&rsquo;s trying to do two conflicting jobs at once: preserve user identity <em>and</em> dramatically transform the background.</p><p>The lesson? Stop asking Stable Diffusion to multitask. I need to handle identity preservation and scene transformation separately. This blogpost shows how to do this on a shoestring compute budget.</p><h2 id=a-three-stage-approach>A Three-Stage Approach</h2><p>I’m a fan of simple solutions, especially under a tight runtime budget. So, I started with the simplest move possible: I isolated the subject and focused Stable Diffusion&rsquo;s efforts on background generation. This created a lightweight, three-stage pipeline:</p><ol><li><p><strong>Segment.</strong> I used <a href=https://developer.apple.com/documentation/vision target=_blank rel=noopener>Apple&rsquo;s Vision framework</a> to perform person segmentation. This process yields <strong>(a)</strong> a cutout of the person with transparent background, and <strong>(b)</strong> an inverted mask marking which pixels need regeneration.</p></li><li><p><strong>Generate.</strong> I feed the inverted mask and text prompt into Stable Diffusion&rsquo;s img2img pipeline. SD regenerates only the masked background regions while leaving the subject&rsquo;s pixels untouched.</p></li><li><p><strong>Composite.</strong> I then layer the original subject cutout over the newly generated background. In order to deliver a photo booth-like user experience, I also added optional Instagram-style filters to make the final outputs more shareable.</p></li></ol><figure><img src=images/pipeline-diagram.png style=width:35%;height:auto><figcaption><strong>Figure 2. </strong>My hyper-optimized three-stage pipeline uses <a href=https://github.com/apple/ml-stable-diffusion>Apple's CoreML Stable Diffusion model</a> for on-device conditional image generation. By isolating the heavy diffusion step to background regeneration, the system preserves identity consistency despite the tiny model's quality constraints.</figcaption></figure><p>The final result is a lean, fully on-device conditional image generation pipeline that runs within an average of ~27 seconds. This puts me safely below my 60 second limit.</p><p>Now, let&rsquo;s dive into the details. To demonstrate each stage&rsquo;s output, we&rsquo;ll successfully transport Sabrina Carpenter from the concert stage to a Winter Wonderland.</p><figure><img src=images/original_photo.jpg style=width:100%;height:auto><figcaption><strong>Figure 3. </strong>Let's assume this concert photo of Sabrina Carpenter, performing onstage during the 2024 Coachella Valley Music and Arts Festival, as our running example. Our goal is to transport her to a Winter Wonderland (<a href=https://www.gettyimages.com/detail/news-photo/sabrina-carpenter-performs-at-the-coachella-stage-during-news-photo/2149329158>image credit</a>).</figcaption></figure><h3 id=stage-1-person-segmentation-1s>Stage 1: Person Segmentation (~1s)</h3><p>First, I extract the subject from the background using Apple&rsquo;s <a href=https://developer.apple.com/documentation/vision target=_blank rel=noopener>Vision framework</a>, specifically the <a href=https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest target=_blank rel=noopener><code>VNGeneratePersonSegmentationRequest</code></a> API. This built-in segmentation model ships with iOS and is already optimized for the Neural Engine.</p><p>Deploying Apple&rsquo;s off the shelf solution let&rsquo;s me focus on core problem without getting distracted by additional deployment overhead. Apple has already hyper-optimized this image segmentation model for their Apple Neural Engine (ANE) hardware accelerator. Meaning, even when I set <a href=https://developer.apple.com/documentation/vision/vngeneratepersonsegmentationrequest/qualitylevel/accurate target=_blank rel=noopener>the preferred quality level to high</a>, the segmentation model still returns a result within ~1 second. I&rsquo;ve alloted about ~67% of my inference time budget to Stable Diffusion (40 seconds) and the remainder to everything else. Keeping the segmentation step leaves me with plenty of breathing room.</p><p>Figure 3 shows an example of this model&rsquo;s outputs, where its separates the subject from her surroundings.</p><figure style=display:flex;flex-direction:column;align-items:center;text-align:center><div style=display:flex;justify-content:center;gap:10px;align-items:flex-end><div style=width:45%;display:flex;flex-direction:column><img src=images/1_isolated_subject.png style=width:100%;height:auto;max-height:300px;object-fit:contain><div style=margin-top:5px;font-size:.75em>(a)</div></div><div style=width:45%;display:flex;flex-direction:column><img src=images/1_background_mask.png style=width:100%;height:auto;max-height:300px;object-fit:contain><div style=margin-top:5px;font-size:.75em>(b)</div></div></div><figcaption style=margin-top:15px><strong>Figure 4.</strong> Sample person segmentation results. <strong>(a)</strong> The subject is now isolated against an alpha transparency background. <strong>(b)</strong> A background mask for subsequent conditional image generation, where only white pixels will be repainted.</figcaption></figure><p>Now, let’s whisk Sabrina Carpenter off the Coachella stage and drop her straight into a glittery Winter Wonderland for a festive, snow-dusted performance.</p><h3 id=stage-2-conditional-background-generation>Stage 2: Conditional Background Generation</h3><p>This step is where the magic happens — and where most of my runtime budget disappears. I feed the background mask from Stage 1 (see Figure 4C) and the text prompt into the Stable Diffusion. The mask acts like a stencil: white regions get regenerated, black pixels (the subject) stay untouched. Everything gets resized to 512×512 before inference, since <a href=https://arxiv.org/abs/2112.10752 target=_blank rel=noopener>that’s SD’s native training resolution</a>.</p><p>For denoising strength, I stayed within <a href=https://huggingface.co/docs/diffusers/en/using-diffusers/write_your_own_pipeline#classifier-free-guidance target=_blank rel=noopener>the commonly recommended 0.65–0.85 range</a>: low enough to preserve subject boundaries, high enough to meaningfully transform the background. I used the standard <a href=../edge-diffusion-2/#scheduler-optimization>25 DPM-Solver steps</a> and set the default guidance scale to 7.5.</p><h4 id=prompt-engineering>Prompt Engineering</h4><p>Prompt engineering took longer than I&rsquo;d like to admit. I wanted to create a visually striking background, so I started with maximalist prompts (<code>"A winter wonderland with snow-covered pine trees, twinkling fairy lights, ice sculptures, frosted windows..."</code>). CoreML Stable Diffusion got overwhelmed and returned incoherent mush. Then I went ultra-minimal (<code>"A winter scene"</code>) and got a bleak, featureless white void.</p><p>The sweet spot was photography-style phrasing with a few concrete details, like <code>"A glittery winter wonderland with snow, twinkling lights, warm glow"</code>. Enough direction, not enough to overwhelm. Along the way, I learned:</p><ul><li>Stable Diffusion trims anything past ~75 tokens</li><li>Evocative scene vibes are better than itemized lists</li><li>Lighting cues, like “warm orange glow” vs. “blue hour twilight”, can set the entire mood</li></ul><p>Now, let’s see what all that work actually produces. Here’s the raw background Stable Diffusion generated before the subject gets composited back in (Figure 5).</p><figure><img src=images/2_generated_background.png style=width:50%;height:auto><figcaption><strong>Figure 5.</strong> Stable Diffusion’s raw output for the prompt “outside in magical winter village at blue hour lighting, charming snow-covered cottage with glowing windows.” The scene is coherent, though not perfect, details like the opaque “window/door” remain ambiguous.</figcaption></figure><h4 id=thread-safety-and-process-survival>Thread Safety and Process Survival</h4><p>Running a Stable Diffusion pipeline on-device means juggling two hard problems:</p><ol><li><strong>Thread safety.</strong> Segmentation, SD inference, and UI updates all touch the same shared state, creating the perfect incubator for race conditions.</li><li><strong>Process survival.</strong> I need to keep the UI responsive while SD runs for ~27 seconds in the background. At the same time, iOS locks the screen after 30 seconds of inactivity and suspends the app, which kills image generation.</li></ol><p>In short, I had to choose between concurrency or chaos. I enabled Swift 6&rsquo;s strict concurrency to catch threading bugs at compile time rather than dealing with surprises in production. With strict concurrency, everything needs explicit actor boundaries. The UI state (<code>@Published</code> properties, view model updates) runs on the main thread, while Stable Diffusion inference runs on background threads to prevent freezing the entire app.</p><pre><code class=language-swift>// Simplified coordinator pattern
func generateBackground() {
    isProcessing = true  // MainActor UI update

    Task.detached {  // Background thread for heavy work
        let result = await pipeline.generate(...)

        await MainActor.run {  // Back to MainActor for UI
            self.outputImage = result
            self.isProcessing = false
        }
    }
}
</code></pre><p style=text-align:center;font-size:.75em;margin-top:.5em;color:#66><strong>Figure 6.</strong> Actor coordination pattern in Swift. The thread hopping pattern runs inference on a background thread, then returns to the main thread (<code>@MainActor</code>) for UI updates.</p><p>I also registered Stable Diffusion as a background task to ensure image generation continues if the screen locks. Without it, we&rsquo;d be left with half a cottage and no Winter Wonderland magic. Once the final image is composited, the background task is released.</p><h3 id=stage-3-compositing--style-filters-1s>Stage 3: Compositing & Style Filters (&lt;1s)</h3><p>With the background generated, I&rsquo;m ready to layer the isolated subject (Fig. 4a) into the new scene. I use <a href=https://developer.apple.com/documentation/coregraphics target=_blank rel=noopener>Core Graphics</a> Apple&rsquo;s low-level 2D rendering framework, to composite these two layers together. This process is fast, clean, and basically free in terms of runtime.</p><figure><img src=images/3_original_output.jpg style=width:100%;height:auto><figcaption><strong>Figure 7. </strong>Let's assume this concert photo of Sabrina Carpenter, performing onstage during the 67th Annual GRAMMY Awards, as our running example. Our goal is to transport her to a Winter Wonderland (<a href=https://github.com/apple/ml-stable-diffusion>image credit</a>).</figcaption></figure><p>Originally, I planned to chain multiple Stable Diffusion generations together to create a flexible style transfer experience, allowing the user to further personalize their images, but each extra pass incurs another ~25 seconds. No one is going to wait 2+ minutes to try on a different look.</p><p>So I switched to <a href=https://developer.apple.com/documentation/coreimage target=_blank rel=noopener>Core Image filters</a>, which run instantly. I added four curated styles plus an intensity slider, letting users experiment in real time, turning the whole system into a fully on-device, pocket-sized photobooth. Figure 8 highlights a few results, any of which could slide neatly onto Sabrina’s holiday-themed merch.</p><figure style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><div style=display:flex;flex-direction:column;gap:15px;width:100%><div style=display:flex;justify-content:center;gap:10px><div style=width:45%><img src=images/3_vintage_output.jpg style=width:100%;height:auto><div style=margin-top:5px;font-size:.7em;text-align:center>(a) Vintage</div></div><div style=width:45%><img src=images/3_pop_art_output.jpg style=width:100%;height:auto><div style=margin-top:5px;font-size:.7em;text-align:center>(b) Pop Art</div></div></div><div style=display:flex;justify-content:center;gap:10px><div style=width:45%><img src=images/3_posterize_output.jpg style=width:100%;height:auto><div style=margin-top:5px;font-size:.7em;text-align:center>(c) Posterize</div></div><div style=width:45%><img src=images/3_mosiac_output.jpg style=width:100%;height:auto><div style=margin-top:5px;font-size:.7em;text-align:center>(d) Mosaic</div></div></div><figcaption style=margin-top:1em;text-align:center><strong>Figure 8.</strong> The same composited image with different Core Image filter styles applied. Each filter applies instantly (&lt;1s) with adjustable intensity.</figcaption></figure><p>Of course, this system isn’t a one-hit wonder. Figure 9 shows the same pipeline dropping Sabrina underneath a Christmas tree, into San Diego’s Balboa Park, and even into a groovy reimagining of La Jolla Cove, proving that this pocket-sized photobooth travels just as well as she does.</p><figure style="margin:2em 0"><div style=display:grid;grid-template-columns:repeat(3,1fr);gap:8px><div><img src=images/3_gifts.PNG style=width:100%;height:200px;object-fit:cover;margin-bottom:3px>
<img src=images/3_gifts_sabrina.JPG style=width:100%;height:200px;object-fit:cover><div style=margin-top:3px;font-size:.75em;text-align:center>(a) Gradient Gift Descent</div></div><div><img src=images/3_balboa.PNG style=width:100%;height:200px;object-fit:cover;margin-bottom:3px>
<img src=images/3_balboa_sabrina.JPG style=width:100%;height:200px;object-fit:cover><div style=margin-top:3px;font-size:.75em;text-align:center>(b) Balboa Park</div></div><div><img src=images/3_la_jolla.PNG style=width:100%;height:200px;object-fit:cover;margin-bottom:3px>
<img src=images/3_la_jolla_sabrina.JPG style=width:100%;height:200px;object-fit:cover><div style=margin-top:3px;font-size:.75em;text-align:center>(c) La Jolla Cove</div></div></div><figcaption style=text-align:center;margin-top:1em><strong>Figure 9.</strong> Sabrina Carpenter, re-imagined in three different scenes. Show both the raw Stable Diffusion generated backgrounds (top) and the final composites (bottom).</figcaption></figure><h2 id=lessons-from-the-edge>Lessons from the Edge</h2><p>When I started this project, I knew bringing the Nano Banana experience to mobile would be tough, but I just didn’t realize how tough. I learned four valuable lessons along the way:</p><h3 id=1-at-small-scales-hardware-aware-wins>1. At small scales, hardware-aware wins</h3><p>Users won’t accept bad images just because the model runs fast. Once we shrink diffusion, quality depends heavily on hardware we don’t control or fully understand. Apple’s vertical integration makes some optimizations look effortless, but replicating them from the outside is anything but.</p><h3 id=2-if-your-prompt-loses-focus-stable-diffusion-will-too>2. If your prompt loses focus, Stable Diffusion will too</h3><p>I learned quickly that mixing themes (e.g., Winter Wonderland + robots) just produces incoherent mush. Chaining prompts or tweaking denoising strength didn’t help either: high strength erased the scene, low strength lead to incoherent, blurry transformation.</p><p>Blending multiple semantic concepts is a completely different problem, one tackled by disentangled-control methods like <a href=https://arxiv.org/abs/2302.05543 target=_blank rel=noopener>ControlNet</a> and <a href=https://arxiv.org/abs/2308.06721 target=_blank rel=noopener>IP-Adapter</a>. But those techniques rely on extra conditioning modules that add hundreds of megabytes and several seconds of latency. That&rsquo;s fine on a workstation, disastrous for a sub-30-second mobile experience.</p><h3 id=3-architecture-solves-capability-gaps>3. Architecture solves capability gaps</h3><p>Splitting the process into Segment → Generate → Composite avoided the pitfalls of an all-in-one model. Segmentation preserved identity, SD produced high-quality backgrounds, and fast filters enabled rapid style iteration. Even if on-device disentangled-control were possible, the gains for the end user would be minimal. The modular workflow already delivers strong speed and consistency. This model has its limitations, but good system design works around them.</p><p>Of course, as base model improves, so does the space for smarter systems built around its new limits.</p><h2 id=conclusion>Conclusion</h2><p>I accomplished my original goal of building a Nano-Banana–style photobooth that runs entirely on-device. Along the way, I also ended up with a compact computer-vision playground ready for whatever comes next.</p><h3 id=future-directions>Future Directions</h3><p>From here, my options are boundless but they include: automating the tedious prompt engineering process using <a href=https://arxiv.org/abs/2305.13301 target=_blank rel=noopener>reinforcement learning techniques like DDPO</a>, trying to recreate some aspects of <a href=https://arxiv.org/abs/2506.06802 target=_blank rel=noopener>identity-preserving style transfer</a> on-device, or fine-tuning my tiny Stable Diffusion model with <a href=https://huggingface.co/blog/lora target=_blank rel=noopener>LoRA adapters</a> so it knows why <a href=https://www.npr.org/2025/09/10/nx-s1-5535826/sabrina-carpenter-mans-best-friend-charts target=_blank rel=noopener>Sabrina Carpenter is &ldquo;Man&rsquo;s Best Friend&rdquo;</a>. I now have a solid foundation and free to explore what’s genuinely interesting.</p><h3 id=the-joy-of-building-small>The Joy of Building Small</h3><p>A GPU cluster would brute-force most of the problems I hit, edge constraints push me to invent better solutions. With tiny, local models, I skip cloud overhead, iterate faster, and avoid unwanted surprise bills. Starting at the bottom of <a href=https://openai.com/index/scaling-laws-for-neural-language-models/ target=_blank rel=noopener>the scale curve</a> is liberating: any capability I unlock here will only get stronger as the model scales. And in the end, the setup stays small, but the possibilities don’t.</p><p>I’m excited to keep iterating on this work. If you want to dig into the details, the full implementation is on <a href=https://github.com/bellanich/pocket-diffusion target=_blank rel=noopener>GitHub</a>. Questions, feedback, or wild ideas? Drop a comment or reach out on <a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener>LinkedIn</a>. I always enjoy meeting people working at the edge of what&rsquo;s possible — on-device or in the cloud.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/computer-vision/>Computer Vision</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-diffusion-3/&amp;text=Frames%20Per%20Second%20%28Part%203%29:%20Turning%20a%20Tiny%20Diffusion%20Model%20into%20a%20Traveling%20Photobooth" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-diffusion-3/&amp;t=Frames%20Per%20Second%20%28Part%203%29:%20Turning%20a%20Tiny%20Diffusion%20Model%20into%20a%20Traveling%20Photobooth" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Frames%20Per%20Second%20%28Part%203%29:%20Turning%20a%20Tiny%20Diffusion%20Model%20into%20a%20Traveling%20Photobooth&amp;body=https://bellanich.github.io/post/edge-diffusion-3/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-diffusion-3/&amp;title=Frames%20Per%20Second%20%28Part%203%29:%20Turning%20a%20Tiny%20Diffusion%20Model%20into%20a%20Traveling%20Photobooth" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Frames%20Per%20Second%20%28Part%203%29:%20Turning%20a%20Tiny%20Diffusion%20Model%20into%20a%20Traveling%20Photobooth%20https://bellanich.github.io/post/edge-diffusion-3/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-diffusion-3/&amp;title=Frames%20Per%20Second%20%28Part%203%29:%20Turning%20a%20Tiny%20Diffusion%20Model%20into%20a%20Traveling%20Photobooth" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu72f3a3ef3a65e4417b2658a21b6b7f3a_388362_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-diffusion-2/>Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion</a></li><li><a href=/post/edge-diffusion-1/>Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model</a></li><li><a href=/post/edge-llm-app/>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</a></li><li><a href=/post/edge-llm-embed-llava/>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</a></li><li><a href=/post/edge-llm-vision-encoders/>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>