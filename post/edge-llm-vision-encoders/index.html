<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="Vision transformers, with help from training frameworks like CLIP and SigLIP, make  multi-modal foundation models like LLaVA possible — bridging the gap between vision and text."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-llm-vision-encoders/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-llm-vision-encoders/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-llm-vision-encoders/"><meta property="og:title" content="Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP | Bella Nicholson"><meta property="og:description" content="Vision transformers, with help from training frameworks like CLIP and SigLIP, make  multi-modal foundation models like LLaVA possible — bridging the gap between vision and text."><meta property="og:image" content="https://bellanich.github.io/post/edge-llm-vision-encoders/featured.png"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-llm-vision-encoders/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-11-30T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-30T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-llm-vision-encoders/"},"headline":"Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP","image":["https://bellanich.github.io/post/edge-llm-vision-encoders/featured.png"],"datePublished":"2024-11-30T00:00:00Z","dateModified":"2024-11-30T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"Vision transformers, with help from training frameworks like CLIP and SigLIP, make  multi-modal foundation models like LLaVA possible — bridging the gap between vision and text."}</script><title>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Nov 30, 2024
</span><span class=middot-divider></span>
<span class=article-reading-time>13 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:449px><div style=position:relative><img src=/post/edge-llm-vision-encoders/featured_hu93c63426a8ef102c91039ee696f86159_814957_720x0_resize_lanczos_3.png alt class=featured-image>
<span class=article-header-caption>Image credit: <a href=https://github.com/black-forest-labs/flux target=_blank rel=noopener><strong>Generated using FLUX</strong></a></span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#llava-chatbots-that-can-see>LLaVA: Chatbots That Can &ldquo;See&rdquo;</a><ul><li><a href=#visual-instruction-tuning-of-text-decoders>Visual Instruction Tuning of Text Decoders</a></li><li><a href=#vision-encoders>Vision Encoders</a></li><li><a href=#training-the-vision-encoder>Training the Vision Encoder</a><ul><li><a href=#clip>CLIP</a></li><li><a href=#siglip>SigLIP</a></li></ul></li><li><a href=#inference>Inference</a></li></ul></li><li><a href=#machine-learning-compiler-implementation>Machine Learning Compiler Implementation</a><ul><li><a href=#the-llava-model-family-on-mlc>The LLaVA Model Family on MLC</a></li><li><a href=#llava-onevision>LLaVA OneVision</a></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#whats-next>What&rsquo;s next?</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>In <a href=../edge-llm-mlc/>my last blog post</a>, I introduced you to the fascinating world of edge foundation models. I was dreaming big, imagining a foundation model that could &ldquo;see&rdquo; — well, at least understand the photos I share with it. Let&rsquo;s be honest, I’ll probably need some help when I’m lost in a new city on my next vacation. A model that understands both text and images? Way more useful when I&rsquo;m wandering around than a single-modal chatbot!</p><p>Recently, things have been getting pretty exciting in the world of multi-modal models. Beyond just text and images, <a href=https://openai.com/index/chatgpt-can-now-see-hear-and-speak/ target=_blank rel=noopener>ChatGPT can now &ldquo;see, hear, and speak&rdquo;</a> — processing text, images, and audio. There&rsquo;s also <a href=https://deepmind.google/technologies/gemini/ target=_blank rel=noopener>Gemini</a>, Google’s latest powerhouse, which can process everything from text to images to audio — <a href=https://deepmind.google/technologies/gemini/pro/ target=_blank rel=noopener>and even long movies, thanks to its multi-million token context window</a>. Sounds pretty impressive, right? But here&rsquo;s the catch: these models are so large and computationally demanding that it’s nearly impossible to run them on edge devices (like phones and laptops).</p><p>In this blog post, we’ll explore some of the latest advancements in small, efficient multi-modal models that can actually be deployed on edge devices. We&rsquo;ll introduce the <a href=https://llava-vl.github.io/ target=_blank rel=noopener>LLaVA</a> <a href=https://huggingface.co/llava-hf target=_blank rel=noopener>model family</a>, which combines vision encoders and text decoders to provide a general-purpose multi-modal chatbot.</p><figure><img src=images/llava_v1_5_performance.jpg width=70%><figcaption>The LLaVA model family stands out as a high-performance, open-source collection of models. Upon its release, the LLaVA-1.5 series achieved state-of-the-art (SoTA) performance across 11 benchmarks (<a href=https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15>image credit</a>).</figcaption></figure><p>We’ll also take a closer look at the architecture behind LLaVA—specifically the vision encoders and text decoders that make it work. This includes the popular CLIP and SigLIP training frameworks.</p><p>Understanding how these models work lays the groundwork for <a href=../edge-llm-embed-llava/>my next blog post</a>.</p><br><h2 id=llava-chatbots-that-can-see>LLaVA: Chatbots That Can &ldquo;See&rdquo;</h2><p>The LLaVA model family is a collection of vision-language models that use pre-trained Vision Encoders to give pre-trained Large Language Models (LLMs) the ability to understand images. Why all the pre-training? Well, pre-trained models help keep training costs low while still leveraging the latest in foundation model technology.</p><p>This combination of pre-trained models has made LLaVA models increasingly popular for multi-modal tasks. These models — <a href=https://huggingface.co/llava-hf/bakLlava-v1-hf target=_blank rel=noopener>some named more creatively than others</a> — are open-sourced in various sizes, ranging from 0.5B to 13B parameters.</p><figure><img src=images/bakllava_model_card.png width=80%><figcaption>Most LLaVA models are named after their base transformer architectures, but this one has been named after a beloved Turkish delicacy (<a href=https://huggingface.co/llava-hf/bakLlava-v1-hf>source</a>).</figcaption></figure><p>Don’t forget that the <a href=https://llm.mlc.ai target=_blank rel=noopener>Machine Learning Compiler (MLC) Engine</a> (introduced in <a href=../edge-llm-mlc/>my last blog post</a>) supports quantizing LLaVA models. In other words, the smallest LLaVA models are promising candidates for my edge multi-modal ambitions.</p><h3 id=visual-instruction-tuning-of-text-decoders>Visual Instruction Tuning of Text Decoders</h3><p>Before we jump into how LLaVA works its magic, let’s take a quick look at how plain text-based LLMs operate. A typical Large Language Model (LLM) begins by breaking your input text into discrete units called <strong>tokens</strong> using a tokenizer. These tokens are then transformed into high-dimensional embeddings — essentially numerical representations the model can &ldquo;understand&rdquo;. The embeddings pass through multiple layers of self-attention mechanisms and feed-forward neural networks, which process the information and predict the next token in the sequence. This process continues iteratively until the model generates the desired output text.</p><p>But here’s the challenge: when it comes to multi-modal tasks, like combining images and text, your traditional text-based LLM hits a wall — it simply can’t “see”. To fix this, we bring in vision encoders. Vision encoders translate images into embeddings, a deep learning model&rsquo;s &ldquo;native language&rdquo;. Afterwards, a text decoder translates these embeddings into an output text based on both the image and the text input. We align the feature space of the vision encoder with the LLM by adding a trainable linear projection layer on top of the vision embeddings. By fine-tuning the text decoder on a multi-modal dataset, the model learns to generate text that is relevant to the input image.</p><figure><center><img src=images/llava_onevision.png></center><figcaption>The LLaVA model family combines a vision encoder with a text decoder to generate human-like text based on images. The vision encoder converts images into embeddings, which are then fed into the text decoder as a visual instruction to generate the output text (<a href=[url](https://arxiv.org/abs/2408.03326)>source</a>).</figcaption></figure><p>This approach is called <strong><a href=https://arxiv.org/abs/2304.08485 target=_blank rel=noopener>Visual Instruction Tuning</a></strong>. If you’re familiar with Instruction Tuning, it’s the same idea with a multi-modal twist. In regular instruction tuning, you give the model a text instruction (“Summarize this paragraph”) and train it to produce the desired text output. Visual Instruction Tuning swaps that text instruction for an image. The goal? Train the model to generate text that describes or explains the image.</p><p>For example, LLaVA models are trained on datasets that pair images with captions or multi-modal Q&amp;A examples. This forces them to become fluent in both visual and linguistic cues. In these finetuning setups, we commonly keep the vision encoder fixed and only fine-tune the text decoder and projection layer on the multi-modal dataset. his way, we can leverage the pre-trained vision encoder to understand images without the need for additional training data, while also benefiting from the power of the pre-trained LLM to generate human-like text.</p><h3 id=vision-encoders>Vision Encoders</h3><p>Essentially, Visual Instruction Tuning swaps out text instructions for images and teach the model to generate text based on what it “sees.” But there’s a big question here: how do we get an image - essentially a 2D grid of pixels — to become compatible with a token-consuming text-based model? While Convolutional Neural Networks (CNNs) have traditionally excelled at extracting features from images, Vision Transformers have shown promising results in processing images as sequences of tokens, similar to text.</p><p>Here’s how it works: we divide up an image into smaller, fixed-size patches rather processing it all at once. Each patch is then flattened and mapped into a high-dimensional feature space — a numerical representation that captures the patch’s visual characteristics. To make sure the model doesn’t lose track of where these patches belong in the image, we add positional encodings. A Vision Transformer then consumes these location-annotated image patches, processing them through multiple layers of self-attention and feed-forward neural networks.</p><p>This process allows the model to understand the relationships between different parts of the image and distill its content into a sequence of semantically meaningful embeddings. This sequence of embeddings is particularly compatible with classical LLMs because it mirrors the token-like structure LLMs expect for text. Thus, the combination of Vision Transformers with Text Decoders gives us high-functioning multi-modal models.</p><p>If you want to learn about Vision Transformers in more detail, I recommend taking a closer look at the <a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformers.html target=_blank rel=noopener>University of Amsterdam&rsquo;s Deep Learning Tutorials</a>. For now, let&rsquo;s focus on the training frameworks for vision encoders, as they are crucial for the performance of the final LLaVA model.</p><h3 id=training-the-vision-encoder>Training the Vision Encoder</h3><p>The quality of the final multi-modal LLaVA model heavily depends on the quality of its vision encoder. This is why it is crucial to train the vision encoder on a diverse and large-scale dataset to ensure that it can understand a wide range of images. Two popular training frameworks for vision encoders are CLIP and SigLIP. We describe both of them in a bit more detail here, since we need this knowledge to properly imbed a LLaVA model onto an edge device.</p><h4 id=clip>CLIP</h4><p><a href=https://arxiv.org/abs/2103.00020 target=_blank rel=noopener>Contrastive Language-Image Pre-training (CLIP)</a> is a pre-training framework that teaches the vision encoder to understand images by associating them with text descriptions. Here’s the gist: CLIP trains the vision encoder to predict what the image means using a text description, and vice versa, to predict the image from a text description. By doing this, the model essentially learns a shared “language” that lets it understand both images and text.</p><p>CLIP has been shown to achieve state-of-the-art performance on a wide range of vision tasks, making it a popular choice for vision encoders in multi-modal models, in particular due to its alignment of the vision and text feature spaces.</p><figure><center><img src=images/clip_diagram.svg></center><figcaption>CLIP trains vision encoders to predict image representations that align with text representations of their respective caption. By doing so contrastively over many image-text pairs in a batch, the vision encoder learns strong, semantic image embeddings (<a href=[url](https://arxiv.org/abs/2103.00020)>source</a>).</figcaption></figure><center></center><p>A key aspect in CLIP is its use of <strong>contrastive learning</strong>, where the model is trained to maximize the similarity between positive pairs (image-text pairs that belong together) and minimize the similarity between negative pairs (image-text pairs that do not belong together).</p><blockquote><p><strong>Example.</strong> The vision encoders learns to match positive pairs (like a picture of a dog with a caption saying &ldquo;a dog&rdquo;) and push apart negative pairs (like a picture of a dog with a caption saying &ldquo;a cat&rdquo;).</p></blockquote><p>The similarity is measured by a softmax, which is applied over the dot products between the representation spaces. This allows the model to learn a discriminative representation space that captures the semantic content of images and text.</p><p>While this approach is intuitive, it doesn&rsquo;t scale well. For large representation spaces, we need large batch sizes to effectively capture a large variety of (negative) image-text pairs and improve the training process. This raises challenges in CLIP, in particular with the softmax. To calculate the CLIP loss, we need to apply the batch-level softmax twice to normalize the pairwise similarity scores across all images for training the text encoder, and across all texts for training the image encoder. These passes over the full batch size can be computationally expensive, especially when the batch size is sharded across multiple devices.</p><p>In a distribution training setup, we are often using data parallelism, where each device processes a part of the batch. While commonly, each device can do the forward and backward pass independently and only the final gradients are communicated, CLIP needs to already communicate the softmax statistics for the loss between all devices. This creates an efficiency bottleneck, especially when we scale to many devices. This bottleneck prevents CLIP from being scaled efficiently, and it&rsquo;s the exact problem SigLIP solves.</p><h4 id=siglip>SigLIP</h4><p>The <a href=https://arxiv.org/abs/2303.15343 target=_blank rel=noopener>Sigmoid Loss for Language Image Pre-Training (SigLIP)</a> replaces the softmax in CLIP with a sigmoid loss, which is applied element-wise to the dot products between the image and text representations. The loss objective is then closer to standard <em>predictive learning</em> with binary cross entropy, training the positive pairs to be $1$ while negative ones are pushed closer to $0$. This allows the model to train on a large batch size without the need for full-batch softmax computation, making it more efficient and scalable.</p><p>SigLIP has been shown to achieve similar or even better performance to CLIP, in particular for smaller datasets of image-caption pairs. Furthermore, SigLIP is more computationally efficient at scale, making it a popular choice for training strong vision encoders in multi-modal models.</p><h3 id=inference>Inference</h3><p>Once the vision encoder is trained, we can use it to generate embeddings for images. However, both CLIP and SigLIP train single-vector representations, meaning that the image is represented by a single vector. This is not ideal for multi-modal models, as we want to generate a sequence of embeddings that can be fed into the text decoder. Furthermore, a single vector representation may not capture the full content of the image.</p><p>To address this, we can use the internal representations of the Vision Encoder, which is commonly a Vision Transformer, to extract a sequence of embeddings for the image. This allows us to capture a more detailed representation of the image, which can be used by the text decoder to generate more accurate and detailed text descriptions. This is a key aspect of the LLaVA model family, which combines the power of Vision Transformers with Large Language Models to generate human-like text based on images.</p><p>In <a href=../edge-llm-embed-llava/>the next blogpost</a>, we will see that there are actually several variations on how a Vision Transformer can be implemented in the CLIP and SigLIP frameworks (e.g. using a separate class embedding, pooling, etc.). These variations can have a significant impact on the performance of the final LLaVA model and require adjusting the model architecture accordingly. Hence, it is important to carefully consider them when implementing your own LLaVA model.</p><br><h2 id=machine-learning-compiler-implementation>Machine Learning Compiler Implementation</h2><p>Now that we are familiar with the LLaVA model family and the vision encoders used in these models, let&rsquo;s discuss how we can deploy these models on edge devices. As we have introduced in <a href=../edge-llm-mlc/>Part 1 of this blog post series</a>, we use <a href=https://llm.mlc.ai target=_blank rel=noopener>the Machine Learning Compiler (MLC) project</a> for on-edge deployment. MLC aims to optimize and compile deep learning models for edge devices, enabling efficient and fast inference on resource-constrained hardware. MLC supports a wide range of deep learning models, including LLaVA models, making it a powerful tool for deploying multi-modal models on edge devices.</p><h3 id=the-llava-model-family-on-mlc>The LLaVA Model Family on MLC</h3><p>Out of the box, the MLC LLM supports the quantization of the LLaVA family of vision encoders and text decoders. For this, the LLaVA implementation of the <a href=https://huggingface.co/docs/transformers/en/model_doc/llava target=_blank rel=noopener>Hugging Face Transformers library</a> has been integrated into the MLC framework using the TVM stack. At the time of writing (November 2024), the default supported text decoders are Llama and Mistral, with a CLIP-trained vision encoder. However, the sizes of these models are often around 7B parameters and larger, making them unsuitable for deployment on small edge devices (and even my M2 MacBook Air). This is why we need to consider smaller models, which are more suitable for our travel recommendation chatbot.</p><h3 id=llava-onevision>LLaVA OneVision</h3><p>One of the smallest LLaVA models is the <a href=https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf target=_blank rel=noopener>LLaVA OneVision Qwen2 0.5B</a>, which, as the name suggests, uses the <a href=https://qwenlm.github.io/blog/qwen2/ target=_blank rel=noopener>Qwen2 language model</a> with 0.5B parameters.</p><figure><img src=images/llava_one_vision_performance.png><figcaption>LLaVA-OneVision models tackle even the trickiest secondary school math problems with ease. In this example, the model demonstrates its ability to interpret multiple images and coherently apply deductive reasoning (<a href=https://arxiv.org/pdf/2408.03326>image credit</a>).</figcaption></figure><p>The LLaVA OneVision Qwen2 0.5B model is particularly suitable for deployment on edge devices, as it is small and lightweight. While we can&rsquo;t expect the same performance that larger models offer, this small LLaVA OneVision model is a good starting point for our travel recommendation chatbot and allows a fast iteration cycle for model development.</p><p>In contrast to the original LLaVA models, the <a href=https://arxiv.org/abs/2408.03326 target=_blank rel=noopener>LLaVA OneVision model</a> uses a SigLIP-pretrained vision encoder, as it has demonstrated higher multi-modal LLM performance among open vision encoders. While we mentioned that in theory, there are no differences between SigLIP and CLIP during inference, the encoders slighlty differ in their Huggingface Transformers implementation. This is why we need to first port the LLaVA OneVision model and its SigLIP vision encoder to the MLC framework. Once again, I&rsquo;ll walk you through how to do in <a href=../edge-llm-embed-llava/>the next blog post</a>.</p><br><h2 id=conclusion>Conclusion</h2><p>Multi-modal foundation models allow us to apply state-of-the-art models to new and more complex applications. We&rsquo;ve introduced how we can use a pre-trained vision encoder and text decoder architecture to build a general-purpose vision-language chatbot. The Machine Learning Compiler Project currently supports embedding the LLaVA vision-to-text models on edge devices. However, due to our restricted local device, we will use the smallest LLaVA OneVision model with only 0.5B, which needs to be manually ported to the MLC framework. Thus, we&rsquo;ve taken some time to understand the intricacies between LlaVA model implementations and what this means during model training and testing time.</p><h3 id=whats-next>What&rsquo;s next?</h3><p>This blog post provides you with the background knowledge needed to deploy a multi-modal vision-to-text model on edge devices. In <a href=../edge-llm-embed-llava/>the next blog post of this series</a>, I&rsquo;ll walk you through how to put this knowledge into practice.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/embedded-systems/>Embedded Systems</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-llm-vision-encoders/&amp;text=Shrinking%20the%20Impossible%20%28Part%202%29:%20Teaching%20Chatbots%20to%20See%20with%20LLaVA,%20CLIP,%20and%20SigLIP" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-llm-vision-encoders/&amp;t=Shrinking%20the%20Impossible%20%28Part%202%29:%20Teaching%20Chatbots%20to%20See%20with%20LLaVA,%20CLIP,%20and%20SigLIP" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Shrinking%20the%20Impossible%20%28Part%202%29:%20Teaching%20Chatbots%20to%20See%20with%20LLaVA,%20CLIP,%20and%20SigLIP&amp;body=https://bellanich.github.io/post/edge-llm-vision-encoders/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-llm-vision-encoders/&amp;title=Shrinking%20the%20Impossible%20%28Part%202%29:%20Teaching%20Chatbots%20to%20See%20with%20LLaVA,%20CLIP,%20and%20SigLIP" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Shrinking%20the%20Impossible%20%28Part%202%29:%20Teaching%20Chatbots%20to%20See%20with%20LLaVA,%20CLIP,%20and%20SigLIP%20https://bellanich.github.io/post/edge-llm-vision-encoders/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-llm-vision-encoders/&amp;title=Shrinking%20the%20Impossible%20%28Part%202%29:%20Teaching%20Chatbots%20to%20See%20with%20LLaVA,%20CLIP,%20and%20SigLIP" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu72f3a3ef3a65e4417b2658a21b6b7f3a_388362_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-llm-app/>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</a></li><li><a href=/post/edge-llm-embed-llava/>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</a></li><li><a href=/post/edge-llm-mlc/>Shrinking the Impossible (Part 1): Optimizing Foundation Models for Edge Devices with MLC</a></li><li><a href=/post/reinforcement-learning/>Reinforcement Learning: Investigating Gradient Stability in Policy Based Methods</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>