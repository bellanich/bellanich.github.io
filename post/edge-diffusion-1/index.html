<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Bella Nicholson"><meta name=description content="How I chased a diffusion model small enough for the iPhone, fast enough for real use, and resilient enough to avoid corruption—unpacking what works, what doesn’t, and why."><link rel=alternate hreflang=en-us href=https://bellanich.github.io/post/edge-diffusion-1/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#6D49FF"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.319d1d4610b53d3ccd8a5eb387a25065.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://bellanich.github.io/post/edge-diffusion-1/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Bella Nicholson"><meta property="og:url" content="https://bellanich.github.io/post/edge-diffusion-1/"><meta property="og:title" content="Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model | Bella Nicholson"><meta property="og:description" content="How I chased a diffusion model small enough for the iPhone, fast enough for real use, and resilient enough to avoid corruption—unpacking what works, what doesn’t, and why."><meta property="og:image" content="https://bellanich.github.io/post/edge-diffusion-1/featured.jpeg"><meta property="twitter:image" content="https://bellanich.github.io/post/edge-diffusion-1/featured.jpeg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-11-24T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-24T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bellanich.github.io/post/edge-diffusion-1/"},"headline":"Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model","image":["https://bellanich.github.io/post/edge-diffusion-1/featured.jpeg"],"datePublished":"2025-11-24T00:00:00Z","dateModified":"2025-11-24T00:00:00Z","author":{"@type":"Person","name":"Bella Nicholson"},"publisher":{"@type":"Organization","name":"Bella Nicholson","logo":{"@type":"ImageObject","url":"https://bellanich.github.io/images/icon_huc1af03d399f8074958ea0ac44dc20f74_179729_192x192_fill_lanczos_center_3.png"}},"description":"How I chased a diffusion model small enough for the iPhone, fast enough for real use, and resilient enough to avoid corruption—unpacking what works, what doesn’t, and why."}</script><title>Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model | Bella Nicholson</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.a30802436269f02f8cb649e501c83c0c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Bella Nicholson</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Bella Nicholson</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Bio</span></a></li><li class=nav-item><a class=nav-link href=/blog/#blog><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/experience/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/projects/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/talks/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/media/BellaNicholson_CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Frames Per Second (Part 1): The Hunt for a Tiny, High-Quality Diffusion Model</h1><div class=article-metadata><div><span>Bella Nicholson</span></div><span class=article-date>Nov 24, 2025
</span><span class=middot-divider></span>
<span class=article-reading-time>13 min read</span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:483px><div style=position:relative><img src=/post/edge-diffusion-1/featured_hu4e482fa6b5af4492f34576bf364fbae3_601533_720x0_resize_q98_lanczos.jpeg alt class=featured-image>
<span class=article-header-caption>Image credit: <a href=https://blog.google/technology/ai/nano-banana-pro/ target=_blank rel=noopener><strong>Nano Banana Pro</strong></a></span></div></div><div class=article-container><div class=article-style><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#introduction>Introduction</a><ul><li><a href=#problem-constraints>Problem Constraints</a></li></ul></li><li><a href=#beyond-the-noise-unpacking-the-architecture-of-diffusion-models>Beyond the Noise: Unpacking the Architecture of Diffusion Models</a><ul><li><a href=#model-architecture>Model Architecture</a></li><li><a href=#text-to-image-vs-image-to-image-generation>Text-to-Image vs. Image-to-Image Generation</a></li><li><a href=#nano-banana-todays-state-of-the-art>Nano Banana: Today&rsquo;s State of the Art</a></li></ul></li><li><a href=#the-model-search>The Model Search</a><ul><li><a href=#tiny-sd-starting-as-small-as-possible>Tiny SD: Starting as Small as Possible</a></li><li><a href=#coreml-stable-diffusion>CoreML Stable Diffusion</a></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#whats-next>What&rsquo;s next?</a></li></ul></li></ul><h2 id=introduction>Introduction</h2><p>I have a problem: I love testing out and applying the latest ML research, but I really dislike managing my own cloud infrastructure. That&rsquo;s why I ended up <a href=../../portfolio//edge-llm/>embedding a multimodal LLM on various edge devices</a> last year. However, generated text doesn&rsquo;t deliver the same immediate, visceral impact that high-quality images do, compelling me to switch domains.</p><p>Unfortunately, deploying a state-of-the-art (SOTA) diffusion model on the edge is far harder than its LLM counterpart. LLMs work in a discrete output space (i.e., tokens). Thus, they tolerate the noise of simple compression algorithms relatively well. In contrast, diffusion models operate on a continuous, dense latent space, causing the same amount of noise to more severely degradate model performance. Attempting to shrink a 4GB model to fit the standard 2GB iOS memory budget is a brutal performance problem. For better or worse, this is my favorite type of problem to solve.</p><h3 id=problem-constraints>Problem Constraints</h3><p>To keep things interesting, I decided to target deployment directly for my iPhone 16 (~8GB of RAM). If this model can run effectively on my phone, I&rsquo;ll always have a tiny, powerful image generator right in my pocket. However, this choice immediately imposed a very strict iOS memory budget. iOS apps encounter a dynamic limit from ~2-4 GB (roughly <a href=https://developer.apple.com/documentation/coreml target=_blank rel=noopener>50-70% of total RAM</a>) that triggers an <code>EXC_RESOURCE RESOURCE_TYPE_MEMORY</code> termination exception and crashes the app.</p><p>Of course, compressing the model is only half the battle. If it&rsquo;s too slow, any reasonable user will just quit the app, rendering the entire point moot. Hence, I set a 60-second end-to-end limit for the pipeline, allotting 40 seconds for model inference.</p><p>This high bar for speed and precision demanded a pipeline built around conditional image generation (Image-to-Image or Img2Img). This is essential because it:</p><ol><li>Provides superior creative control and output fidelity.</li><li>Elevates the ML-side challenge by requiring hands-on control of the model&rsquo;s neural network sub-components</li><li>Delivers a more engaging user experience by actively transforming the source photo</li></ol><p>The final, non-negotiable rule was that the output images needed to be of reasonable quality. Meaning, the model needs to generate easily identifiable objects that are in-line with the provided text prompt.</p><h2 id=beyond-the-noise-unpacking-the-architecture-of-diffusion-models>Beyond the Noise: Unpacking the Architecture of Diffusion Models</h2><p>The path to modern image generation was surprisingly quick. <a href=https://openai.com/index/dall-e-2/ target=_blank rel=noopener>Open AI released DALL-E in January 2021</a>, <a href=https://stability.ai/news/stable-diffusion-public-release target=_blank rel=noopener>Stable Diffusion democratized the field in August 2022</a>, and suddenly everyone had access to conditional image synthesis.</p><p>This new era of vision models is powered by <a href=https://arxiv.org/pdf/2006.11239 target=_blank rel=noopener>diffusion</a>, where the model learns to destroy images systematically and then reverses the process.</p><figure><img src=images/noise_to_image.jpg style=width:100%;height:auto><figcaption><strong>Figure 1.</strong> A visualization of the diffusion process. Noise is added to (forward pass) or removed from the image (reverse processes) (<a href=https://www.siam.org/publications/siam-news/articles/generalization-of-diffusion-models-principles-theory-and-implications/>image credit</a>).</figcaption></figure><p>We start with the original image $x_0$ and progressively corrupt it by adding Gaussian noise over $T$ timesteps. At each step, we keep some fraction of the previous image and add fresh noise:</p><p>$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)$$</p><p>where $\beta_t$ controls the noise intensity. This defines a <strong>Markov chain</strong>, a sequence of random states where each state $x_t$ depends only on the immediately previous state $x_{t-1}$, nothing earlier.</p><p>This Markov property ensures that while each image follows a different random path to noise, all images at timestep t share identical noise statistics (same signal-to-noise ratio). This predictable structure lets us train a neural network to predict the noise $\epsilon$ added at any timestep, which we can then subtract to reverse the corruption. Once trained, the model generates images by starting with pure noise and iteratively denoising over 20-50 steps, guided by a text prompt.</p><p>During training, we need noisy images at various timesteps to teach the model this denoising function. Stepping through $x_1 \rightarrow x_2 \rightarrow \cdots \rightarrow x_t$ sequentially for every training sample would be computationally infeasible. Fortunately, a key mathematical property of Markov chains with Gaussian transitions is that the entire sequence collapses into a closed-form solution:</p><p>$$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon $$</p><p>where $\epsilon \sim \mathcal{N}(0, I)$ is random noise and $\bar{\alpha}_t = \prod_{i=1}^{t}(1 - \beta_i)$ accumulates all the noise scaling factors up to time $t$. This <strong>reparameterization trick</strong> lets us jump directly to any timestep in one shot, making model training computationally feasible</p><h3 id=model-architecture>Model Architecture</h3><p>The <a href=https://arxiv.org/abs/2006.11239 target=_blank rel=noopener>original diffusion paper</a> introduced the denoising process, but it operated directly on pixels, making it computationally expensive. <a href=https://arxiv.org/abs/2112.10752 target=_blank rel=noopener>Stable Diffusion</a> changed the game by running diffusion in a compressed latent space, which dramatically reduces computational costs while maintaining quality.</p><figure><img src=images/stable-diffusion-components.png style=width:100%;height:auto><figcaption><strong>Figure 2.</strong> A visualization of how Stable Diffusion's three neural components work together to do text-to-image generation. In image-to-image generation, a vision encoder also maps the original image to a latent space representation (<a href=https://jalammar.github.io/illustrated-stable-diffusion/>image credit</a>).</figcaption></figure><p>Stable Diffusion is a composite of four neural networks:</p><ol><li><strong>Text encoder.</strong> We use a <a href=https://arxiv.org/abs/2103.00020 target=_blank rel=noopener>Contrastive Language-Image Pre-training (CLIP)</a> network to convert text prompts into $77 \times 768$ embedding vectors. These embeddings semantically link language to visual concepts, allowing our model to &ldquo;understand&rdquo; our text inputs.</li><li><strong>Image encoder.</strong> We use a <a href=https://arxiv.org/abs/1312.6114 target=_blank rel=noopener>Variational Autoencoder Encoder (VAE)</a> to compress images from pixel space ($3 \times H \times W$) to a compact latent space ($4 \times \frac{H}{8} \times \frac{W}{8}$). This ~64× compression is the key efficiency innovation, since it lets us denoise the compressed representations rather than the full-resolution images.</li><li><strong>Image denoiser.</strong> The <a href=https://arxiv.org/abs/1505.04597 target=_blank rel=noopener>U-Net</a> is the core component that implements the reverse diffusion process. It accepts noisy latents, the current timestep, and text embeddings (via <a href=https://arxiv.org/abs/1706.03762 target=_blank rel=noopener>cross-attention</a>) to predict what noise to remove.<ul><li>This is the model&rsquo;s heaviest component, consuming <a href=https://github.com/CompVis/stable-diffusion target=_blank rel=noopener>roughly 85% of the Stable Diffusion&rsquo;s total memory footprint</a>. The immense size is mandatory because the U-Net must model a universal, continuous denoising function spanning all timesteps and image content. This dense predictive complexity is precisely why the U-Net resists compression.</li></ul></li><li><strong>Image decoder.</strong> The VAE decoder decompresses our latents back to high-resolution pixel-space images. This reconstruction is the final output that the model returns.</li></ol><p>Even with these efficiency gains, deploying Stable Diffusion on mobile devices requires further aggressive but non-destructive U-Net compression. This architecture supports two distinct generation modes, each with different performance characteristics.</p><h3 id=text-to-image-vs-image-to-image-generation>Text-to-Image vs. Image-to-Image Generation</h3><p>Stable Diffusion supports two generation modes, each starting from a different point in the noise spectrum:</p><ol><li><strong>Text-to-Image (T2I)</strong> starts with pure random noise and relies solely on the text prompt to guide generation. This maximizes the model&rsquo;s creative freedom, but means we get little control over the final image&rsquo;s structure or composition.</li><li><strong>Image-to-Image (Img2Img)</strong> adds noise to an existing image&rsquo;s latent representation, then denoises it while being guided by both the original image structure and a text prompt. This trades creative flexibility for precise control over image composition.<ul><li>The <a href=https://github.com/CompVis/stable-diffusion/blob/main/README.md#image-modification-with-stable-diffusion target=_blank rel=noopener>strength parameter</a> sets how aggressively the model transforms its input. At $0.0$, the model returns the original image untouched. At $1.0$, the input is completely replaced with noise, making the output nearly identical to Text-to-Image generation.</li></ul></li></ol><p>These two modes establish the foundational mechanics of image creation, but what happens when you scale that process to an unbelievable level of fidelity and control? That&rsquo;s where Nano Banana Pro enters the frame.</p><h3 id=nano-banana-todays-state-of-the-art>Nano Banana: Today&rsquo;s State of the Art</h3><p>In August 2025, Google DeepMind released <a href=https://aistudio.google.com/models/gemini-2-5-flash-image target=_blank rel=noopener>Nano Banana</a>, a Gemini model natively generates interleaved text and images. It excels at image editing thanks to its strong character-consistency across different image edits. And unlike most image-generation models, which still benefited from long, detailed prompts, Nano Banana performs well with simple instructions.</p><figure><img src=images/nano-banana-figurine.png style=width:60%;height:auto><figcaption><strong>Figure 3.</strong> An example of the viral "Nano Banana" trend, where users generated figures representations of themselves and their favorite film characters, including James Bond (<a href=https://www.instagram.com/p/DOiy0wciShc/>image credit</a>).</figcaption></figure><p>These capabilities were striking enough to <a href=https://www.cnbc.com/2025/11/20/google-nano-banana-pro-gemini-3.html target=_blank rel=noopener>spark a viral and global social media trend</a>, where users turned themselves into figurines (Figure 3).</p><p>Three months later, <a href=https://deepmind.google/models/gemini-image/pro/ target=_blank rel=noopener>Nano Banana Pro</a> arrived. It extended its predecessor’s multi-character consistency to handle scenes with 10+ people (Figure 4) and dramatically improved text rendering, enabling designer-level infographics to be generated in minutes.</p><figure><img src=images/nano_banana_consistency.png style=width:100%;height:auto><figcaption><strong>Figure 4.</strong> Nano Banana Pro demonstrates an uncanny level of character consistency with its ability to merge 14 distinct, cute, and fuzzy characters into a cohesive scene (<a href=https://blog.google/technology/ai/nano-banana-pro/>image credit</a>).</figcaption></figure><p>Figure 5 shows one such example: “Best Chocolate Around the World: A Global Taste Odyssey,” which illustrates how cocoa is grown, processed, and enjoyed across regions.</p><figure><img src=images/chocolate_infographic.jpg style=width:100%;height:auto><figcaption><strong>Figure 5.</strong> Nano Banana Pro can generate beautifully illustrated and ultra-detailed infographic on demand. Consider this delicious example about where cocoa beans are grown and how they're turned into chocolate.</figcaption></figure><p>However, both models remain closed-source and proprietary, so we can only infer how they work. We know performs some form of planning-style reasoning for image generation, because <a href=https://x.com/karpathy/status/1992655330002817095 target=_blank rel=noopener>it can solve university level phsyics and chemistry problems</a> by generating neatly written, correct solutions directly onto a blank exam page. It is, frankly, impressively capable. And since it is built on Gemini, it&rsquo;s also probably too large to run on edge devices, even with aggressive model compression and optimization.</p><figure><img src=images/nano-banana-exam.jpeg style=width:100%;height:auto><figcaption><strong>Figure 6.</strong> Nano Banana Pro was able to successfully generate the correct solutions, including doodles, for university-level Physics and Chemistry exam questions (<a href=https://x.com/karpathy/status/1992655330002817095/photo/1>image credit</a>).</figcaption></figure><p>This leaves us with a clear goal: replicate as much of this functionality as possible using open-source, on-device alternatives. Unfortunately, current mobile-friendly diffusion models more closely resemble 2021–2022 Stable Diffusion systems. They can produce coherent images, but they cannot match Nano Banana’s full range of abilities.</p><br><h2 id=the-model-search>The Model Search</h2><p>Since Nano Banana Pro&rsquo;s advanced capabilities only emerge at massive scale, we have to accept two harsh realities:</p><ol><li>We have to rely on open-source, convertible models that often lag 6-12 months behind industry SOTA; and</li><li>The model we choose won&rsquo;t have the same magical coherence of its cloud-scale counterparts.</li></ol><p>Simply put, I need to select a model that&rsquo;s small enough to run on-device but still capable enough to produce usable results. We can translate our earlier <a href=#problem-constraints>problem constraints</a> into the model specifications shown in Table 1.</p><figure><table><thead><tr><th>Requirement</th><th>Specification</th><th>Reasoning</th></tr></thead><tbody><tr><td><strong>Size</strong></td><td>Memory footprint &lt;2GB</td><td>iOS apps face strict memory limits (~2-4GB), allocate 2GB primarily for model usage to prevent crashes</td></tr><tr><td><strong>Performance</strong></td><td>Inference &lt;40 seconds</td><td>Fits within 60-second end-to-end pipeline budget, leaves room for pre/post-processing</td></tr><tr><td><strong>Hardware Support</strong></td><td>Apple Neural Engine (ANE) optimization required</td><td>Standard Metal GPU processing will be too slow, need leverage the iPhone's built-in AI accelerator</td></tr><tr><td><strong>Methodology</strong></td><td>Separate component access (CLIP, U-Net, VAE Encoder/Decoder)</td><td>Conditional image-to-image generation is inherently modular, components must be accessed separate for img2img tasks</td></tr><tr><td><strong>Quality</strong></td><td>Maintain human subject identity with high-fidelity</td><td>Core product requirement: failure to maintain character consistency leads to poor user experience.</td></tr></tbody></table><figcaption><strong>Table 1.</strong> Model specifications for my edge conditional image generation application</figcaption></figure><h3 id=tiny-sd-starting-as-small-as-possible>Tiny SD: Starting as Small as Possible</h3><p>To establish a minimum viable quality baseline, I targeted the smallest available contender: <a href=https://huggingface.co/segmind/tiny-sd target=_blank rel=noopener>Segmind&rsquo;s Tiny SD</a>. As a 55% parameter reduction of Stable Diffusion, it is among the most aggressively compressed models from an established maintainer. Since it only consumed half of my tight 2GB iOS memory ceiling, it was the perfect, low-risk candidate to stress-test the absolute lower bound of acceptable quality and performance.</p><p>My next move was optimizing tiny SD for speed. I used <a href=https://developer.apple.com/documentation/coreml target=_blank rel=noopener>CoreML</a> , Apple&rsquo;s dedicated framework for integrating machine learning models into apps, to convert the weights into an <a href=https://github.com/hollance/neural-engine target=_blank rel=noopener>Apple Neural Engine (ANE)</a> optimized format. The ANE is the dedicated hardware accelerator built into Apple Silicon, specifically designed to run on-device neural network inference with superior power efficiency. Meaning, if this works, I will be able to conditionally generate images without killing my phone&rsquo;s battery.</p><p>In my initial test, I wanted to validate the model&rsquo;s basic text-to-image (T2I) generation. To keep this assessment fair, I used the same text prompt Segmind provided in <a href=https://huggingface.co/segmind/tiny-sd target=_blank rel=noopener>their model card</a>: <code>"Portrait of a pretty girl"</code>. Despite my best efforts to meet the 40-second deadline (via 25-30 diffusion steps), the model failed quality control. Instead of images, I was left with psychedelic noise and low-fidelity artifacts (Figure 7).</p><figure style=display:flex;flex-direction:column;align-items:center;text-align:center><div style=display:flex;justify-content:center;gap:10px><div style=width:45%><img src=images/tiny_sd_simple_prompt.png style=width:100%><div>(a)</div></div><div style=width:45%><img src=images/tiny_sd_elaborate_prompt.png style=width:100%><div>(b)</div></div></div><figcaption><strong>Figure 7. </strong>The samples illustrate Tiny SD's quality collapse. <strong>(a)</strong> Default prompt at recommended <a href=https://huggingface.co/docs/diffusers/en/using-diffusers/write_your_own_pipeline#classifier-free-guidance>guidance scale</a> (7.5). <strong>(b)</strong> Enhanced prompt and an aggressive guidance (11.0) to force better prompt adherence. Both outputs were generated within the 25–30 step limit and exhibit severe artifacts and image distortions.</figcaption></figure><p>Despite the artifacts, rough semantic alignment remains: Fig. 7A shows a framed &ldquo;portrait&rdquo; of a woman and Fig. 7B renders a woman with &ldquo;flowing hair.&rdquo; Crucially, Figure 8 confirms the original Tiny SD model produces coherent, acceptable (if blurry) outputs. This performance gap strongly suggests our CoreML pipeline is sound, but the model weights are being corrupted during the conversion or loading process.</p><figure><img src=images/tiny_sd_readme_outputs.png style=width:60%;height:auto><figcaption><strong>Figure 8.</strong> Official examples of Tiny SD outputs. These samples confirm the original Tiny SD is capable of generating coherent, if slightly blurry, portraits of people (e.g., center-left image), establishing an acceptable quality baseline prior to CoreML conversion (<a href=https://huggingface.co/segmind/tiny-sd>source</a>).</figcaption></figure><p>So, where did the weights go wrong? The corruption must have originated from one of these three technical suspects:</p><ol><li><p><strong>Inference step requirements.</strong> Distilled models often require higher step counts for convergence than their parent models, a detail missing from the Tiny SD documentation. Our current $25-30$ steps may be too few.</p></li><li><p><strong>CoreML quantization precision loss.</strong> CoreML&rsquo;s model packaging applies weight quantization (typically FP16 or mixed precision) that could compound errors in an already-distilled model, potentially degrading performance below acceptable limits.</p></li><li><p><strong>VAE decoder corruption during CoreML conversion.</strong> The VAE decoder is the model component most sensitive to weight corruption . It is a critical single point of failure because it performs the final, irreversible $64\times$ spatial upsampling. CoreML conversion might corrupt its transposed convolution weights, and (unlike the self-correcting U-Net) even the slightest VAE decoder corruption turns perfect latents into unusable outputs.</p></li></ol><p>Pinpointing the exact cause of corruption would require a costly series of controlled ablation studies on <a href=https://huggingface.co/SG161222/Realistic_Vision_V4.0_noVAE target=_blank rel=noopener>the teacher PyTorch model</a>: testing step counts, comparing FP32 vs. FP16 precision, and measuring degradation at each CoreML conversion stage.</p><p>However, this investigation isn&rsquo;t needed. This test already validates Tiny SD&rsquo;s non-viability: I need CoreML/ANE compilation and a small number of diffusion steps to meet my strict sub-40-second latency budget. As seen, tiny SD can&rsquo;t deliver under these constraints. It&rsquo;s time to pivot to a model explicitly designed with Apple&rsquo;s silicon in mind.</p><h3 id=coreml-stable-diffusion>CoreML Stable Diffusion</h3><p>The search for a viable replacement led me to Apple&rsquo;s <a href=https://github.com/apple/ml-stable-diffusion target=_blank rel=noopener>CoreML Stable Diffusion</a>. This is a professionally tuned implementation of <a href=https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 target=_blank rel=noopener>Runway ML&rsquo;s stable-diffusion-v1-5</a> (1.3B parameters) that&rsquo;s compressed into ~1.5GB.</p><p>What makes this model viable where Tiny SD collapsed? Its key advantage is co-design with Apple&rsquo;s hardware team. This grants engineers access to proprietary optimizations—like deep operator fusion and memory layout—to produce a calibrated FP16 model guaranteeing peak ANE performance unavailable through generic conversions. Crucially, this implementation is also battle-tested for iOS deployment, eliminating the risk of weight corruption I previously faced.</p><figure><img src=images/coreml-quick-test.png style=width:40%;height:auto><figcaption><strong>Figure 9.</strong> Prototype testing. A generated image from the text prompt <code>"A beautiful landscape with mountains and a lake, golden hour lighting"</code> using Apple’s CoreML Stable Diffusion model. The output is a high-quality, realistic rendering of a rustic mountain range at sunset.</figcaption></figure><p>The trade-off is simple: I accept a ~1.5GB footprint (still well within budget) for a solution that guarantees production quality. Sometimes the &ldquo;smallest&rdquo; solution isn&rsquo;t the best one. Hardware-aware optimizations at a reasonable scale beat model over-compression.</p><br><h2 id=conclusion>Conclusion</h2><p>In this post, we explored the tight constraints required to deliver a Nano Banana-like experience on the edge. Our initial exploration led us to select Apple&rsquo;s CoreML Stable Diffusion model due its aggressive hardware co-design.</p><h3 id=whats-next>What&rsquo;s next?</h3><p>Before we can attempt to replicate the Nano Banana experience on device, we first need to understand what problems Apple&rsquo;s CoreML optimizations truly solved and which technical challenges remain. <a href=../edge-diffusion-2/>My next blog post</a> covers exactly how Apple safely compressed a diffusion model that is so easy to corrupt.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/generative-ai/>Generative AI</a>
<a class="badge badge-light" href=/tag/edge-ml/>Edge ML</a>
<a class="badge badge-light" href=/tag/computer-vision/>Computer Vision</a>
<a class="badge badge-light" href=/tag/article/>Article</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://bellanich.github.io/post/edge-diffusion-1/&amp;text=Frames%20Per%20Second%20%28Part%201%29:%20The%20Hunt%20for%20a%20Tiny,%20High-Quality%20Diffusion%20Model" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://bellanich.github.io/post/edge-diffusion-1/&amp;t=Frames%20Per%20Second%20%28Part%201%29:%20The%20Hunt%20for%20a%20Tiny,%20High-Quality%20Diffusion%20Model" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Frames%20Per%20Second%20%28Part%201%29:%20The%20Hunt%20for%20a%20Tiny,%20High-Quality%20Diffusion%20Model&amp;body=https://bellanich.github.io/post/edge-diffusion-1/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://bellanich.github.io/post/edge-diffusion-1/&amp;title=Frames%20Per%20Second%20%28Part%201%29:%20The%20Hunt%20for%20a%20Tiny,%20High-Quality%20Diffusion%20Model" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Frames%20Per%20Second%20%28Part%201%29:%20The%20Hunt%20for%20a%20Tiny,%20High-Quality%20Diffusion%20Model%20https://bellanich.github.io/post/edge-diffusion-1/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://bellanich.github.io/post/edge-diffusion-1/&amp;title=Frames%20Per%20Second%20%28Part%201%29:%20The%20Hunt%20for%20a%20Tiny,%20High-Quality%20Diffusion%20Model" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://bellanich.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu72f3a3ef3a65e4417b2658a21b6b7f3a_388362_270x270_fill_q98_lanczos_center.jpg alt="Bella Nicholson"></a><div class=media-body><h5 class=card-title><a href=https://bellanich.github.io/>Bella Nicholson</a></h5><h6 class=card-subtitle>Machine Learning Engineer</h6><ul class=network-icon aria-hidden=true><li><a href=https://github.com/bellanich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/bella-nicholson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=mailto:bellanich.software@gmail.com><i class="far fa-envelope"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/post/edge-diffusion-3/>Frames Per Second (Part 3): Turning a Tiny Diffusion Model into a Traveling Photobooth</a></li><li><a href=/post/edge-diffusion-2/>Frames Per Second (Part 2): Quantization, Kernels, and the Path to On-Device Diffusion</a></li><li><a href=/post/edge-llm-app/>Shrinking the Impossible (Part 4): Deploying My Own Pocket-Sized Multi-Modal Large Language Model</a></li><li><a href=/post/edge-llm-embed-llava/>Shrinking the Impossible (Part 3): Embedding a Custom-Defined LLaVA-OneVision Model with MLC</a></li><li><a href=/post/edge-llm-vision-encoders/>Shrinking the Impossible (Part 2): Teaching Chatbots to See with LLaVA, CLIP, and SigLIP</a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",event:"Events",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/wowchemy.min.2715644373c13ab983bf49e4043ffe04.js></script></body></html>